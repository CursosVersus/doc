<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook"
	 xmlns:xlink="http://www.w3.org/1999/xlink"
	 version="5.0" xml:lang="es">
  <title>Pasos previos</title>
  <para>Antes de comenzar con la instalación de paquetes en sí, hay que
  plantear claramente la estructura de red y los componentes que se
  instalarán en cada equipo. La descripción física de los equipos ya se realizó
  en <link xlink:href="???">enlace</link>, por lo que nos centraremos
  ahora en los demás aspectos.</para>
  <section xml:id="nombres">
    <title>Nombres de los equipos</title>
    <para>Se han elegido los siguientes nombres para los equipos en los que se
    va a realizar la instalación del Cloud:</para>
    <itemizedlist>
      <listitem><para>jupiter: para el nodo controlador, que será el encargado
      de gestionar todos los recursos del cloud, interaccionar con los clientes
      y ordenar a los nodos de virtualización que ejecuten las instancias, pero
      en el que no se ejecutarán instancias del cloud. La mayor parte de
      componentes del Cloud y configuración se realizará en este
      equipo.</para></listitem>
      <listitem><para>io, europa, ganimedes y calisto (son las 4 lunas
      principales de júpiter): para los 4 nodos de virtualización. En estos
      equipos se instalará nova-compute y el resto de componentes necesarios
      para que se ejecuten las instancias en ellos.</para></listitem>
      <listitem><para>saturno: para el nodo de almacenamiento, ya que es tan
      importante como júpiter y a la vez independiente. En este equipo todavía
      no está definido el software que se instalará.</para></listitem>
      <listitem><para>venus: un equipo convencional en el que se instalarán los
      paquetes necesarios para usar el cliente nova con el que podemos
      gestionar el cloud desde línea de comandos sin la necesidad de
      realizar las operaciones desde jupiter. Estará conectado sólo a
      la red privada.</para></listitem>
    </itemizedlist>
  </section>
  <section xml:id="esquema_red">
    <title>Esquema de red</title>
    <para>Cada uno de estos equipos está conectado a dos redes (salvo venus), lo
    que en terminología de OpenStack se conoce como una red
    <emphasis>pública</emphasis> y una red <emphasis>privada</emphasis>, 
    términos que a los que no se debe dar el sentido habitual que tienen en redes
    IPv4, ya que en OpenStack la red pública se refiere a la que interaccionará
    con los clientes y la privada la que se utiliza para la intercomunicación de
    los componentes del cloud y en nuestro caso muy destacadamente, de la
    transferencia de imágenes a los nodos en los que se deben ejecutar las
    instancias. La red privada es una red aislada que no está conectada con
    otras redes, mientras que la red pública sí lo está, bien a una red local o
    incluso a Internet. Las transferencias de datos más importante se realizan a
    través de la red privada, por lo que se ha optado por conectar cada equipo
    (salvo jupiter) mediante tres interfaces de red Gigabit, para que
    configurándolas adecuadamente en modo <emphasis>link aggregation</emphasis>
    de acuerdo al estándar 802.3ad, puedan hasta triplicar las tasas de
    transferencia en la red privada.</para>
    <para>En la siguiente imagen, se puede ver de forma esquemática los equipos
    y las redes a las que están conectados:</para>
    <mediaobject>
      <imageobject>
	<imagedata fileref="figures/esquema_red_iesgn.png" width="10cm" format="PNG"/>
      </imageobject>
      <caption>
	<para>Esquema de red del Cloud, en la que aparecen los 6 equipos
	conectados a las redes privada y pública del cloud.</para>
      </caption>
    </mediaobject>
  </section>
  <section xml:id="direccionamieno_ip">
    <title>Direcciones IP de los equipos</title>
    <para>En la siguiente tabla aparecen las direcciones IPv4 privadas
    y públicas que tienen todos los nodos del cloud.</para>
    <table xml:id="tabla_ips">
      <title>Direcciones IP de los equipos del cloud</title>
      <tgroup cols='3' align='left' colsep='1' rowsep='1'>
	<thead>
	  <row>
	    <entry align="center"></entry>
	    <entry align="center">IP pública</entry>
	    <entry align="center">IP privada</entry>
	  </row>
	</thead>
	<tbody>
	  <row>
	    <entry align="center">jupiter</entry>
	    <entry align="center">172.22.222.1</entry>
	    <entry align="center">192.168.222.1</entry>
	  </row>
	  <row>
	    <entry align="center">saturno</entry>
	    <entry align="center">172.22.222.2</entry>
	    <entry align="center">192.168.222.2</entry>
	  </row>
	  <row>
	    <entry align="center">venus</entry>
	    <entry align="center"></entry>
	    <entry align="center">192.168.222.10</entry>
	  </row>
	  <row>
	    <entry align="center">io</entry>
	    <entry align="center">172.22.222.11</entry>
	    <entry align="center">192.168.222.11</entry>
	  </row>
	  <row>
	    <entry align="center">europa</entry>
	    <entry align="center">172.22.222.12</entry>
	    <entry align="center">192.168.222.12</entry>
	  </row>
	  <row>
	    <entry align="center">ganimedes</entry>
	    <entry align="center">172.22.222.13</entry>
	    <entry align="center">192.168.222.13</entry>
	  </row>
	  <row>
	    <entry align="center">calisto</entry>
	    <entry align="center">172.22.222.14</entry>
	    <entry align="center">192.168.222.14</entry>
	  </row>
	</tbody>
      </tgroup>
    </table>
    </section>
    <section xml:id="instalacion_jupiter">
      <title>Instalación de jupiter</title>
      <section xml:id="configuracion_raid_jupiter">
	<title>Configuración previa de los discos en RAID</title>
	<para>jupiter incluye dos discos duros SATAII de 500 GiB y una
	controladora de disco 3ware 9650SE. Se ha optado por configurar esta
	controladora de disco en modo RAID 1, para incluir un nivel elemental de
	seguridad y consistencia de datos, aunque obviamente esto no descarta la
	utilización adicional de otros mecanismos de copias de seguridad que se
	configurarán posteriormente. Al tratarse de una controladora RAID
	hardware y estar configurado previamente, el sistema operativo que
	arranque en el equipo sólo verá un disco duro en <code>/dev/sda</code>
	de 500GB aproximadamente.</para>
	<!-- <para>http;//jonas.genannt.name/  Hay repositorio, con -->
	<!-- aplicaciones para manejar el RAID 3ware 9650SE</para> -->
      </section>
      <section xml:id="instalacion_debian_jupiter">
	<title>Instalación de Debian Wheezy</title>
	<para>El sistema operativo elegido para los equipos del cloud
	es la versión de pruebas (<emphasis>testing</emphasis>) de Debian
	GNU/Linux, que actualmente se conoce con el nombre de código
	<emphasis>wheezy</emphasis> y que cuando finalmente se estabilice se
	conocerá como Debian 7. Pensamos que es la mejor opción en el caso de
	Debian, ya que <emphasis>squeeze</emphasis>, la versión estable
	actualmente, no incluye paquetes de OpenStack y la opción de instalar
	directamente desde los ficheros fuente o bien utilizar repositorios no
	oficiales, no nos parece tan acertada. La
	<emphasis>inestabilidad</emphasis> que comporta utilizar una versión no
	estabilizda de Debian, nos parece en este momento asumible, más teniendo
	en cuenta que vamos a implementar un cloud privado y no crítico.</para>
	<para>De todas las versiones de Debian, obviamente elegimos el
	<emphasis>port</emphasis> amd64, para aprovechar toda la capacidad de
	los procesadores.</para>
	<para>Durante la instalación realizamos el siguiente esquema de
	particionado:</para>
	<para>
	  <programlisting>
	    Disposit. Inicio    Comienzo      Fin      Bloques  Id Sistema
	    /dev/sda1   *        2048     1953791      975872   83 Linux
	    /dev/sda2         1953792   976539647   487292928   8e Linux LVM
	  </programlisting>
	</para>
	<para>Es decir, reservamos una pequeña partición de 1G en la
	que ubicaremos el directorio /boot y el resto lo gestionamos
	con LVM. Inicialmente creamos sólo dos volúmenes
	lógicos (uno para el sistema raíz y otro para el área de
	intercambio):</para>
	<para>
	  <programlisting>
	    LV           VG   Attr   LSize
	    jupiter-root vg   -wi-ao 18,62g
	    jupiter-swap vg   -wi-ao  1,86g
	  </programlisting>
	</para>
	<para>Aunque la controladora RAID es hardware y el sistema
	operativo no la gestiona, es importante que se pueda controlar
	su estado a través de alǵun módulo del kernel. En este caso el
	módulo 3w-9xxx que se carga automáticamente y nos envía estos
	mensajes al log del sistema:</para>
	<para>
	  <programlisting>
[    2.381798] 3ware 9000 Storage Controller device driver for Linux v2.26.02.014.
[    2.381841] 3w-9xxx 0000:01:00.0: PCI INT A -> GSI 19 (level, low) -> IRQ 19
[    2.381849] 3w-9xxx 0000:01:00.0: setting latency timer to 64
[    2.612067] scsi0 : 3ware 9000 Storage Controller
[    2.612128] 3w-9xxx: scsi0: Found a 3ware 9000 Storage Controller at 0xfe8df000, IRQ: 19.
[    2.952039] 3w-9xxx: scsi0: Firmware FE9X 4.08.00.006, BIOS BE9X 4.08.00.001, Ports: 2.
	  </programlisting>
	</para>
      </section>
      <section xml:id="configuracion_red_jupiter">
	<title>Configuración de red jupiter</title>
	<para>La configuración de red de jupiter es muy sencilla, ya que es un
	equipo con dos interfaces de red (eth0 y eth1), conectadas a la red
	pública y a la red privada respectivamente, el contenido del fichero
	<filename>/etc/network/interfaces</filename> incluye la configuración 
	estática de las dos interfaces de red:</para>
	<para>
	  <programlisting>
auto lo
iface lo inet loopback

auto eth0
iface eth0 inet static
        address 172.22.222.1
        netmask 255.255.0.0
        network 172.22.0.0
        broadcast 172.22.255.255
        gateway 172.22.0.1

auto eth1
iface eth1 inet static
        address 192.168.222.1
        netmask 255.255.255.0
        network 192.168.222
        broadcast 255.255.255.0
	  </programlisting>
	</para>
      </section>
    </section>
    <section xml:id="instalacion_4_lunas">
      <title>Instalación de io, europa, ganimedes y calisto</title>
      <para>La instalación de estos 4 nodos es idéntica, por lo que
      sólo explicaremos la instalación de uno de ellos.</para>
      <section xml:id="configuracion_raid_4_lunas">
	<title>RAID</title>
	<para>Cada uno de estos equipos incluye dos discos duros SAS
	de 300 GiB a 15000 rpm y una controladora ??? que se configura
	en modo RAID-0 al iniciar el equipo por primera vez. Una vez
	configurado el raid, el sistema operativo que arranque en el
	equipo sólo ve un disco duro en <code>/dev/sda</code> de
	600GB.</para>
      </section>
      <section xml:id="configuracion_bonding">
	<title>Configuración de <emphasis>Link aggregation</emphasis></title>
	<para></para>
      </section>
    </section>
    <section xml:id="instalacion_mysql">
      <title>Instalación y configuración básica de MySQL</title>
      <para>Todos los componentes de OpenStack (incluso glance y
      keystone que por defecto utilizan sqlite) guardarán sus datos en
      bases de datos MySQL.</para>
      <para>En primer lugar instalamos en jupiter el servidor de bases
      de datos MySQL:</para>
      <para><programlisting>root@jupiter:~# aptitude install mysql-server</programlisting></para>
      <para>Introducimos la contraseña del usurio root cuando se
      requiera y esperamos a que finalice la instalación.</para>
      <!-- Una vez concluida, abrimos el fichero /etc/mysql/my.cnf y
	   editamos la línea bind_address = 127.0.0.1 , quedando de la
	   siguiente forma: --> 

      <!-- bind_address = 0.0.0.0 (cuando OpenStack esté funcionando
	   habría que cambiarla por 172.22.0.0/16) -->
      <!-- root@jupiter:~# service mysql restart -->
      <para>Entramos en MySQL como root y creamos las bases de datos y
      un usuario para OpenStack, que tendrá todos los permisos sobre
      las bases de datos:</para>
      <para><programlisting>
	mysql> CREATE DATABASE keystone;
	mysql> CREATE DATABASE glance;
	mysql> CREATE DATABASE nova;
	mysql> CREATE USER "usuario_admin_openstack" IDENTIFIED BY
	'password';
	mysql> GRANT ALL PRIVILEGES ON keystone.* TO
	'"usuario_admin_openstack"'@'localhost' IDENTIFIED BY
	'password';
	>mysql> GRANT ALL PRIVILEGES ON glance.* TO
	'"usuario_admin_openstack"'@'localhost';
	mysql> GRANT ALL PRIVILEGES ON nova.* TO
      '"usuario_admin_openstack"'@'localhost';
      </programlisting></para>
    <para>En nuestra configuración, no es necesario que se realicen
    conexiones a la base de datos desde otro equipo diferente a
    jupiter, por lo que los usuarios se crean sólo en el ámbito local
    y el servidor aceptará peticiones al puerto 3306, sólo realizadas
    desde localhost.</para>
    </section>
    <section xml:id="otros_paquetes">
      <title>Instalación de otros paquetes</title>
      <para>Antes de instalar propiamente los paquetes relacionados
      con los diferentes componentes de Openstack, es recomendable
      instalar los siguientes paquetes:</para>
      <para><code>root@jupiter:~# apt-get install rabbitmq-server
      memcached</code></para>
      <para>rabbitmq-server se encarga de la gestión de mensajes
      entre los diferentes componentes de OpenStack (es un paquete
      obligatorio que si no instalamos ahora se instalará por
      dependencias) y memcached de cachear en memoria peticiones a
      bases de datos o a APIs</para>      
      <para>ntp</para>
    </section>
  </chapter>
