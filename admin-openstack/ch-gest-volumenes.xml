<?xml version="1.0" encoding="utf-8"?>
<chapter xmlns="http://docbook.org/ns/docbook"
         xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
         xml:id="ch-gest-volumenes">
  <title>Gestión de Volúmenes</title>

  <para><literal>Nova-volume</literal> es el servicio de OpenStack que permite
  proporcionar almacenamiento extra a las instancias, de forma similar al
  servicio <emphasis>Elastic Block Storage (EBS)</emphasis> de <emphasis>Amazon
  EC2</emphasis> pero con una implementación distinta.</para>

  <para>Nova-volume es una solución iSCSI, proporciona almacenamiento a nivel de
  bloque utilizando el Gestor de Volúmenes Lógico (LVM, Logical Volume Manager)
  de Linux. Al tratarse de almacenamiento a nivel de bloque, un volumen solo
  puede conectarse con una instancia en un momento determinado, no se trata de
  almacenamiento compartido como el proporcionado por sistemas de ficheros como
  NFS ó GlusterFS.</para>

  <para>El servicio nova-volume presenta volúmenes lógicos LVM a los nodos de
  computación que ejecutan las instancias a través del protocolo iSCSI (Internet
  SCSI, SCSI sobre TCP/IP). En esta arquitectura aparecen dos componentes
  principales:</para>

  <itemizedlist>
    <listitem>
      <para>LVM (<literal>lvm2</literal>), que utiliza un grupo de volúmenes
      denominado <literal>nova-volumes</literal>. El nombre del volumen es
      configurable a través del fichero de configuración
      <filename>nova.conf</filename></para>
    </listitem>

    <listitem>
      <para><literal>open-iscsi</literal>, la implementación de iSCSI que
      gestiona las sesiones iSCSI en los nodos de computación.</para>
    </listitem>
    
  </itemizedlist>

  <para>Los pasos que se producen desde la creación de un volumen hasta su
  conexión son:</para>

  <orderedlist>
    <listitem>
      <para>Se crea el volumen a través del comando <command>nova
      volume-create</command>, el cual crea un volumen lógico (LV) sobre el
      grupo de volúmenes (VG) "nova-volumes".</para>
    </listitem>

    <listitem>
      <para>El volumen se conecta a una instancia a través del comando
      <command>nova volume-attach</command>, el cual crea un identificador IQN
      (iSCSI Qualified Name) único que se presentará al nodo de computación.</para>
    </listitem>

    <listitem>
      <para>El nodo de computación que ejecuta la instancia a la que queremos
      conectar el volumen tiene ahora una sesión iSCSI activa y un nuevo
      dispositivo de almacenamiento local (normalmente un disco accesible a
      través de <filename>/dev/sdX</filename>).</para>
    </listitem>

    <listitem>
      <para><literal>libvirt</literal> utiliza dicho almacenamiento local como
      almacenamiento para la instancia, la instancia obtiene dicho
      almacenamiento a través de un nuevo disco, normalmente
      <filename>/dev/vdX</filename></para>
    </listitem>
    
  </orderedlist>
  
  <para>Para configurar este servicio, nuestra infraestructura debe tener un
  nodo controlador del Cloud ejecutando los servicios
  <literal>nova-api</literal>, <literal>nova-scheduler</literal>,
  <literal>nova-objectstore</literal>, <literal>nova-network</literal> y
  <literal>nova-volume</literal>. También necesitaremos uno o más nodos
  ejecutando el servicio <literal>nova-compute</literal>.</para>

  <para>El nodo que ejecuta el servicio nova-volume, en nuestro caso el nodo
  controlador aunque puede ser otro, debe tener una partición de unos 60 GiB o
  más de espacio de almacenamiento etiquetada del tipo "Linux LVM" (código
  <literal>0x8e</literal>).</para>

  <para>La topología de red para OpenStack, FlatDHCP en nuestro caso, debe estar
  configurada, así como la conectividad entre los nodos a través de
  TCP/IP.</para>

  <section xml:id="id-iscsi">
    <title>Sobre iSCSI</title>
    <para><literal>iSCSI, Internet Small Computer System Interface</literal> es
    un estándar de almacenamiento en red basado en TCP/IP, básicamente describe
    cómo transportar comandos SCSI sobre redes IP. El protocolo permite que
    clientes, <emphasis>initiators</emphasis> tal como los denomina la
    terminología iSCSI, puedan enviar comandos SCSI (CDBs) a dispositivos de
    almacenamiento de servidores remotos (targets).</para>

    <para>iSCSI es un protocolo de redes de almacenamiento SAN (Storage Area
    Network), una SAN permite a una organización consolidar el almacenamiento
    de su centro de datos en arrays o cabinas de discos que más tarde se presentan a
    los servidores que ofrecen servicios (web, bases de datos, ...). Dichos
    servidores obtienen el almacenamiento como si fuera local, iSCSI cuenta con
    las ventajas de ser más económico y de poder reutilizar la infraestructura
    de red, frente a otros protocolos más rápidos pero que necesitan de un
    hardware y cableado específico como Fibre Channel o Infiniband.</para>

    <para>iSCSI es un protocolo de nivel de aplicación que trabaja sobre TCP,
    normalmente los puertos 860 ó 3260. Básicamente, iSCSI simplemente permite a
    dos hosts negociar ciertos parámetros para poder después intercambiar
    comandos SCSI utilizando redes IP. De esta forma iSCSI es capaz de tomar un
    protocolo de alto rendimiento muy utilizado en buses de almacenamiento local
    y emularlo sobre redes de área extensa creando una red de almacenamiento
    (SAN). De forma diferente a otros protocolos utilizados en redes de
    almacenamiento, iSCSI no necesita de cableado especial ya que puede utilizar
    la infraestructura de red existente (interfaces de red, cableado, switches,
    etc.), por lo que a menudo se ve como una alternativa más económica a Fibre
    Channel (exceptuando FCoE, Fibre Channel sover Ethernet) o Infiniband. Sin
    embargo, el rendimiento de iSCSI en un despliegue SAN se puede verse
    reducido en gran medida si no se configura en una red o subred separada (LAN
    ó VLAN).</para>

    <para>Aunque iSCSI permite comunicar cualquier tipo de dispositivo SCSI,
    casi siempre se utiliza por los administradores de sistemas para permitir
    que determinados hosts que ofrecen servicios (web, FTP, bases de datos,
    servicios de virtualización, ...), puedan acceder a volúmenes de discos
    situados en arrays de almacenamiento. Las SAN iSCSI se diseñan normalmente
    con dos objetivos en mente:</para>

    <itemizedlist>
      <listitem>
	<para><emphasis role="bold">Consolidación de almacenamiento.</emphasis>
	Consiste en trasladar los diferentes recursos de almacenamiento a un
	lugar centralizado y más eficiente. Un entorno SAN proporciona un mayor
	grado de flexibilidad en el almacenamiento ya que es posible localizar
	nuevos volúmenes de discos para cualquier servidor en cualquier momento
	sin modificar nada ni en el cableado ni en la configuración hardware.</para>
      </listitem>
      
      <listitem>
	<para><emphasis role="bold">Recuperación de desastres.</emphasis>En un
	entorno SAN, los recursos de almacenamiento pueden fácilmente tanto
	migrarse, como replicarse (mirroring) de una localización a otra.</para>
      </listitem>

    </itemizedlist>

    <para>iSCSI viene definido en los RFCs 3720 y 3783.</para>
    
    <simplesect>
      <title>Conceptos importantes</title>
      <para>Entre los conceptos más importantes a la hora de trabajar con iSCSI,
      destacamos los siguientes:</para>

      <variablelist>
	<title>Conceptos iSCSI</title>
	<varlistentry>
	  <term>Initiator</term>
	  <listitem>
	    <para>Un initiator (iniciador) es un cliente iSCSI. Un initiator en
	    iSCSI cumple el mismo propósito que un adaptador SCSI
	    hardware,exceptuando el cableado de los dispositivos. Un initiator
	    SCSI se limita en enviar comandos SCSI a través de la red IP. Los
	    initiators pueden ser de dos tipos:</para>
	    <itemizedlist>
	      <listitem>
		<para>Los initiators software utilizan código para implementar
		el protocolo. Normalmente en drivers de dispositivo a nivel de
		kernel utilizando toda la pila de red del sistema
		operativo.</para>
		<para>Estos initiators están disponibles para los sistemas
		operativos más comunes y es el método más utilizado a la hora de
		desplegar soluciones iSCSI.</para>
	      </listitem>

	      <listitem>
		<para>Hardware. Un initiator hardware utiliza hardware dedicado,
		combinado normalmente con un determinado sofwtare (firmware)
		para implementar iSCSI. Este tipo de initiators reducen en gran
		medida la sobrecarga del procesamiento del protocolo sobre TCP y
		de las interrupciones Ethernet, por lo que aumentan el
		rendimiento global de los servidores a la hora de utilizar
		iSCSI.</para>

		<para>Un HBA (iSCSI Host Bus Adapter) es una tarjeta que
		implementa un initiator por hardware. Un HBA empaqueta en la
		misma tarjeta una interfaz Ethernet a Gigabit (o 10 Gigabits),
		una implementación TCP/IP y un adaptador SCSI, que es como se
		presenta al sistema operativo. Optativamente pueden incluir una
		ROM que permita al sistema arrancar desde una SAN.</para>
	      </listitem>
	      
	    </itemizedlist>
	  </listitem>
	</varlistentry>
	
	<varlistentry>
	  <term>Target</term>
	  <listitem>
	    <para>La especificación iSCSI define target como cada uno de los
	    recursos de almacenamiento que se localizan en un servidor.</para>

	    <para>Un target iSCSI es un dispositivo de almacenamiento conectado
	    a la red, pero también pueden serlo los dipositivos de
	    almacenamiento que un sistema operativo ofrezca a través de
	    iSCSI. De la misma forma que un sistema operativo puede implementar
	    un initiator por software, también puede implementar un
	    target. Estas solución está implementada en los sistemas operativos
	    más comunes como Linux, BSD, Solaris o Windows Server.</para>

	    <para>También existen ciertas distribuciones como FreeNAS, Openfiler
	    u OpenMediaVault especializadas en ofrecer soporte iSCSI.</para> 
	  </listitem>
	</varlistentry>

	<varlistentry>
	  <term>LUN</term>
	  <listitem>
	    <para>En terminología SCSI, un LUN (Logical Unit Number) es una dirección lógica. Un LUN
	    representa un dispositivo SCSI direccionable individualmente, que es
	    parte de un target (de un dispositivo físico SCSI). En un entorno
	    iSCSI un LUN representa una unidad de disco. Cuando se inicia la
	    negociacion initiator/target, se establece la conectividad hacia un
	    LUN, el resultado es una conexión iSCSI que simula una conexión con
	    un disco duro SCSI. Los initiators tratan los LUN iSCSI de la misma
	    forma que un disco SCSI físico o un disco IDE. Esto permite a los
	    sistemas iSCSI formatear y gestionar sistemas de ficheros
	    directamente sobre LUN iSCSI, en vez de montar de forma remota
	    directorios tal como lo hace NFS o CIFS.</para>
	    <para>En despliegues más grandes, los LUNs normalmente representan
	    "rodajas" (slices) de arrays de discos RAID, reservadas de forma
	    individual a los servidores cliente.</para>
	    <para>iSCSI no impone ningún tipo de regla o restricción sobre la
	    compartición de un LUN por varios servidores, el acceso concurente
	    por parte de varios servidores a un mismo sistema de ficheros es una
	    tarea del sistema operativo y que depende principalmente de que el
	    sistema de ficheros lo permita.</para>
	  </listitem>
	</varlistentry>

	<varlistentry>
	  <term>IQN</term>
	  <listitem>
	    <para>IQN, iSCSI Qualified Name. </para>
	    <para>Hay tres formas de especificar un nombre para los targets y os
	    initiators en un entorno iSCSI: IQN (iSCSI Qualified Name), EUI
	    (Extended Unique Identifier) y NAA(T11 Network Address
	    Authority).</para>
	    <para>IQN se describe en los RFC 3720 y 3721 y es la más utilizada,
	    consiste baśicamente en asignar a cada target e initiator una cadena
	    que contiene los siguientes campos:</para>
	    <itemizedlist>
	      <listitem>
		<para>El literal <literal>iqn</literal>.</para>
	      </listitem>

	      <listitem>
		<para>La fecha en la que se creó el dominio o se creó el
		target/initiator. Se especifica con el formato:
		<literal>yyyy-mm</literal></para>
	      </listitem>

	      <listitem>
		<para>Nombre del dominio DNS con los componentes en orden inverso.</para>
	      </listitem>

	      <listitem>
		<para>De forma optativa el carácter ':' usado como prefijo para
		una cadena de texto con el nombre asignado al almacenamiento.</para>
	      </listitem>	      
	    </itemizedlist>

	    <para>Los siguientes ejemplos se pueden obtener del RFC 3720:</para>
<programlisting>
                  Naming     String defined by
     Type  Date    Auth      "example.com" naming authority
+--++-----+ +---------+ +-----------------------------+
|  ||     | |         | |                             |     

iqn.1992-01.com.example:storage:diskarrays-sn-a8675309
iqn.1992-01.com.example
iqn.1992-01.com.example:storage.tape1.sys1.xyz
iqn.1992-01.com.example:storage.disk2.sys1.xyz[3]
</programlisting>
	  </listitem>
	</varlistentry>
	
      </variablelist>

    </simplesect>

    <simplesect>
      <title>Implementaciones iSCSI en GNU/Linux</title>
      <para>Desde el punto de vista del initiator, del cliente iSCSI que inicia
      las conexiones al target o servidor, hay una clara solución dominante y es
      la proporcionada por el proyecto <link
      xlink:href="http://www.open-iscsi.org">open-iscsi</link>. Soportada
      actualmente por las principales distribuciones está
      ampliamente aceptada como el referente en cuanto a implementaciones de
      initiators iSCSI en el mundo GNU/Linux.</para>

      <para>En cuanto a targets, las cosas son un poco más complicadas, al menos
      cuatro implementaciones FLOSS (Free/Libre Open Source Software) están
      disponibles para el almacenamiento sobre TCP/IP</para>

      <itemizedlist>
	<listitem>
	  <para><emphasis role="bold">IET</emphasis>, the iSCSI Enterprise
	  Target, es una implementación en espacio kernel disponible como módulo pero no
	  incluida aún en los fuentes oficiales del kernel de Linux.</para> <para>IET
	  tiene su origen en un fork GPL de una implementación iSCSI de la empresa
	  holandesa Ardis Technologies y es usada ampliamente por vendedores de soluciones
	  iSCSI de bajo coste. Varias distribuciones populares, incluyendo a Debian,
	  Ubuntu o SUSE, incluyen IET en sus repositorios oficiales, en Ubuntu es incluso
	  el target por defecto (paquete <literal>iscsitarget</literal>). Su desarrollo
	  continúa de forma activa siendo Scott Walker y Arne Redlich los principales
	  desarrolladores del proyecto.</para>
	</listitem>

	<listitem>
	  <para><emphasis role="bold">STGT</emphasis>, the Linux SCSI target
	  framework. Con STGT, el ex-desarrollador del proyecto IET, Fujita Tomonori,
	  intentó escribir un reemplazo multiprotocolo y en espacio de usuario de
	  IET.</para>
	  <para>Solo una porción muy pequeña del código de STGT se ejecuta en
	  espacio de kernel y fue incluida en la versión 2.6.20, el resto del
	  target se ejecuta en espacio de usuario. Red Hat adoptó rápidamente STGT
	  como su target iSCSI por defecto durante el lanzamiento de Red Hat
	  Enterprise Linux 5 y continúa actualmente su soporte en RHEL 6. SUSE
	  también adoptó esta implementación para su SUSE Linux Enterprise Server
	  (SLES) 11, oras distribuciones como Debian o Ubuntu también lo
	  incorporan en sus repositorios oficiales. Sin embargo, el activo desarrollo
	  de STGT parece que se ha detenido y actualmente el proyecto se encuentra
	  en un modo de solo mantenimiento.</para>
	</listitem>

	<listitem>
	  <para><emphasis role="bold">SCST</emphasis>, Generic SCSI Target
	  Subsystem for Linux. SCST, mantenido principalmente por Vladislav Bolkhovitin y
	  Bart van Assche es otro fork de IET con la meta principal de solucionar "todos
	  los problemas, casos límite y violaciones del estándar que IET tiene", tal como
	  indican en su página web. SCST tiene un gran número de devotos seguidores y
	  usuarios, que alaban los beneficios del mayor rendimiento de SCST en comparación
	  con el resto de implementaciones de targets.</para> <para>Contrariamente a STGT,
	  pero de la misma forma que IET, SCST realiza gran parte de su trabajo en espacio
	  de kernel, y sus desarrolladores han solicitado repetidamente su inclusión en la
	  rama principal del kernel de Linux. De momento y hasta ahora, dichos esfuerzos
	  no han tenido recompensa, parcialmente por la llegada de un nuevo cuarto
	  target: LIO.</para>
	</listitem>

	<listitem>
	  <para><emphasis role="bold">LIO</emphasis>, linux-iscsi.org, toma su
	  nombre del dominio al que el proyecto pertenece, su principal
	  desarrollador es Nicholas Bellinger. LIO es una implementación
	  genérica de target que se ejecuta en espacio de kernel, de la cual,
	  iSCSI es solamente uno de sus muchos frontends. Es el único que no
	  tiene nada que ver con ninguno de los otros tres y éste no es
	  únicamente su rasgo más destacado. LIO utiliza una aproximación muy
	  inusual basada en ConfigFS para su configuración, la cual produjo
	  varias polémicas en las listas de desarrollo del kernel. A pesar de
	  esto, a principio del 2011, LIO derrotó a SCST en la batalla de ser el
	  sustituto de STGT como subsistema de target preferido en la rama
	  principal del kernel de Linux. Por esta razón muchas distribuciones ya
	  han incluido a LIO en sus repositorios, como Ubuntu, pero otras aún
	  no.</para>
	</listitem>

      </itemizedlist>

      <para>Cualquiera de estas cuatro implementaciones sirven perfectamente para
      el cometido de construir una solución de almacenamiento estable, simple
      y completamente software libre basada en iSCSI.</para>
      
    </simplesect>

  </section>

  <section xml:id="id-vol-instalacion">
    <title>Instalación y configuración</title>
    <para>Para la instalación y configuración del servicio seguiremos los
    siguientes pasos:</para>

    <orderedlist>
      <listitem>
	<para>Instalaremos todo el software necesario en el controlador:</para>
	<screen><prompt>root@jupiter:~# </prompt><userinput>apt-get install lvm2 nova-volume</userinput></screen>
      </listitem>

      <listitem>
	<para>Configuramos LVM con un grupo de volúmenes denominado
	<literal>nova-volumes</literal>, para ello:</para>
	<orderedlist>
	  <listitem>
	    <para>Creamos una partición (de unos 60 GiB o más) etiquetada del
	    tipo "Linux LVM" (<literal>0x8e</literal>), supongamos que sea la
	    partición <filename>/dev/sda4</filename>.</para>
	  </listitem>

	  <listitem>
	    <para>Creamos el VG:</para>
	    <screen>
<prompt>root@jupiter:~# </prompt><userinput>pvcreate /dev/sda4</userinput>
<prompt>root@jupiter:~# </prompt><userinput>vgcreate nova-volumes /dev/sda4</userinput>
	    </screen>
	  </listitem>
	  
	</orderedlist>
      </listitem>

      <listitem>
	<para>Instalación y configuración de iSCSI: recordamos que todos los
	nodos de computación actúan como <emphasis>iSCSI initiators</emphasis>
	mientras que el controlador actúa como <emphasis>iSCSI
	target</emphasis>. Esto implica que los nodos de computación deben
	poder establecer conexiones con el controlador en el puerto TCP
	<literal>3260</literal>.</para>

	<para>Instalamos el software iSCSI en todos los nodos de
	computación:</para>

	<screen><prompt>root@io:~# </prompt><userinput>apt-get install open-iscsi</userinput></screen>

	<para>Como paso optativo podemos modificar el nombre del initiator, para
	ello modificamos el fichero
	<filename>/etc/iscsi/initiatorname.iscsi</filename> dejando el siguiente
	contenido:</para>
	<programlisting>
## DO NOT EDIT OR REMOVE THIS FILE!
## If you remove this file, the iSCSI daemon will not start.
## If you change the InitiatorName, existing access control lists
## may reject this initiator.  The InitiatorName must be unique
## for each iSCSI initiator.  Do NOT duplicate iSCSI InitiatorNames.
InitiatorName=iqn.2012-08.net.iescierva:initiator:io
	</programlisting>
	<para>Tan solo hay que cambiar la última parte con el nombre del nodo,
	tenemos que recordar que el nombre del initiator iSCSI debe ser único en
	toda nuestra instalación.</para>
      </listitem>

      <listitem>
	<para>Instalación y configuración iSCSI: instalamos el target en el
	controlador, como implementación elegimos STGT (Linux SCSI target
	framework). La implementación que aparece en la documentación de
	OpenStack, concretamente en el manual de administración es IET (the iSCSI Enterprise
	Target), pero el paquete de instalación <literal>nova-volume</literal>
	lleva a STGT (paquete <literal>tgt</literal>) como dependencia. Así que
	optamos por ésta última. El paquete se habrá instalado al instalar
	nova-volume, si no, lo hacemos a través de este comando:</para>

	<screen><prompt>root@jupiter:~# </prompt><userinput>apt-get install iscsitarget</userinput></screen>

	<warning>
	  <para>La documentación oficial de OpenStack instala el paquete
	  <literal>iscsitarget</literal> y su módulo del kernel correspondiente,
	  <literal>iscsitarget-dkms</literal>. Esto es un error ya que deja
	  el sistema con dos implementaciones distintas de targets iSCSI, con el
	  correspondiente conflicto de configuración, puertos, etc. Solo hay que
	  instalar uno, por lo que nos decantamos por TGT.</para>
	  <para><emphasis role="bold">No hay que instalar ni el paquete iscsitarget ni el
	  paquete iscsitarget-dkms</emphasis></para>
	</warning>

	<para>Reiniciamos el servicio en el controlador:</para>

	<screen><prompt>root@jupiter:~# </prompt><userinput>service tgt restart</userinput></screen>

	<para>Iniciamos también los servicios iSCSI en los nodos de
	computación:</para>

	<screen><prompt>root@jupiter:~# </prompt><userinput>service open-iscsi restart</userinput></screen>
      </listitem>

      <listitem>
	<para>Configuramos el servicio nova-volume modificando las siguientes
	directivas del fichero <filename>/etc/nova/nova.conf</filename> en el
	<emphasis role="bold">controlador</emphasis>:</para>
	<programlisting>
volume_group=nova-volumes
volume_name_template=volume-%08x
iscsi_helper=tgtadm
iscsi_ip_prefix=172.20.251
iscsi_ip_address=172.20.251.190
iscsi_target_prefix=iqn.2012-08.net.iescierva:jupiter:
	</programlisting>
      </listitem>

      <listitem>
	<para>Iniciamos el servicio:</para>
	<screen><prompt>root@jupiter:~# </prompt><userinput>service nova-volume	stop</userinput></screen>
	<screen><prompt>root@jupiter:~# </prompt><userinput>service nova-volume start</userinput></screen>

	<para>Comprobamos que el servicio está activo a través del
	comando:</para>
	<screen><prompt>root@jupiter:~# </prompt><userinput>nova-manage service	list</userinput>
Binary           Host                                 Zone             Status     State Updated_At
nova-scheduler   jupiter                              nova             enabled    :-)   2012-08-06 08:13:10
nova-consoleauth jupiter                              nova             enabled    :-)   2012-08-06 08:13:09
nova-cert        jupiter                              nova             enabled    :-)   2012-08-06 08:13:10
nova-compute     jupiter                              nova             enabled    :-)   2012-08-06 08:13:12
nova-volume      jupiter                              nova             enabled    :-)   2012-08-06 08:13:11
nova-network     jupiter                              nova             enabled    :-)   2012-08-06 08:13:13
nova-compute     io                                   nova             enabled    :-)   2012-08-06 08:13:13
	</screen>

      </listitem>

      <listitem>
	<para>Creamos y conectamos un volumen.</para>
	<para>Para crear un volumen necesitamos asignarle un nombre y un tamaño en
	GiB, podemos hacerlo a través del siguiente comando:</para>

	<screen><prompt>root@jupiter:~# </prompt><userinput>nova volume-create --display_name volumen00 5</userinput></screen>

	<para>Podemos ver el volumen recién creado y su estado a través del
	comando:</para>

	<screen><prompt>root@jupiter:~# </prompt><userinput>nova volume-list</userinput>
+----+-----------+--------------+------+-------------+-------------+
| ID |   Status  | Display Name | Size | Volume Type | Attached to |
+----+-----------+--------------+------+-------------+-------------+
| 1  | available | volumen00    | 5    | None        |             |
+----+-----------+--------------+------+-------------+-------------+
	</screen>

	<para>Una vez que el estado es 'disponible' (available), el volumen ya puede
	conectarse a una instancia. Al crear el volumen, podemos ver que se ha
	creado un nuevo volumen lógico:</para>
	<screen>
<prompt>root@jupiter:~# </prompt><userinput>lvs</userinput>
  LV              VG           Attr   LSize   Origin Snap%  Move Log Copy%  Convert
  volume-00000001 nova-volumes -wi-ao   5,00g
<prompt>root@jupiter:~# </prompt><userinput>lvdisplay</userinput>
  --- Logical volume ---
  LV Name                /dev/nova-volumes/volume-00000001
  VG Name                nova-volumes
  LV UUID                MHDJ7R-eeZP-dfCm-Tt81-gacq-4EV7-ZddL8s
  LV Write Access        read/write
  LV Status              available
  # open                 1
  LV Size                5,00 GiB
  Current LE             1280
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           252:0
	</screen>

	<para> Y que a nivel de iSCSI, el volumen también está disponible:</para> 
	<screen>
<prompt>root@jupiter:~# </prompt><userinput>tgtadm --lld iscsi --op show --mode target</userinput>
Target 1: iqn.2012-08.net.iescierva:jupiter:volume-00000001
    System information:
        Driver: iscsi
        State: ready
    I_T nexus information:
    LUN information:
        LUN: 0
            Type: controller
            SCSI ID: IET     00010000
            SCSI SN: beaf10
            Size: 0 MB, Block size: 1
            Online: Yes
            Removable media: No
            Readonly: No
	    Backing store type: null
	    Backing store path: None
            Backing store flags: 
        LUN: 1
            Type: disk
            SCSI ID: IET     00010001
            SCSI SN: beaf11
            Size: 5369 MB, Block size: 512
            Online: Yes
            Removable media: No
            Readonly: No
            Backing store type: rdwr
            Backing store path: /dev/nova-volumes/volume-00000001
            Backing store flags: 
    Account information:
    ACL information:
        ALL
	</screen>

	<para>Podemos conectar el volumen a una instancia a través del siguiente
	comando, para ello necesitamos:</para>
	<itemizedlist>
	  <listitem>
	    <para>El UID de la instancia, lo obtenemos a partir de los comandos
	    <command>nova list</command> y <command>nova show</command>, este
	    últimos solo si queremos obtener más detalles de la instancia y asegurarnos
	    de ella.</para>
	  </listitem>

	  <listitem>
	    <para>El identificador del volumen, lo obtenemos a través del comando
	    <command>nova volume-list</command>.</para>
	  </listitem>

	  <listitem>
	    <para>El dispositivo que verá la instancia que se ejecuta en el nodo de
	    computación. Normalmente un dispositivo de la forma
	    <filename>/dev/vdX</filename>. KVM nombra a los dispositivos de otra
	    forma, distinguiendo los discos de las máquinas virtuales (invitados),
	    <literal>/dev/vdX</literal>, de los dispositivos del anfitrión,
	    <literal>/dev/sdX</literal>.</para>
	  </listitem>
	  
	</itemizedlist>

	<para>El comando queda así:</para>

	<screen><prompt>root@jupiter:~# </prompt><userinput>nova volume-attach de701111-5b42-456a-b3ea-1609d49ebc2f 1 /dev/vdb</userinput></screen>

	<para>Tras la ejecución podemos ver comprobar varias cosas:</para>

	<itemizedlist>
	  <listitem>
	    <para>El estado del volumen ha cambiado:</para>
	    <screen>
<prompt>root@jupiter:~# </prompt><userinput>nova volume-list</userinput>
+----+--------+--------------+------+-------------+--------------------------------------+
| ID | Status | Display Name | Size | Volume Type |             Attached to              |
+----+--------+--------------+------+-------------+--------------------------------------+
| 1  | in-use | volumen00    | 5    | None        | de701111-5b42-456a-b3ea-1609d49ebc2f |
+----+--------+--------------+------+-------------+--------------------------------------+
	    </screen>
	  </listitem>

	  <listitem>
	    <para>Hay una sesión iSCSI iniciada:</para>
	    <screen>
<prompt>root@jupiter:~# </prompt><userinput>tgtadm --lld iscsi --op show --mode target</userinput>
Target 1: iqn.2012-08.net.iescierva:jupiter:volume-00000001
    System information:
        Driver: iscsi
        State: ready
    I_T nexus information:
        I_T nexus: 1
        Initiator: iqn.2012-08.net.iescierva:initiator:io
        Connection: 0
            IP Address: 172.20.251.191
    LUN information:
        LUN: 0
            Type: controller
            SCSI ID: IET     00010000
            SCSI SN: beaf10
            Size: 0 MB, Block size: 1
            Online: Yes
            Removable media: No
            Readonly: No
	    Backing store type: null
	    Backing store path: None
            Backing store flags: 
        LUN: 1
            Type: disk
            SCSI ID: IET     00010001
            SCSI SN: beaf11
            Size: 5369 MB, Block size: 512
            Online: Yes
            Removable media: No
            Readonly: No
            Backing store type: rdwr
            Backing store path: /dev/nova-volumes/volume-00000001
            Backing store flags: 
    Account information:
    ACL information:
        ALL
	    </screen>

	    <para>También podemos verlo en el nodo de computación, ya que tendrá una
	    sesión iSCSI abierta y un nuevo disco disponible:</para>
	    <screen>
<prompt>root@io:~# </prompt><userinput>iscsiadm -m session</userinput>
tcp: [1] 172.20.251.190:3260,1 iqn.2012-08.net.iescierva:jupiter:volume-00000001
<prompt>root@io:~# </prompt><userinput>lsscsi</userinput>
[4:2:0:0]    disk    SMC      SMC2108          2.12  /dev/sda
[7:0:0:0]    cd/dvd  KVM      vmDisk-CD        0.01  /dev/sr0
[8:0:0:0]    disk    KVM      vmDisk           0.01  /dev/sdb
[9:0:0:0]    storage IET      Controller       0001  -       
[9:0:0:1]    disk    IET      VIRTUAL-DISK     0001  /dev/sdc
<prompt>root@io:~# </prompt><userinput>fdisk -l /dev/sdc</userinput>

Disk /dev/sdc: 5368 MB, 5368709120 bytes
166 heads, 62 sectors/track, 1018 cylinders, total 10485760 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x00000000

Disk /dev/sdc doesn't contain a valid partition table
	    </screen>
	    <para>El comando <command>lsscsi</command> muestra como tipo "disk
	    IET" los dispositivos importados a través de iSCSI.</para>
	  </listitem>

	  <listitem>
	    <para>A la máquina virtual, se le presenta un nuevo dispositivo:</para>
	    <screen><prompt>root@server00:~# </prompt><userinput>fdisk -l /dev/vdb</userinput>

Disco /dev/vdb: 5368 MB, 5368709120 bytes
16 cabezas, 63 sectores/pista, 10402 cilindros, 10485760 sectores en total
Unidades = sectores de 1 * 512 = 512 bytes
Tamaño de sector (lógico / físico): 512 bytes / 512 bytes
Tamaño E/S (mínimo/óptimo): 512 bytes / 512 bytes
Identificador del disco: 0x00000000

El disco /dev/vdb no contiene una tabla de particiones válida
	    </screen>

	    <para>... y podemos hacer uso de él a través de los siguientes comandos,
	    una vez particionado el disco correctamente:</para>
	    <screen>
<prompt>root@server00:~# </prompt><userinput>mkfs.xfs /dev/vdb1</userinput>
meta-data=/dev/vdb1              isize=256    agcount=4, agsize=327616 blks
         =                       sectsz=512   attr=2, projid32bit=0
data     =                       bsize=4096   blocks=1310464, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0
log      =registro interno       bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =ninguno                extsz=4096   blocks=0, rtextents=0
<prompt>root@server00:~# </prompt><userinput>mkdir /doc</userinput>
<prompt>root@server00:~# </prompt><userinput>mount /dev/vdb1 /doc</userinput>
<prompt>root@server00:~# </prompt><userinput>df -h</userinput>
S.ficheros     Tamaño Usado  Disp Uso% Montado en
/dev/vda1        5,0G  1,1G  3,7G  23% /
udev             112M  8,0K  112M   1% /dev
tmpfs             48M  252K   48M   1% /run
none             5,0M     0  5,0M   0% /run/lock
none             120M     0  120M   0% /run/shm
/dev/vdb1        5,0G   33M  5,0G   1% /doc
	    </screen>

	    <note>
	      <para>Utilizando KVM como hipervisor, la máquina virtual no verá el
	      disco a no ser que se reinicie. Se puede evitar esto si cargamos en la
	      máquina virtual el módulo <literal>acpiphp</literal>. Este módulo
	      proporciona el driver ACPI para conexión en caliente de dispositivos
	      PCI. Podemos hacer el cambio permanente a través del comando:</para>
	      <screen><prompt>root@server00:~# </prompt><userinput>echo acpiphp >> /etc/modules</userinput></screen>
	    </note>
	  </listitem>
	  
	</itemizedlist>    

      </listitem>


    </orderedlist>

    
  </section>

  <section xml:id="id-vol-comandos">
    <title>Comandos Nova para la gestión de volúmenes</title>
    <para>Para la configuración y trabajo con volúmenes tendremos a nuestra
    disposición los siguientes comandos:</para>

    <itemizedlist>
      <listitem>
	<para><emphasis role="bold">nova volume-create</emphasis></para>
	<para>Añade un nuevo volumen.</para>
	<programlisting>
<![CDATA[
nova volume-create [--snapshot_id <snapshot_id>]
                   [--display_name <display_name>]
                   [--display_description <display_description>]
                   [--volume_type <volume_type>]
                   <size>
]]>  
	</programlisting>
	<para>Normalmente indicaremos un nombre y un tamaño en
	GiB. Ejemplos:</para>
	<screen>
<prompt>root@jupiter:~# </prompt><userinput>nova volume-create --display_name volumen01 5</userinput>
<prompt>root@jupiter:~# </prompt><userinput>nova volume-create --display_name volumen02 5</userinput>
	</screen>
	<para>El nombre no es necesario que sea único ya que internamente
	OpenStack le asigna otro identificador que sí lo es.</para>
      </listitem>
      
      <listitem>
	<para><emphasis role="bold">nova volume-attach</emphasis></para>
	<para>Conecta un volumen a una instancia (máquina virtual).</para>
	<programlisting>
<![CDATA[
nova volume-attach <server> <volume> <device>
]]>
	</programlisting>
	<para>Es necesario indicar:</para>
	<itemizedlist>
	  <listitem>
	    <para>La instancia a la que conectaremos el volumen, podemos obtener
	    su ID a través del comando <command>nova list</command>.</para>
	  </listitem>

	  <listitem>
	    <para>El volumen a conectar. Hay que indicar su identificador,
	    podemos obtenerlo a través del comando <command>nova volume-list.</command></para>
	  </listitem>

	  <listitem>
	    <para>El dispositivo que verá la máquina virtual. Los volúmenes se
	    exportan por iSCSI, por lo que el nodo de computación que contiene
	    la máquina virtual, verá un nuevo disco son su fichero de
	    dispositivo asociado de la forma <filename>/dev/sdX</filename>,
	    normalmente <literal>/dev/sdb</literal>. La máquina virtual lo verá
	    a través de KVM, a través de un fichero de dispositivo  de la forma
	    /dev/vdX, nomrmalmente <literal>/dev/vdb</literal>. Hay que tener
	    muy en cuenta a la hora de indicar este parámetro cuál es el
	    siguiente libre en la máquina virtual. El comando no funcionará si
	    el dispositivo ya existe, si lo hará aunque asigne otra letra si el
	    dispositivo que se pasa como parámetro no es el siguiente a asignar
	    en la máquina virtual.</para>
	  </listitem>
	  
	</itemizedlist>

	<para>Ejemplos:</para>
	<screen><prompt>root@jupiter:~# </prompt><userinput>nova volume-attach de701111-5b42-456a-b3ea-1609d49ebc2f 1 /dev/vdc</userinput></screen>
	<para>Este comando conecta el volumen 1 con la instancia indicada por el
	UUID. En la instancia el volumen aparecerá como /dev/vdc, siempre y
	cuando vda y vdb estén ocupados y vdc libre.</para>	
      </listitem>

      <listitem>
	<para><emphasis role="bold">nova volume-detach</emphasis></para>
	<para>Desconecta un volumen de una instancia (máquina virtual).</para>
	<programlisting>
<![CDATA[
nova volume-detach <server> <volume>
]]>  
	</programlisting>
	<para>Es necesario indicar:</para>
	<itemizedlist>
	  <listitem>
	    <para>La instancia a la que desconectaremos el volumen, podemos obtener
	    su ID a través del comando <command>nova list</command>.</para>
	  </listitem>

	  <listitem>
	    <para>El volumen a desconectar. Hay que indicar su identificador,
	    podemos obtenerlo a través del comando <command>nova volume-list.</command></para>
	  </listitem>
	  
	</itemizedlist>

	<para>Ejemplos:</para>
	<screen><prompt>root@jupiter:~#  </prompt><userinput>nova volume-detach de701111-5b42-456a-b3ea-1609d49ebc2f 1</userinput></screen>
      </listitem>

      <listitem>
	<para><emphasis role="bold">nova volume-delete</emphasis></para>
	<para>Elimina un volumen.</para>
	<programlisting>
<![CDATA[
nova volume-delete <volume>
]]>  
	</programlisting>
	<para>Es necesario indicar:</para>
	<itemizedlist>
	  <listitem>
	    <para>El volumen a eliminar. Hay que indicar su identificador,
	    podemos obtenerlo a través del comando <command>nova volume-list.</command></para>
	  </listitem>
	  
	</itemizedlist>

	<para>Ejemplos:</para>
	<screen><prompt>root@jupiter:~#  </prompt><userinput>nova volume-delete	1</userinput></screen>
	<para>El borrado es una operación costosa ya que, por razones de
	seguridad, nova-volume sobrescribe con ceros todos los bloques de datos
	físicos que conforman el volumen. Podemos acelerar el borrado
	sobrescribiendo únicamente el primer gigabyte, eliminando el MBR y el
	principio del sistema de ficheros, tal como se cuenta en:
	https://dijks.wordpress.com/2012/07/09/how-to-speedup-removal-of-a-volume/
	Basta realizar el siguiente cambio:</para>
	<itemizedlist>
	  <listitem>
	    <para>Editamos el fichero
	    <filename>/usr/lib/python2.7/dist-packages/nova/volume/driver.py</filename>
	    modificando la línea 131 con este cambio:</para>
	    <programlisting language="Python">
	      self._copy_volume('/dev/zero', self.local_path(volume), 1)
	    </programlisting>
	  </listitem>
	  <listitem>
	    <para>Podemos automatizar esto a través del siguiente
	    script:</para>
	    <programlisting language="Bash">
<![CDATA[
#!/bin/bash
# https://dijks.wordpress.com/2012/07/09/how-to-speedup-removal-of-a-volume/
# Function for easy string replacement
function sedeasy {
    sed -i "s/$(echo $1 | sed -e 's/\([[\/.*]\|\]\)/\\&/g')/$(echo $2 | sed -e 's/[\/&]/\\&/g')/g" $3
}

# Backup original file
cp /usr/lib/python2.7/dist-packages/nova/volume/driver.py\
/usr/lib/python2.7/dist-packages/nova/volume/driver.py.bak

# Change writing zero's to the whole file with only the first GB
sedeasy "self._copy_volume('/dev/zero', self.local_path(volume), size_in_g)"\
"self._copy_volume('/dev/zero', self.local_path(volume), 1)"\
/usr/lib/python2.7/dist-packages/nova/volume/driver.py
]]>
	    </programlisting>
	  </listitem>
	</itemizedlist>
      </listitem>

      <listitem>
	<para><emphasis role="bold">nova volume-list</emphasis></para>
	<para>Muestra un listado con todos los volúmenes e información sobre
	ellos.</para>
	<programlisting>
usage: nova volume-list
	</programlisting>
      </listitem>

      <listitem>
	<para><emphasis role="bold">nova volume-show</emphasis></para>
	<para>Muestra información detallada sobre un volumen.</para>
	<programlisting>
<![CDATA[[
usage: nova volume-show <volume>
]]>
	</programlisting>
	<para>Es necesario indicar:</para>
	<itemizedlist>
	  <listitem>
	    <para>El volumen a mostrar la información. Hay que indicar su identificador,
	    podemos obtenerlo a través del comando <command>nova volume-list.</command></para>
	  </listitem>
	  
	</itemizedlist>

	<para>Ejemplos:</para>
	<screen><prompt>root@jupiter:~#  </prompt><userinput>nova volume-show 1</userinput></screen>
      </listitem>

    </itemizedlist>
    
  </section>

  <section xml:id="id-vol-problemas">
    <title>Resolución de problemas</title>
    <para>Si por cualquier motivo, no se consigue la conexión entre los
    volúmenes y las instancias, podemos realizar ciertas comprobaciones para
    chequear que todo esté correcto. La fuente de información más importante son
    los ficheros de logs, concretamente:</para>
    <itemizedlist>
      <listitem>
	<para><filename>/var/log/nova/nova-volume.log</filename>, en el controlador.</para>
      </listitem>

      <listitem>
	<para><filename>/var/log/nova/nova-compute.log></filename>, en los nodos
	de computación.</para>
      </listitem>
      
    </itemizedlist>

    <para>Los errores más comunes pueden ser:</para>

    <itemizedlist>
      <listitem>
	<para><emphasis>Error: "Stderr: 'iscsiadm: No portal found.".</emphasis>
	Es un error de iSCSI, producido porque el nodo de computación no puede
	conectarse al target, normalmente por un error en la inicialización del
	subsistema iSCSI en el nodo controlador. También puede aparecer este
	error si se tienen instalados, por error, los dos paquetes que
	proporcionan una implementación target iSCSI: iscsitarget y tgt. Este
	último como dependencia del paquete nova-volume.</para>
	<para>Para solucionarlo nos tenemos que asegurar de que el subsistema
	iSCSI funciona correctamente. Nos tenemos que asegurar también de que
	solo uno de los paquetes anteriormente mencionados esté
	instalado.</para>
	<para>También se puede producir este error si todo está bien configurado
	pero el target del volumen en concreto no se está exportando. En ese
	caso debemos borrar las referencias al volumen de la tabla
	block_device_mapping de la base de datos de Nova.</para>
      </listitem>

      <listitem>
	<para><emphasis>Error: "Stderr: 'iscsiadm: Cannot resolve
	host...".</emphasis>Este error se produce cuando el nodo de computación no puede
	encontrar el nombre del servidor que proporciona los servicios
	nova-volume.</para> <para>Para solucionarlo nos tenemos que asegurar del
	correcto funcionamiento de la resolución de nombres, bien vía DNS o bien vía
	<filename>/etc/hosts</filename>.</para>
      </listitem>

      <listitem>
	<para><emphasis>Error: "No route to host".</emphasis> Este error puede
	estar causado por multitud de motivos, pero significa solo una cosa, el
	proceso <emphasis>iscsid</emphasis> (open-iscsi) no puede contactar con
	el servidor nova-volume (y por tanto con el target iSCSI). Desde el nodo
	de computación podemos comprobar la ejecución de este comando:</para>
	<screen><prompt>root@io:~# </prompt><userinput>telnet 172.20.254.190 3260</userinput></screen>
	<para>Siendo <literal>172.20.254.190</literal> la dirección IP del nodo
	y <literal>3260</literal> el puerto usado en iSCSI. Si el comando
	finaliza con un timeout, hay un problema de conectividad, debemos
	entonces chequear cortafuegos, comprobar conectividad a través del
	comando <command>ping</command>, <command>traceroute</command>,
	etc. Nos puede ayudar también comprobar si una operación
	<emphasis>discovery</emphasis> de iSCSI tiene éxito:</para>
	<screen><prompt>$ </prompt><userinput>iscsiadm --mode discovery --type sendtargets --portal 172.20.251.190</userinput></screen>
      </listitem>

      <listitem>
	<para><emphasis>Pérdida de conexión entre nova-volume y nova-error. Cómo
	recuperar a un estado limpio. </emphasis></para> 
	<para>Las desconexiones de red se producen, desde el punto de vista
	iSCSI, una pérdida de conexión implica la extracción física de un disco en el
	servidor (el cliente iSCSI). Si la instancia estaba utilizando el volumen cuando
	se produjo la desconexión, nova-volumen no podrá desconectar el volumen
	(<literal>detach</literal>)</para>
	<para>Para solucionar este problema y que todo vuelva a la "normalidad"
	seguimos estos pasos:</para>
	<orderedlist>
	  <listitem>
	    <para>Desde el nodo de computación eliminamos todas las sesiones
	    iSCSI activas (stalled), obtenemos el listado de todas ellas:</para>
	    <screen><prompt>root@callisto:~# </prompt><userinput>iscsiadm -m session</userinput></screen>
	    <para>... y eliminamos las problemáticas:</para>
	    <screen><prompt>root@callisto:~# </prompt><userinput>iscsiadm -m session-r &lt;ID&gt;> -u</userinput></screen>
	    <para>... siendo ID el identificador de la sesión a cerrar. Ejemplo:
	    podemos cerrar la sesión iSCSI 10 de esta salida:</para>
	    <programlisting>
tcp: [10] 172.20.251.190:3260,1 iqn.2012-08.net.iescierva:jupiter:volume-00000004	      
	    </programlisting>
	    <para>A través de este comando:</para>
	    <screen>
<prompt>root@callisto:~# </prompt><userinput>iscsiadm -m session -r 10 -u</userinput>
Logging out of session [sid: 10, target: iqn.2012-08.net.iescierva:jupiter:volume-00000004, portal: 172.20.251.190,3260]
Logout of [sid: 10, target: iqn.2012-08.net.iescierva:jupiter:volume-00000004, portal: 172.20.251.190,3260]: successful
	    </screen>
	  </listitem>

	  <listitem>
	    <para>A pesar de cerrar la sesión, desde el punto de vista de
	    nova-volume, el volumen sigue "en uso" tal como se muestra en la
	    salida de nova volume-list. Para delvolver el volumen al estado
	    "disponible" ejecutamos las siguientes sentencias MySQL sobre la
	    base de datos de Nova:</para>
	    <orderedlist>
	      <listitem>
		<para>Obtenemos el identificador del volumen y nos aseguramos de
		la información:</para>
		<screen>
<prompt>mysql> </prompt><userinput>select id, status, attach_status, instance_id, mountpoint from volumes;</userinput>
		</screen>
	      </listitem>

	      <listitem>
		<para>Reseteamos el volumen:</para>
		<screen>
<prompt>mysql> </prompt><userinput>update volumes set status="available", attach_status="detached", instance_id=NULL, mountpoint=NULL where id=5;</userinput>
		</screen>
	      </listitem>
	    </orderedlist>
	  </listitem>

	  <listitem>
	    <para>Ya podemos volver a conectar el volumen.</para>
	  </listitem>
	</orderedlist>
      </listitem>

      <listitem>
	<para><emphasis>Reinicio de nova-volume. </emphasis></para>
	<para>Si por cualquier motivo deseamos volver a restaurar el estado
	inicial de nova-volume debemos seguir los siguientes pasos:</para>
	<orderedlist>
	  <listitem>
	    <para>Nos aseguramos de que ninguna instancia esté usando ningún
	    volumen. Podemos desconectar los volúmenes a través del comando
	    <command>nova nova-detach</command>. Este paso no es obligatorio,
	    pero sí recomendable.</para>
	  </listitem>

	  <listitem>
	    <para>Detenemos el servicio iSCSI en los nodos de computación:</para>
	    <screen><prompt>root@io:~# </prompt><userinput>service open-iscsi stop</userinput></screen>
	  </listitem>

	  <listitem>
	    <para>Detenemos el servicio nova-volume y el servicio iSCSI en el
	    nodo controlador:</para>
	    <screen>
<prompt>root@jupiter:~# </prompt><userinput>service nova-volume stop</userinput>
<prompt>root@jupiter:~# </prompt><userinput>service tgt stop</userinput>
	    </screen>
	  </listitem>

	  <listitem>
	    <para>No debe quedar ningún target en el nodo controlador:</para>
	    <screen>
<prompt>root@jupiter:~# </prompt><userinput>service tgt stop</userinput>

	    </screen>
	    <para>Si quedara alguno, lo eliminamos:</para>
	    <screen>
<prompt>root@jupiter:~# </prompt><userinput>tgtadm --lld iscsi --op show --mode target</userinput>
<prompt>root@jupiter:~# </prompt><userinput>tgtadm --lld iscsi --op delete --mode target --tid=ID</userinput>
	    </screen>
	    <para>... siendo ID el identificador del target a eliminar.</para>
	  </listitem>

	  <listitem>
	    <para>Eliminamos todos los volúmenes lógicos LVM creados:</para>
	    <screen>
<prompt>root@jupiter:~# </prompt><userinput>lvremove /dev/nova-volumes/volume-00000001</userinput>
<prompt>root@jupiter:~# </prompt><userinput>lvremove /dev/nova-volumes/volume-00000002</userinput>
...
	    </screen>
	  </listitem>

	  <listitem>
	    <para>Eliminamos las siguientes entradas de la base de datos de
	    Nova:</para>
	    <screen>
<prompt>root@jupiter:~# </prompt><userinput>mysql-u root -p</userinput>
<prompt>mysql> </prompt><userinput>delete from iscsi_targets;</userinput>
<prompt>mysql> </prompt><userinput>delete from volume_metadata;</userinput>
<prompt>mysql> </prompt><userinput>delete from volume_type_extra_specs;</userinput>
<prompt>mysql> </prompt><userinput>delete from volume_types;</userinput>
<prompt>mysql> </prompt><userinput>delete from block_device_mapping;</userinput>
<prompt>mysql> </prompt><userinput>delete from volumes;</userinput>
	    </screen>
	  </listitem>

	  <listitem>
	    <para>Volvemos a iniciar el servicio iSCSI en los nodos de
	    computación:</para>
	    <screen><prompt>root@io:~# </prompt><userinput>service open-iscsi start</userinput></screen>
	  </listitem>

	  <listitem>
	    <para>Iniciamos el servicio iSCSI y el servicio nova-volume en el
	    nodo controlador:</para>
	    <screen>
<prompt>root@jupiter:~# </prompt><userinput>service tgt start</userinput>
<prompt>root@jupiter:~# </prompt><userinput>service nova-volume start</userinput>
	    </screen>
	  </listitem>
	  
	</orderedlist>

      </listitem>
      
    </itemizedlist>
  </section>

  <section xml:id="id-vol-backups">
    <title>Copias de seguridad de los volúmenes</title>
    <para>AQUIAQUIAQUIAQUIAQUIAQUIAQUIAQUIAQUIAQUIAQUIAQUIAQUIAQUI</para>
    
  </section>

  <section xml:id="id-drivers">
    <title>Controladores de Volúmenes (Volume Drivers)</title>
    <para>El comportamiento por defecto de nova-volume se puede alterar usando
    diferentes drivers para nova-volume. Estos drivers ya están incluidos en el
    código base de Nova, tan solo hay que seleccionarlos a través de la
    directiva <literal>volume_driver</literal> del fichero
    <filename>nova.conf</filename>. El valor por defecto es el siguiente:</para>
    <programlisting>
volume_driver=nova.volume.driver.ISCSIDriver
    </programlisting>

    <para>Pero Nova pone a disposición drivers para los siguientes backends de almacenamiento:</para>

    <itemizedlist>
      <listitem>
	<para>Ceph Rados block device (RBD).</para>
	<para>Si se usa KVM/QEMU como hipervisor, se puede configurar Nova para
	que use RBD para la gestión de volúmenes. Tan solo hay que añadir las
	siguientes directivas en los nodos que ejecuten
	<literal>nova-volume</literal>:</para>
	<programlisting>
volume_driver=nova.volume.driver.RBDDriver
rbd_pool=nova	  
	</programlisting>
      </listitem>

      <listitem>
	<para>Nexenta.</para>
	<para>El Sistema Operativo NexentaStor es una plataforma software
	NAS/SAN para la construcción de arrays de almacenamiento en red rápido y
	confiable, al estar basado en el sistema operativo OpenSolaris y usar
	ZFS como sistema de ficheros nativo. NexentaStor puede actuar como nodo
	de almacenamiento en una infraestructura OpenStack y proporcionar
	volúmenes a nivel de bloque para las instancias en ejecución a través
	del protocolo iSCSI.</para>
	<para>El driver de Nexenta permite utilizar Nexenta SA para almacenar
	volúmenes de Nova. Cada volumen Nova está representado a través de un
	único <literal>zvol</literal> en un volumen predefinido de Nexenta. Para
	cada nuevo volumen, el driver crea un target iSCSI y un grupo de targets
	que se utilizarán para el acceso por las instancias.</para>
	<para>Para el uso de Nexenta por Nova, hay que configurar los siguientes
	parámetros:</para>
	<itemizedlist>
	  <listitem>
	    <para><literal>volume_driver=nova.volume.nexenta.volume.NexentaDriver</literal></para>
	  </listitem>

	  <listitem>
	    <para><literal>nexenta_host</literal>: nombre del host o la
	    dirección IP del servidor NexentaStor.</para>
	  </listitem>

	  <listitem>
	    <para><literal>nexenta_user</literal> y
	    <literal>nexenta_password</literal>: usename y password del usuario
	    que cuente con todos los privilegios necesarios en el servidor,
	    incluyendo acceso a la API REST.</para>
	  </listitem>

	  <listitem>
	    <para><literal>nexenta_volume</literal>: nombre del volumen Nexenta
	    a utilizar, por defecto se utilizará <literal>nova</literal>.</para>
	  </listitem>
	  
	</itemizedlist>
	<para>Otros parámetros interesantes para modificar serían los
	siguientes:</para>
	<itemizedlist>
	  <listitem>
	    <para><literal>nexenta_target_prefix</literal>: prefijo añadido al
	    identificador de volumen para formar el nombre del target que
	    Nexenta ofrecerá.</para>
	  </listitem>

	  <listitem>
	    <para><literal>nexenta_target_group_prefix</literal>: prefijo para
	    el grupo de targets.</para>
	  </listitem>

	  <listitem>
	    <para><literal>nexenta_blocksize</literal>: tamaño del bloque de
	    datos a utilizar. Ejemplo: <literal>8K</literal></para>
	  </listitem>

	  <listitem>
	    <para><literal>nexenta_sparse</literal>: valor booleano que indica
	    si se usarán zvols libres para ahorrar espacio.</para>
	  </listitem>
	  
	</itemizedlist>

	<para>Los siguientes parámetros se pueden fijar en sus valores por
	defecto:</para>
	
	<itemizedlist>
	  <listitem>
	    <para><literal>nexenta_rest_port</literal>: puerto donde Nexenta
	    escucha las peticiones REST.</para>
	  </listitem>

	  <listitem>
	    <para><literal>nexenta_rest_protocol</literal>: puede ser
	    <literal>http</literal> o <literal>https</literal>, el valor por
	    defecto es <literal>auto</literal>.</para>
	  </listitem>

	  <listitem>
	    <para><literal>nexenta_iscsi_target_portal_port</literal>: puerto donde Nexenta
	    escucha las peticiones iSCSI.</para>
	  </listitem>
	  
	</itemizedlist>

      </listitem>

      <listitem>
	<para>Xen Storage Manager</para>
      </listitem>
      
    </itemizedlist>

  </section>

  <section xml:id="id-arranque-volumenes">
    <title>Iniciando una instancia desde un volumen</title>
    <para></para>
    
  </section>

</chapter>
