<?xml version="1.0" encoding="utf-8"?>
<chapter xmlns="http://docbook.org/ns/docbook"
         xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
         xml:id="ch-instalacion">
  <title>Instalación de OpenStack en GNU/Linux Ubuntu 12.04</title>

  <section xml:id="id_instUbuntu_intro">
    <title>Introducción</title>
    <para>Esta sección muestra el proceso detallado de instalación y
    configuración de OpenStack basado en la plataforma Ubuntu 12.04 (Precise
    Pangolin) usando 5 servidores (nodos). Se usará uno de los servidores como
    nodo controlador, ejecutando los componentes Nova, Glance, Swift, Keystone y
    Horizon y  el resto de nodos como nodos de computación, ejecutando
    únicamente Nova Compute.</para>

    <para>En los siguientes pasos se resume una visión global de todo el proceso:</para>

    <orderedlist>
      <listitem>
	<para>Instalación de servicios básicos (NTP, MySQL, ...).</para>
      </listitem>

      <listitem>
	<para>Instalación y configuración de Keystone (servicio de autenticación).</para>
      </listitem>

      <listitem>
	<para>Instalación y configuración de Glance (servicio de gestión de imágenes).(</para>
      </listitem>

      <listitem>
	<para>Instalación y configuración de Nova (servicios de computación).</para>
      </listitem>

      <listitem>
	<para>Configuración de la red del Cloud.</para>
      </listitem>

      <listitem>
	<para>Añadir imágenes para la creación de máquinas virtuales.</para>
      </listitem>

      <listitem>
	<para>Iniciar una máquina virtual de prueba.</para>
      </listitem>

      <listitem>
	<para>Instalación y configuración del Dashboard (Horizon).</para>
      </listitem>

    </orderedlist>

    <para>El proceso de instalación y configuración de OpenStack es un proceso
    complejo y muy propenso a errores, ya que se trata de un proyecto muy joven
    y bajo un desarrollo muy activo. Para obtener más información más allá de
    este documento se ruega visitar las páginas oficiales:</para>

    <itemizedlist>
      <listitem>
	<para><link xlink:href="http://www.openstack.org">OpenStack Site.</link></para>
      </listitem>
      
      <listitem>
	<para><link xlink:href="http://docs.openstack.org">OpenStack Docs site.</link></para>
      </listitem>
      
    </itemizedlist>

  </section>

  <section xml:id="id_instUbuntu_prerrequisitos">
    <title>Prerrequisitos</title>
    <para>Para el seguimiento del proceso de instalación y configuración de
    OpenStack, es necesario cumplir ciertos requisitos y partir de algunas
    suposiciones. Son las siguientes:</para>

    <itemizedlist>
      <listitem>
	<para>Se necesitan, al menos tres nodos con Ubuntu 12.04 LTS instalado.</para>
      </listitem>

      <listitem>
	<para>Uno de los nodos será el nodo controlador, que ejecutará todos los
	servicios excepto <literal>nova-compute</literal>.</para>
      </listitem>

      <listitem>
	<para>El esquema de particiones deberá usar LVM, se detalla la
	configuración en la siguiente sección.</para>
      </listitem>

      <listitem>
	<para>La resolución de nombres DNS para todas las máquinas debe ser
	perfecta.</para>
	<para>Nuestra red cuenta con un nombre de dominio, concretamente
	<literal>iescierva.net</literal>, este será el dominio utilizado durante
	todo el documento.</para>
      </listitem>

      <listitem>
	<para>Todos los nodos que participen en el Cloud deben tener fecha y
	hora sincronizada a través del servicio NTP.</para>
      </listitem>

      <listitem>
	<para>Se utilizará KVM como hipervisor.</para>
      </listitem>

      <listitem>
	<para>La contraseña para todos los usuarios y para todos los servicios
	será la misma: <literal>calex2010!!</literal></para>
      </listitem>


      <listitem>
	<para>El sistema deberá estar actualizado antes de empezar con el
	proceso de instalación/configuración:</para>
	<para><command>apt-get update</command></para>
	<para><command>apt-get upgrade</command></para>
      </listitem>
 
    </itemizedlist>
    
  </section>

  <section xml:id="id_instUbuntu_servBasicos">
    <title>Servicios y configuración básica</title>
    <para>A continuación se detallan los aspectos clave en la instalación de
    Ubuntu y lo servicios básicos a instalar.</para>

    <section>
      <title>Nombres de los equipos y configuración de la red</title>
      <para>Se han elegido los siguientes nombres para los equipos en los que se
      va a realizar la instalación de la infraestructura del Cloud:</para>
      <itemizedlist>
	<listitem>
	  <para><emphasis role="bold">jupiter</emphasis>: para el nodo controlador, que será el encargado
	  de gestionar todos los recursos del cloud, interaccionar con los clientes
	  y ordenar a los nodos de virtualización que ejecuten las instancias, pero
	  en el que no se ejecutarán máquinas virtuales. La mayor parte de
	  componentes del Cloud y configuración se realizará en este
	  equipo, pero comparado con los <emphasis>nodos de
	  computación</emphasis> la carga de trabajo será pequeña, por lo
	  que no es necesario un equipo con mucha memoria RAM o gran
	  capacidad de procesamiento.</para>
	</listitem>
	<listitem>
	  <para><emphasis role="bold">io, europa, ganimedes y calisto</emphasis> (las 4 lunas
	  principales de júpiter): para los 4 nodos de virtualización o
	  nodos de computación, como se les denomina habitualmente en la
	  jerga propia de OpenStack. En estos equipos se instalarán
	  sólo los componentes necesarios para que se ejecuten las
	  instancias (máquinas virtuales) en ellos y estarán esperando las órdenes de
	  jupiter.</para>
	</listitem>
	<listitem>
	  <para><emphasis role="bold">saturno</emphasis>: para el nodo de almacenamiento, ya que es tan
	  importante como júpiter y a la vez independiente. En este equipo todavía
	  no está definido el software que se instalará, pero lo más  probable es
	  que sea una distribución especializada en infraestructuras SAN/NAS como
	  <literal>OpenFiler</literal> ó <literal>FreeNAS</literal>.</para>
	</listitem>
	<listitem>
	  <para><emphasis role="bold">venus</emphasis>: un equipo convencional en el que se instalarán los
	  paquetes necesarios para usar el cliente <literal>nova</literal> con el que podemos
	  gestionar el cloud desde línea de comandos sin la necesidad de
	  realizar las operaciones desde jupiter.</para>
	</listitem>
      </itemizedlist>

      <para>Para la configuración de la red se ha optado por configurar bonding
      y redes VLAN tal como se describe en la siguiente figura:</para>

      <para>FIGURA BONDING y RED HACERRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR</para>
      
    </section>


    <section>
      <title>Instalación de Ubuntu 12.04.</title>
      <para>La máquina <literal>jupiter</literal> incluye dos discos duros
      SATAII de 500 GiB y una controladora de disco 3ware 9650SE. Se ha optado
      por configurar esta controladora de disco en modo RAID 1, para incluir un
      nivel elemental de seguridad y consistencia de datos, aunque obviamente
      esto no descarta la utilización adicional de otros mecanismos de copias de
      seguridad que se configurarán posteriormente. Al tratarse de una
      controladora RAID hardware y estar configurada previamente, el sistema
      operativo que arranque en el equipo sólo verá un disco duro en
      <literal>/dev/sda</literal> de 500GB aproximadamente.</para>

      <para>Para prevenir corrupción de datos es muy importante que la
      controladora RAID se configure con una política de escritura
      <literal>Write Through</literal>. En esta configuración, el rendimiento
      del RAID se resiente un poco, pero lo hace más inmune a una posible pérdida de datos
      en caso de cortes en el suministro eléctrico.</para>

      <para>El sistema operativo elegido para los equipos del cloud
      es la distribución de GNU/Linux Ubuntu 12.04 LTS, que actualmente se
      conoce con el nombre de código <emphasis>Precise
      Pangolin</emphasis>. Pensamos que es la mejor opción entre todas las
      distribuciones de GNU/Linux ya que es la descrita en toda la documentación
      de OpenStack y la mejor soportada.</para>

      <para>Durante la instalación realizamos el siguiente esquema de
      particionamiento:</para>
      <programlisting>
Disposit.   Tamaño    Id    Sistema     SF
/dev/sda1    32GiB    83    Linux       ext4
/dev/sda2     4GiB    82    Linux swap  intercambio
/dev/sda3  -resto-    8e    Linux LVM   volúmenes físicos
      </programlisting>

      <para>Crearemos tres particiones, una partición de unos 32 GiB para el
      sistema (directorio <literal>/</literal>), una partición de 4 GiB como
      área de intercambio, y el resto configurado para su uso a través de
      volúmenes lógicos con LVM.</para>

      <para>Tras la instalación del sistema, configuraremos el subsistema LVM
      pero sin llegar a crear ningún volumen:</para>
      <programlisting>
COMPLETAR
      </programlisting>

      <para>Aunque la controladora RAID es hardware y el sistema
        operativo no la gestiona, es importante que se pueda controlar
        su estado a través de alǵun módulo del kernel. En este caso el
        módulo 3w-9xxx que se carga automáticamente y nos envía estos
        mensajes al log del sistema:</para>
        <programlisting>
[    2.799824] 3ware 9000 Storage Controller device driver for Linux v2.26.02.014.
[    2.799867] 3w-9xxx 0000:01:00.0: PCI INT A -> GSI 19 (level, low) -> IRQ 19
[    2.799880] 3w-9xxx 0000:01:00.0: setting latency timer to 64
[    3.080257] 3w-9xxx: scsi6: Found a 3ware 9000 Storage Controller at 0xfe8df000, IRQ: 19.
[    3.416117] 3w-9xxx: scsi6: Firmware FE9X 4.08.00.006, BIOS BE9X 4.08.00.001, Ports: 2.
[   12.307881] 3w-9xxx: scsi6: ERROR: (0x03:0x0101): Invalid command opcode:opcode=0x85.
[   12.308300] 3w-9xxx: scsi6: ERROR: (0x03:0x0101): Invalid command opcode:opcode=0x85.
[   12.308695] 3w-9xxx: scsi6: ERROR: (0x03:0x0101): Invalid command opcode:opcode=0x85.
[127301.671991] 3w-9xxx: scsi6: AEN: INFO (0x04:0x0029): Verify started:unit=0.
[131817.864145] 3w-9xxx: scsi6: AEN: INFO (0x04:0x002B): Verify completed:unit=0.
        </programlisting>           
        <para>Pendiente de hacer:</para>
        <para>http;//jonas.genannt.name/  Hay repositorio, con
        aplicaciones para manejar el RAID 3ware 9650SE</para>
    </section>

    <section>
      <title>NTP</title>
      <para>Para mantener todos los servicios sincronizados (a nivel de fecha y
      hora) es necesario instalar un cliente NTP (Network Time Protocol). En el
      caso de instalaciones multinodo hay que configurar uno de los nodos como
      servidor NTP, o confiar en otro servidor de nuestra red o de
      Internet.</para>

      <para>La red <literal>iescierva.net</literal> ya cuenta con un servidor
      NTP, por lo que únicamente hay que configurar correctamente el cliente,
      para ello basta con que sigamos los siguientes pasos:</para>

      <orderedlist>
	<listitem>
	  <para>Nos aseguramos que el paquete <literal>ntpdate</literal> esté
	  instalado.</para>
	  <screen><prompt>root@jupiter:~# </prompt><userinput>dpkg --list | grep
	  ntpdate</userinput></screen>
	  <para>Si no lo estuviera lo instalamos:</para>
	  <screen><prompt>root@jupiter:~# </prompt><userinput>apt-get install ntpdate</userinput></screen>

	</listitem>
	
	<listitem>
	  <para>Configuramos crontab para que se ejecute el comando ntpdate de
	  forma periódica. Para ello ejecutamos (como root) el comando
	  <command>crontab -e</command> y editamos el fichero que nos sugiere
	  con el siguiente contenido:</para>
	  <programlisting>
0 4 * * * ntpdate ntp.iescierva.net ; hwclock --systohc
	  </programlisting>
	  <para>Podemos sustituir el servidor <literal>ntp.iescierva.net</literal> por uno en
	  Internet como <literal>ntp.ubuntu.com</literal>.</para>

	  <para>Podemos comprobar la configuración a través del comando
	  <command>crontab -l</command></para>

	</listitem>

      </orderedlist>
      
    </section>

    <section>
      <title>MySQL</title>
      <para>Vamos a configurar todos los servicios para que utilicen como base
      de datos MySQL, en vez de la base de datos SQLite que usan por
      defecto. Para ello será necesario instalar MySQL y fijar una contraseña
      para el usuario root. Para ello basta que sigamos los siguientes
      pasos:</para>

      <orderedlist>
	<listitem>
	  <para>Instalamos los paquetes necesarios, básicamente el servidor
	  MySQL, y la interfaz (DB-API) de Python para MySQL:</para>
	  <screen><prompt>root@jupiter:~# </prompt><userinput>apt-get install mysql-server python-mysqldb</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Durante la instalación del servidor se nos pedirá que
	  introduzcamos la contraseña para el usuario root de
	  MySQL.</para>
	</listitem>

	<listitem>
	  <para>Modificamos la interfaz de escucha de MySQL, aunque inicialmente
	  la fijamos a 0.0.0.0 para que el servidor escuche en todas las
	  interfaces, posteriormente fijaremos la interfaz con la IP de la red
	  privada.</para>

	  <para>Configuramos el fichero <filename>/etc/mysql/my.cnf</filename>
	  modificando la siguiente línea:</para>
	  <programlisting>
bind-address        = 127.0.0.1
	  </programlisting>

	  <para>Por esta otra:</para>
	  <programlisting>
bind-address        = 0.0.0.0
	  </programlisting>
	</listitem>

	<listitem>
	  <para>Reiniciamos el demonio de MySQL:</para>
	  <screen><prompt>root@jupiter:~# </prompt><userinput>service mysql restart</userinput></screen>
	</listitem>
	
	<listitem>
	  <para>Probamos el cliente MySQL con la contraseña fijada:</para>
	  <screen><prompt>root@jupiter:~# </prompt><userinput>mysql -u root -p</userinput></screen>
	</listitem>
      </orderedlist>
     
    </section>

  </section>

  <section xml:id="id_instUbuntu_keystone">
    <title>Instalación de KeyStone</title>
    <para>En este apartado describiremos la instalación y configuración de unos
    de los servicios básicos de OpenStack, el servicio de autenticación y
    gestión de la identidad (Identity Service): Keystone</para>

    <section>
      <title>¿Qué es Keystone?</title>
      <para>Keystone, es el componente de OpenStack encargado de la
      autentificación y la autorización de los distintos componentes que
      conforman OpenStack. Se incluyó desde la versión Essex y es el encargado
      de dos tareas principales:</para>

      <para>REVISARRRRRRRRRRRRRRRRRRRRRRRRRRRRR</para>
      <itemizedlist>
	<listitem>
	  <para>Gestión de usuarios: Keystone es el encargado de
	  mantener un registro de usuarios y los permisos que tiene cada 
	  uno de ellos.</para>
	</listitem>
	<listitem>
	  <para>Registro los servicios ofrecidos: Keystone ofrece un catálogo de
	  los servicios ofrecidos, así como información sobre cómo acceder a sus
	  APIs.</para>
	</listitem>
      </itemizedlist>

      <para>Los componentes básicos de los servicios de identidad son:</para>
      <itemizedlist>
	<listitem>
	  <para>Usuario: Podemos guardar su nombre, 
	  correo electrónico y contraseña.</para>
	</listitem>
	<listitem>
	  <para>Proyecto (<emphasis>tenant</emphasis> en la jerga de
	  OpenStack): En un proyecto podemos ejecutar un conjunto de
	  instancias con características en común, por ejemplo pueden estar
	  todas las instancias en el misma red, pueden utilizar una serie de
	  imágenes de sistemas o tener limitado el uso de recursos del
	  cloud.</para>
	</listitem>
	<listitem>
	  <para>Rol: Nos indica qué operaciones puede realizar
	  cada usuario. A un usuario se le pueden asignar diferentes roles
	  en cada proyecto.</para>
	</listitem>
      </itemizedlist>

      <para>Los conceptos fundamentales del <emphasis>registro de
      servicio</emphasis> son:</para>
      <itemizedlist>
	<listitem><para>Servicio: Corresponde a un componente de
	OpenStack que puede utilizar el módulo de
	autentificación.</para></listitem>
	<listitem><para>Endpoints: Representa las URL que nos permiten
	acceder a las API de cada uno de los servicios o componentes de
	OpenStack</para></listitem>
      </itemizedlist>
      
    </section>

    <section>
      <title>Instalación y configuración</title>
      <para>Una infraestructura OpenStack solo necesita un servidor que ejecute
      el servicio KeyStone, en nuestra instalación el servicio se ejecutará en
      el controlador, es decir, en la máquina <literal>jupiter</literal>.</para>

      <para>Para la instalación de Keystone, como para la instalación del resto
      de servicios, partiremos de los repositorios oficiales de Ubuntu, basta
      con que sigamos los siguientes pasos:</para>

      <orderedlist>
	<listitem>
	  <para>Creamos la base de datos que el servicio necesita en
	  MySQL. Para ello iniciamos el cliente MySQLs:</para>
	  <screen><prompt>root@jupiter:~# </prompt><userinput>mysql -u root -p</userinput>
	  </screen>
	  <para>Desde el prompt de MySQL ejecutamos las siguientes
	  sentencias:</para>
	  <screen>
	    <prompt>mysql> </prompt><userinput>CREATE DATABASE keystone;</userinput>
	    <prompt>mysql> </prompt><userinput>GRANT ALL ON keystone.* to 'keystone'@'%' IDENTIFIED BY 'calex2010!!';</userinput>
	    <prompt>mysql> </prompt><userinput>FLUSH PRIVILEGES;</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Instalamos todo el software necesario:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>apt-get install keystone python-keystone python-keystoneclient</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Keystone utiliza por defecto una base de datos <literal>SQLite</literal>,
	  por lo que no tendremos que borrar la base de datos que va con la
	  instalación por defecto:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>rm /var/lib/keystone/keystone.db</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Editamos el fichero de configuración principal de Keystone para
	  realizar los siguientes cambios:</para>
	  <itemizedlist>
	    <listitem>
	      <para>La cadena de conexión a la base de datos
	      <literal>keystone</literal> de MySQL.</para>
	    </listitem>

	    <listitem>
	      <para>El token administrativo (admin_token).</para>
	    </listitem>   
	  </itemizedlist>

	  <para>Para ello editamos el fichero
	  <filename>/etc/keystone/keystone.conf</filename> haciendo las
	  siguientes modificaciones:</para>
	  <programlisting>
admin_token = CALEX2010!!
connection = mysql://keystone:calex2010!!@172.20.254.190/keystone
	  </programlisting>
	  <para>Ver nota posterior sobre <literal>admin_token</literal>)</para>
	</listitem>

	<listitem>
	  <para>Reiniciamos el servicio:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>service keystone restart</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Creamos la base de datos inicial:</para>
	  <screen><prompt>root@jupiter:~# </prompt><userinput>keystone-manage db_sync</userinput></screen>
	</listitem>

	<listitem>
	  <para>Exportamos las siguientes variables de entorno en el fichero
	  .bashrc del usuario que ejecute el cliente, en principio el usuario
	  <literal>root</literal> de la máquina
	  <literal>jupiter</literal>:</para>
	  <programlisting language="Bash">
export SERVICE_ENDPOINT="http://172.20.254.190:35357/v2.0"
export SERVICE_TOKEN=CALEX2010!!
	  </programlisting>
	</listitem>

	<listitem>
	  <para>Creamos los usuarios, proyectos (tenants) y roles.</para>

	  <para>Crearemos ciertos usuarios a los que daremos ciertos roles en
	  ciertos proyectos, tal como se muestra en la siguiente tabla:</para>

	  <table rules="all">
	    <caption>Usuarios, proyectos y roles</caption>
	    <col width="34%"/>
	    <col width="33%"/>
	    <col width="33%"/>
	    <thead>
	      <tr>
		<td>Usuarios</td>
		<td>Proyectos (tenants)</td>
		<td>Roles</td>
	      </tr>
	    </thead>
	    <tbody>
	      <tr>
		<td>admin</td>
		<td>admin</td>
		<td>admin</td>
	      </tr>
	      <tr>
		<td>
		  <para>nova</para>
		  <para>glance</para>
		  <para>swift</para>
		</td>
		<td>service</td>
		<td>admin</td>
	      </tr>
	      <tr>
		<td>admin</td>
		<td>admin</td>
		<td>Member</td>
	      </tr>
	    </tbody>
	  </table>

	  <para>En la tabla podemos leer que al usuario <literal>admin</literal>
	  le daremos el rol <literal>admin</literal> en el proyecto
	  <literal>admin</literal>, o que a los usuarios <literal>nova</literal>,
	  <literal>glance</literal> y <literal>swift</literal> les daremos el
	  rol <literal>admin</literal> en el proyecto <literal>service</literal>.</para>

	  <para>La creación de usuarios, proyectos y roles es un proceso
	  tedioso, muy repetitivo y propenso a errores, por lo que ejecutaremos
	  el siguiente script que nos simplificará mucho este proceso:</para>

	  <programlisting language="Bash">
	    <xi:include parse="text" href="src/roles.sh"/>
	  </programlisting>

	  <para>Para la correcta ejecución de script hay que configurar las
	  siguientes variables de entorno en el inicio del script:</para>

	  <para>COMPLETARRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR</para>

	  <para>El script se limita a:</para>

	  <itemizedlist>
	    <listitem>
	      <para>Crear los proyectos necesarios.</para>
	    </listitem>

	    <listitem>
	      <para>Crear todos los usuarios y asignarles contraseña y
	      dirección de correo electrónico.</para>
	    </listitem>

	    <listitem>
	      <para>Crear los roles necesarios.</para>
	    </listitem>

	    <listitem>
	      <para>Asignar roles a los usuarios en los proyectos.</para>
	    </listitem>

	    <listitem>
	      <para>Crear los servicios y asignar a éstos los endpoints.</para>
	    </listitem>
	    
	  </itemizedlist>

	</listitem>

	<listitem>
	  <para>Verificamos que todo se ha creado correctamente a través de los
	  siguientes comandos (los identificadores no serán los mismos):</para>
	  <programlisting>
root@jupiter:~# keystone tenant-list
+----------------------------------+---------+---------+
|                id                |   name  | enabled |
+----------------------------------+---------+---------+
| 634675c752634b53879656c81da70a83 | service | True    |
| e7b1868b24a742318f1d73f612ecfe1d | admin   | True    |
+----------------------------------+---------+---------+
root@jupiter:~# keystone user-list
+----------------------------------+---------+--------------------+--------+
|                id                | enabled |       email        |  name  |
+----------------------------------+---------+--------------------+--------+
| 05743001bbf14700bcdf2ecc43edbf9b | True    | alex@iescierva.net | admin  |
| 246ba4e3d81c4ae8bdde8ec5784e74d3 | True    | alex@iescierva.net | swift  |
| 291c58f7258747758d109ecee2eb4a69 | True    | alex@iescierva.net | glance |
| 404dafc53b364a7e9f1476aa98082966 | True    | alex@iescierva.net | nova   |
+----------------------------------+---------+--------------------+--------+
root@jupiter:~# keystone role-list
+----------------------------------+--------+
|                id                |  name  |
+----------------------------------+--------+
| 2a716d669ce349eb88e44049723e0cb7 | admin  |
| 622c78f77bbe4a298452783f25cb4635 | Member |
+----------------------------------+--------+
root@jupiter:~# keystone endpoint-list
+----------------------------------+----------+--------------------------------------------------+--------------------------------------------------+---------------------------------------------+
|                id                |  region  |                    publicurl                     |                   internalurl                    |                   adminurl                  |
+----------------------------------+----------+--------------------------------------------------+--------------------------------------------------+---------------------------------------------+
| 0dd9427a0a624b38abf1876651ac3b69 | myregion | http://172.20.254.190:8776/v1/$(tenant_id)s      | http://172.20.253.190:8776/v1/$(tenant_id)s      | http://172.20.253.190:8776/v1/$(tenant_id)s |
| 2d471f9b23c84bac91c17615e157bb9f | myregion | http://172.20.254.190:5000/v2.0                  | http://172.20.253.190:5000/v2.0                  | http://172.20.253.190:35357/v2.0            |
| c9f799e2c6444bbda40dbc059207de09 | myregion | http://172.20.254.190:8773/services/Cloud        | http://172.20.253.190:8773/services/Cloud        | http://172.20.253.190:8773/services/Admin   |
| e72307edc9b84c2d9403b019c3a2e5a7 | myregion | http://172.20.254.190:9292/v1                    | http://172.20.253.190:9292/v1                    | http://172.20.253.190:9292/v1               |
| ec8b9d8d41ea496492730b20e1c34380 | myregion | http://172.20.254.190:8080/v1/AUTH_$(tenant_id)s | http://172.20.253.190:8080/v1/AUTH_$(tenant_id)s | http://172.20.253.190:8080/v1               |
| edf05355fc274c02a13f900605c56769 | myregion | http://172.20.254.190:8774/v2/$(tenant_id)s      | http://172.20.253.190:8774/v2/$(tenant_id)s      | http://172.20.253.190:8774/v2/$(tenant_id)s |
+----------------------------------+----------+--------------------------------------------------+--------------------------------------------------+---------------------------------------------+
root@jupiter:~# keystone service-list
+----------------------------------+----------+--------------+----------------------------+
|                id                |   name   |     type     |        description         |
+----------------------------------+----------+--------------+----------------------------+
| 04b121f16e6045a79a4601c5171998d3 | glance   | image        | OpenStack Image Service    |
| 298de2e4a0f04b42b54818fd57d1bf2e | keystone | identity     | OpenStack Identity Service |
| 368b238c02734ee4ac1a6ac6bf440ffa | nova     | compute      | OpenStack Compute Service  |
| a075b6add5564f729253dd0339b1d5d9 | volume   | volume       | OpenStack Volume Service   |
| c23937017a7e4baa8811a24ff9c8086c | swift    | object-store | OpenStack Storage Service  |
| e998168d3d264e3aab37d4b8413fe736 | ec2      | ec2          | OpenStack EC2 Service      |
+----------------------------------+----------+--------------+----------------------------+
root@jupiter:~# 
	  </programlisting>
	</listitem>
      </orderedlist>
    </section>
  </section>

  <section xml:id="id_instUbuntu_glance">
    <title>Instalación de Glance</title>
    <para></para>

    <section>
      <title>¿Qué es Glance?</title>
      <para></para>
      
    </section>

    <section>
      <title>Instalación y configuración</title>
      <para></para>
      
    </section>    
  </section>

<!-- Falta Nova y Horizon -->

</chapter>
