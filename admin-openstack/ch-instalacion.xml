<?xml version="1.0" encoding="utf-8"?>
<chapter xmlns="http://docbook.org/ns/docbook"
         xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
         xml:id="ch-instalacion">
  <title>Instalación de OpenStack en GNU/Linux Ubuntu 12.04</title>

  <section xml:id="id_instUbuntu_intro">
    <title>Introducción</title>
    <para>Esta sección muestra el proceso detallado de instalación y
    configuración de OpenStack basado en la plataforma Ubuntu 12.04 (Precise
    Pangolin) usando 5 servidores (nodos). Se usará uno de los servidores como
    nodo controlador, ejecutando los componentes Nova, Glance, Swift, Keystone y
    Horizon y  el resto de nodos como nodos de computación, ejecutando
    únicamente Nova Compute.</para>

    <para>Podemos resumir el proceso global de instalación en los siguientes
    pasos:</para>

    <orderedlist>
      <listitem>
	<para>Configuración de la red del Cloud.</para>
      </listitem>

      <listitem>
	<para>Instalación de servicios básicos (NTP, MySQL, ...).</para>
      </listitem>

      <listitem>
	<para>Instalación y configuración de Keystone (servicio de autenticación).</para>
      </listitem>

      <listitem>
	<para>Instalación y configuración de Glance (servicio de gestión de imágenes).(</para>
      </listitem>

      <listitem>
	<para>Instalación y configuración de Nova (servicios de computación).</para>
      </listitem>

      <listitem>
	<para>Añadir imágenes para la creación de máquinas virtuales.</para>
      </listitem>

      <listitem>
	<para>Iniciar una máquina virtual de prueba.</para>
      </listitem>

      <listitem>
	<para>Instalación y configuración del Dashboard (Horizon).</para>
      </listitem>

    </orderedlist>

    <para>El proceso de instalación y configuración de OpenStack es un proceso
    complejo y muy propenso a errores, ya que se trata de un proyecto muy joven
    y bajo un desarrollo muy activo. Para obtener más información más allá de
    este documento se ruega visitar las páginas oficiales:</para>

    <itemizedlist>
      <listitem>
	<para><link xlink:href="http://www.openstack.org">OpenStack Open Source
	Cloud Computing Software.</link></para>
      </listitem>
      
      <listitem>
	<para><link xlink:href="http://docs.openstack.org">OpenStack Docs: Essex.</link></para>
      </listitem>

      <listitem>
	<para><link xlink:href="https://launchpad.net/openstack/">OpenStack in Launchpad.</link></para>
      </listitem>
      
    </itemizedlist>

  </section>

  <section xml:id="id_instUbuntu_prerrequisitos">
    <title>Prerrequisitos</title>
    <para>Para el seguimiento del proceso de instalación y configuración de
    OpenStack que se detalla posteriormente, es necesario partir de algunas suposiciones y de cumplir ciertos
    requisitos. Como los siguientes:</para>

    <itemizedlist>
      <listitem>
	<para>Se necesitan, al menos tres nodos con Ubuntu 12.04 LTS instalado.</para>
      </listitem>

      <listitem>
	<para>Uno de los nodos será el nodo controlador, que ejecutará todos los
	servicios excepto <literal>nova-compute</literal>.</para>
      </listitem>

      <listitem>
	<para>Se recomienda el uso de LVM (el gestor de volúmenes lógicos de
	Linux), la configuración de LVM y el listado del esquema de
	particionamiento a seguir, se detalla en las siguientes
	secciones.</para>
      </listitem>

      <listitem>
	<para>La resolución de nombres DNS para todas las máquinas debe ser
	perfecta.</para>
	<para>Nuestra red cuenta con un nombre de dominio, concretamente
	<literal>iescierva.net</literal>, este será el dominio utilizado durante
	todo el documento.</para>
	<para>Si no se cuenta con un servidor DNS en la red, todas las máquinas
	deberán contar con el fichero <filename>/etc/hosts</filename> con todas
	las máquinas correctamente registradas.</para>
      </listitem>

      <listitem>
	<para>Todos los nodos que participen en el Cloud deben tener la fecha y
	hora sincronizada a través del servicio NTP (Network Time Protocol). Se
	recomienda que la red cuente con uno varios servidores NTP.</para>
      </listitem>

      <listitem>
	<para>Entre todos los hipervisores disponibles en la comunidad FLOSS,
	hemos optado por KVM.</para>
      </listitem>

      <listitem>
	<para>La contraseña para todos los usuarios y para todos los servicios
	será la misma: <literal>calex2010!!</literal></para>
      </listitem>

      <listitem>
	<para>El sistema deberá estar actualizado antes de empezar con el
	proceso de instalación/configuración:</para>
	<para><command>apt-get update</command></para>
	<para><command>apt-get upgrade</command></para>
      </listitem>
 
    </itemizedlist>
    
  </section>

  <section xml:id="id_instUbuntu_servBasicos">
    <title>Servicios y configuración básica</title>
    <para>A continuación se detallan los aspectos clave en la instalación de
    Ubuntu y lo servicios básicos a instalar.</para>

    <section>
      <title>Nombres de los equipos y configuración de la red</title>
      <para>Se han elegido los siguientes nombres para los equipos en los que se
      va a realizar la instalación de la infraestructura de OpenStack:</para>
      <itemizedlist>
	<listitem>
	  <para><emphasis role="bold">jupiter</emphasis>: para el nodo
	  controlador, que será el encargado de gestionar todos los recursos del
	  cloud, interaccionar con los clientes y ordenar a los nodos de
	  virtualización que ejecuten las instancias, pero en el que no se
	  ejecutarán máquinas virtuales. La mayor parte de componentes del Cloud
	  y configuración se realizará en este equipo, pero comparado con los
	  <emphasis>nodos de computación</emphasis> la carga de trabajo será
	  pequeña, por lo que no es necesario un equipo con mucha memoria RAM o
	  gran capacidad de procesamiento.</para>
	</listitem>
	<listitem>
	  <para><emphasis role="bold">io, europa, ganimedes y calisto</emphasis> (las 4 lunas
	  principales de júpiter): para los 4 nodos de virtualización o
	  nodos de computación, como se les denomina habitualmente en la
	  jerga propia de OpenStack. En estos equipos se instalarán
	  sólo los componentes necesarios para que se ejecuten las
	  instancias (máquinas virtuales) en ellos y estarán esperando las órdenes de
	  <literal>jupiter</literal>.</para>
	</listitem>
	<listitem>
	  <para><emphasis role="bold">saturno</emphasis>: para el nodo de
	  almacenamiento, ya que es tan importante como júpiter y a la vez
	  independiente. En este equipo todavía no está definido el software que
	  se instalará, pero lo más  probable es que sea una distribución
	  especializada en infraestructuras SAN/NAS como
	  <literal>OpenFiler</literal> ó <literal>FreeNAS</literal>.</para>
	</listitem>
	<listitem>
	  <para><emphasis role="bold">venus</emphasis>: un equipo convencional
	  en el que se instalarán los paquetes necesarios para usar el cliente
	  <literal>nova</literal> con el que podemos gestionar el cloud desde
	  línea de comandos sin la necesidad de realizar las operaciones desde
	  <literal>jupiter</literal>.</para> 
	</listitem>
      </itemizedlist>

      <para>Para la configuración de la red se ha optado por configurar bonding
      y redes VLAN tal como se describe en la siguiente figura:</para>

      <para>FIGURA BONDING y RED HACERRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR</para>
      
    </section>


    <section>
      <title>Instalación de Ubuntu 12.04.</title>
      <para>La máquina <literal>jupiter</literal> incluye dos discos duros
      SATAII de 500 GiB y una controladora de disco <literal>3ware 9650SE</literal>. Se ha optado
      por configurar esta controladora de disco en modo RAID 1, para incluir un
      nivel elemental de seguridad y consistencia de datos, aunque obviamente
      esto no descarta la utilización adicional de otros mecanismos de copias de
      seguridad que se configurarán posteriormente. Al tratarse de una
      controladora RAID hardware y estar configurada previamente, el sistema
      operativo que arranque en el equipo sólo verá un disco duro en
      <literal>/dev/sda</literal> de aproximadamente 500 GiB.</para>

      <para>Para prevenir corrupción de datos es muy importante que la
      controladora RAID se configure con una política de escritura
      <emphasis>Write Through</emphasis>. En esta configuración, el rendimiento
      del RAID se resiente un poco, pero lo hace más inmune a una posible
      corrupción de datos en caso de cortes en el suministro eléctrico.</para>

      <para>El sistema operativo elegido para los equipos del cloud
      es la distribución de GNU/Linux Ubuntu 12.04 LTS, que actualmente se
      conoce con el nombre de código <emphasis>Precise
      Pangolin</emphasis>. Pensamos que es la mejor opción entre todas las
      distribuciones de GNU/Linux ya que es la utilizada en toda la documentación
      de OpenStack y la mejor soportada.</para>

      <para>Durante la instalación realizamos el siguiente esquema de
      particionamiento:</para>
      <programlisting>
Disposit.   Tamaño    Id    Sistema     SF
/dev/sda1    32GiB    83    Linux       ext4
/dev/sda2     4GiB    82    Linux swap  intercambio
/dev/sda3  -resto-    8e    Linux LVM   volúmenes físicos
      </programlisting>

      <para>Crearemos tres particiones, una partición de unos 32 GiB para el
      sistema (directorio <literal>/</literal>), una partición de 4 GiB como
      área de intercambio, y el resto configurado para su uso a través de
      volúmenes lógicos con LVM.</para>

      <para>Tras la instalación del sistema, configuraremos el subsistema LVM
      pero sin llegar a crear ningún volumen:</para>
      <programlisting>
COMPLETAR
      </programlisting>

      <para>Aunque la controladora RAID es hardware y el sistema
        operativo no la gestiona, es importante que se pueda controlar
        su estado a través de algún módulo del kernel. En este caso el
        módulo <literal>3w-9xxx</literal> que se carga automáticamente y nos
	envía estos mensajes al log del sistema:</para>
        <programlisting>
[    2.799824] 3ware 9000 Storage Controller device driver for Linux v2.26.02.014.
[    2.799867] 3w-9xxx 0000:01:00.0: PCI INT A -> GSI 19 (level, low) -> IRQ 19
[    2.799880] 3w-9xxx 0000:01:00.0: setting latency timer to 64
[    3.080257] 3w-9xxx: scsi6: Found a 3ware 9000 Storage Controller at 0xfe8df000, IRQ: 19.
[    3.416117] 3w-9xxx: scsi6: Firmware FE9X 4.08.00.006, BIOS BE9X 4.08.00.001, Ports: 2.
[   12.307881] 3w-9xxx: scsi6: ERROR: (0x03:0x0101): Invalid command opcode:opcode=0x85.
[   12.308300] 3w-9xxx: scsi6: ERROR: (0x03:0x0101): Invalid command opcode:opcode=0x85.
[   12.308695] 3w-9xxx: scsi6: ERROR: (0x03:0x0101): Invalid command opcode:opcode=0x85.
[127301.671991] 3w-9xxx: scsi6: AEN: INFO (0x04:0x0029): Verify started:unit=0.
[131817.864145] 3w-9xxx: scsi6: AEN: INFO (0x04:0x002B): Verify completed:unit=0.
        </programlisting>           
        <para>Pendiente de hacer:</para>
        <para>http;//jonas.genannt.name/  Hay repositorio, con
        aplicaciones para manejar el RAID 3ware 9650SE</para>
    </section>

    <section>
      <title>NTP</title>
      <para>Para mantener todos los servicios sincronizados (a nivel de fecha y
      hora) es necesario instalar un cliente NTP (Network Time Protocol). En el
      caso de instalaciones multinodo hay que configurar uno de los nodos como
      servidor NTP, o confiar en otro servidor de nuestra red o de
      Internet.</para>

      <para>La red <literal>iescierva.net</literal> ya cuenta con un servidor
      NTP, por lo que únicamente hay que configurar correctamente el cliente,
      para ello basta con que sigamos los siguientes pasos:</para>

      <orderedlist>
	<listitem>
	  <para>Nos aseguramos que el paquete <literal>ntpdate</literal> esté
	  instalado.</para>
	  <screen><prompt>root@jupiter:~# </prompt><userinput>dpkg --list | grep ntpdate</userinput></screen>
	  <para>Si no lo estuviera lo instalamos:</para>
	  <screen><prompt>root@jupiter:~# </prompt><userinput>apt-get install ntpdate</userinput></screen>

	</listitem>
	
	<listitem>
	  <para>Configuramos crontab para que se ejecute el comando
	  <command>ntpdate</command> de forma periódica. Para ello ejecutamos
	  (como root) el comando <command>crontab -e</command> y editamos el
	  fichero que nos sugiere con el siguiente contenido:</para>
	  <programlisting>
0 4 * * * ntpdate ntp.iescierva.net ; hwclock --systohc
	  </programlisting>
	  <para>Podemos sustituir el servidor <literal>ntp.iescierva.net</literal> por uno en
	  Internet como <literal>ntp.ubuntu.com</literal> o como
	  <literal>hora.rediris.es</literal>.</para>

	  <para>Podemos comprobar que la configuración es correcta a través del
	  comando <command>crontab -l</command></para>
	  <programlisting>
root@jupiter:~# crontab -l
# m h  dom mon dow   command
0 4 * * * ntpdate ntp.iescierva.net ; hwclock -w
root@jupiter:~#
	  </programlisting>
	</listitem>

      </orderedlist>
      
    </section>

    <section>
      <title>MySQL</title>
      <para>Vamos a configurar todos los servicios del Cloud para que utilicen
      como base de datos <literal>MySQL</literal>, en vez de la base de datos
      <literal>SQLite</literal> que usan en la configuración por defecto. Para
      ello será necesario instalar MySQL y fijar una contraseña para el usuario
      root. Para ello seguimos los siguientes pasos:</para>

      <orderedlist>
	<listitem>
	  <para>Instalamos los paquetes necesarios, básicamente el servidor
	  MySQL, y la interfaz (DB-API) de Python para MySQL:</para>
	  <screen><prompt>root@jupiter:~# </prompt><userinput>apt-get install mysql-server python-mysqldb</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Durante la instalación del servidor se nos pedirá que
	  introduzcamos la contraseña para el usuario root de
	  MySQL, en nuestro caso <literal>calex2010!!</literal></para>
	</listitem>

	<listitem>
	  <para>Modificamos la interfaz de escucha de MySQL, aunque inicialmente
	  la fijamos a 0.0.0.0 para que el servidor escuche en todas las
	  interfaces, posteriormente fijaremos la interfaz con la IP de la red
	  privada del Cloud.</para>

	  <para>Configuramos el fichero <filename>/etc/mysql/my.cnf</filename>
	  modificando la siguiente línea:</para>
	  <programlisting>
bind-address        = 127.0.0.1
	  </programlisting>

	  <para>Por esta otra:</para>
	  <programlisting>
bind-address        = 0.0.0.0
	  </programlisting>
	</listitem>

	<listitem>
	  <para>Reiniciamos el demonio de MySQL:</para>
	  <screen><prompt>root@jupiter:~# </prompt><userinput>service mysql restart</userinput></screen>
	</listitem>
	
	<listitem>
	  <para>Probamos el cliente MySQL con la contraseña fijada:</para>
	  <programlisting>
root@jupiter:~# mysql -u root -p
Enter password: ***********
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 414
Server version: 5.5.24-0ubuntu0.12.04.1 (Ubuntu)

Copyright (c) 2000, 2011, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql>	    
	  </programlisting>
	</listitem>

      </orderedlist>
     
    </section>

  </section>

  <section xml:id="id_instUbuntu_keystone">
    <title>Instalación de KeyStone</title>
    <para>En este apartado describiremos la instalación y configuración de unos
    de los servicios básicos de OpenStack, el servicio de autenticación y
    gestión de la identidad (Identity Service): Keystone</para>

    <section>
      <title>¿Qué es Keystone?</title>
      <para>Keystone, es el componente de OpenStack encargado de la
      autentificación y la autorización de los distintos componentes que
      conforman OpenStack. Se incluyó desde la versión Essex y es el encargado
      de dos tareas principales:</para>

      <para>REVISARRRRRRRRRRRRRRRRRRRRRRRRRRRRR</para>
      <itemizedlist>
	<listitem>
	  <para>Gestión de usuarios: Keystone es el encargado de
	  mantener un registro de usuarios y los permisos que tiene cada 
	  uno de ellos.</para>
	</listitem>
	<listitem>
	  <para>Registro los servicios ofrecidos: Keystone ofrece un catálogo de
	  los servicios ofrecidos, así como información sobre cómo acceder a sus
	  APIs.</para>
	</listitem>
      </itemizedlist>

      <para>Los componentes básicos de los servicios de identidad son:</para>
      <itemizedlist>
	<listitem>
	  <para>Usuario: Podemos guardar su nombre, 
	  correo electrónico y contraseña.</para>
	</listitem>
	<listitem>
	  <para>Proyecto (<emphasis>tenant</emphasis> en la jerga de
	  OpenStack): En un proyecto podemos ejecutar un conjunto de
	  instancias con características en común, por ejemplo pueden estar
	  todas las instancias en el misma red, pueden utilizar una serie de
	  imágenes de sistemas o tener limitado el uso de recursos del
	  cloud.</para>
	</listitem>
	<listitem>
	  <para>Rol: Nos indica qué operaciones puede realizar
	  cada usuario. A un usuario se le pueden asignar diferentes roles
	  en cada proyecto.</para>
	</listitem>
      </itemizedlist>

      <para>Los conceptos fundamentales del <emphasis>registro de
      servicio</emphasis> son:</para>
      <itemizedlist>
	<listitem><para>Servicio: Corresponde a un componente de
	OpenStack que puede utilizar el módulo de
	autentificación.</para></listitem>
	<listitem><para>Endpoints: Representa las URL que nos permiten
	acceder a las API de cada uno de los servicios o componentes de
	OpenStack</para></listitem>
      </itemizedlist>
      
    </section>

    <section>
      <title>Instalación y configuración</title>
      <para>Una infraestructura OpenStack solo necesita un servidor que ejecute
      el servicio KeyStone, en nuestra instalación el servicio se ejecutará en
      el controlador, es decir, en la máquina <literal>jupiter</literal>.</para>

      <para>Para la instalación de Keystone, como para la instalación del resto
      de servicios, partiremos de los repositorios oficiales de Ubuntu, basta
      con que sigamos los siguientes pasos:</para>

      <orderedlist>
	<listitem>
	  <para>Creamos la base de datos que el servicio necesita en
	  MySQL. Para ello iniciamos el cliente MySQL:</para>
	  <screen><prompt>root@jupiter:~# </prompt><userinput>mysql -u root -p</userinput>
	  </screen>
	  <para>Desde el prompt de MySQL ejecutamos las siguientes
	  sentencias:</para>
	  <screen>
	    <prompt>mysql> </prompt><userinput>CREATE DATABASE keystone;</userinput>
	    <prompt>mysql> </prompt><userinput>GRANT ALL ON keystone.* to 'keystone'@'%' IDENTIFIED BY 'calex2010!!';</userinput>
	    <prompt>mysql> </prompt><userinput>FLUSH PRIVILEGES;</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Instalamos todo el software necesario:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>apt-get install keystone python-keystone python-keystoneclient</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Keystone utiliza por defecto una base de datos <literal>SQLite</literal>,
	  por lo que tendremos que borrar la base de datos que se configura en
	  la instalación por defecto:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>rm /var/lib/keystone/keystone.db</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Editamos el fichero de configuración principal de Keystone para
	  realizar los siguientes cambios:</para>
	  <itemizedlist>
	    <listitem>
	      <para>La cadena de conexión a la base de datos
	      <literal>keystone</literal> de MySQL.</para>
	    </listitem>

	    <listitem>
	      <para>El token administrativo (admin_token).</para>
	    </listitem>   
	  </itemizedlist>

	  <para>Para ello editamos el fichero
	  <filename>/etc/keystone/keystone.conf</filename> haciendo las
	  siguientes modificaciones:</para>
	  <programlisting>
admin_token = CALEX2010!!
connection = mysql://keystone:calex2010!!@172.20.254.190/keystone
	  </programlisting>
	  <para>Ver nota posterior sobre <literal>admin_token</literal>.</para>
	</listitem>

	<listitem>
	  <para>Reiniciamos el servicio:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>service keystone restart</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Creamos la base de datos inicial:</para>
	  <screen><prompt>root@jupiter:~# </prompt><userinput>keystone-manage db_sync</userinput></screen>
	</listitem>

	<listitem>
	  <para>Exportamos las siguientes variables de entorno en el fichero
	  .bashrc del usuario que ejecute el cliente, en principio el usuario
	  <literal>root</literal> de la máquina
	  <literal>jupiter</literal>:</para>
	  <programlisting language="Bash">
export SERVICE_ENDPOINT="http://172.20.254.190:35357/v2.0"
export SERVICE_TOKEN=CALEX2010!!
	  </programlisting>
	</listitem>

	<listitem>
	  <para>Creamos los usuarios, proyectos (tenants) y roles.</para>

	  <para>Crearemos ciertos usuarios a los que daremos ciertos roles en
	  ciertos proyectos, tal como se muestra en la siguiente tabla:</para>

	  <table rules="all">
	    <caption>Usuarios, proyectos y roles</caption>
	    <col width="34%"/>
	    <col width="33%"/>
	    <col width="33%"/>
	    <thead>
	      <tr>
		<td>Usuarios</td>
		<td>Proyectos (tenants)</td>
		<td>Roles</td>
	      </tr>
	    </thead>
	    <tbody>
	      <tr>
		<td>admin</td>
		<td>admin</td>
		<td>admin</td>
	      </tr>
	      <tr>
		<td>
		  <para>nova</para>
		  <para>glance</para>
		  <para>swift</para>
		</td>
		<td>service</td>
		<td>admin</td>
	      </tr>
	      <tr>
		<td>admin</td>
		<td>admin</td>
		<td>Member</td>
	      </tr>
	    </tbody>
	  </table>

	  <para>En la tabla podemos leer que al usuario <literal>admin</literal>
	  le daremos el rol <literal>admin</literal> en el proyecto
	  <literal>admin</literal>, o que a los usuarios <literal>nova</literal>,
	  <literal>glance</literal> y <literal>swift</literal> les daremos el
	  rol <literal>admin</literal> en el proyecto <literal>service</literal>.</para>

	  <para>La creación de usuarios, proyectos y roles es un proceso
	  tedioso, muy repetitivo y propenso a errores, por lo que ejecutaremos
	  el siguiente script que nos simplificará mucho este proceso:</para>

	  <programlisting language="Bash">
<xi:include parse="text" href="src/roles.sh"/>
	  </programlisting>

	  <para>Para la correcta ejecución de script hay que configurar las
	  siguientes variables de entorno en el inicio del script:</para>

	  <para>COMPLETARRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR</para>

	  <para>El script se limita a:</para>

	  <itemizedlist>
	    <listitem>
	      <para>Crear los proyectos necesarios.</para>
	    </listitem>

	    <listitem>
	      <para>Crear todos los usuarios y asignarles contraseña y
	      dirección de correo electrónico.</para>
	    </listitem>

	    <listitem>
	      <para>Crear los roles necesarios.</para>
	    </listitem>

	    <listitem>
	      <para>Asignar roles a los usuarios en los proyectos.</para>
	    </listitem>

	    <listitem>
	      <para>Crear los servicios y asignar a éstos los endpoints.</para>
	    </listitem>
	    
	  </itemizedlist>

	</listitem>

	<listitem>
	  <para>Verificamos que todo se ha creado correctamente a través de los
	  siguientes comandos (los identificadores no serán los mismos):</para>
	  <programlisting>
root@jupiter:~# keystone tenant-list
+----------------------------------+---------+---------+
|                id                |   name  | enabled |
+----------------------------------+---------+---------+
| 634675c752634b53879656c81da70a83 | service | True    |
| e7b1868b24a742318f1d73f612ecfe1d | admin   | True    |
+----------------------------------+---------+---------+
root@jupiter:~# keystone user-list
+----------------------------------+---------+--------------------+--------+
|                id                | enabled |       email        |  name  |
+----------------------------------+---------+--------------------+--------+
| 05743001bbf14700bcdf2ecc43edbf9b | True    | alex@iescierva.net | admin  |
| 246ba4e3d81c4ae8bdde8ec5784e74d3 | True    | alex@iescierva.net | swift  |
| 291c58f7258747758d109ecee2eb4a69 | True    | alex@iescierva.net | glance |
| 404dafc53b364a7e9f1476aa98082966 | True    | alex@iescierva.net | nova   |
+----------------------------------+---------+--------------------+--------+
root@jupiter:~# keystone role-list
+----------------------------------+--------+
|                id                |  name  |
+----------------------------------+--------+
| 2a716d669ce349eb88e44049723e0cb7 | admin  |
| 622c78f77bbe4a298452783f25cb4635 | Member |
+----------------------------------+--------+
root@jupiter:~# keystone endpoint-list
+----------------------------------+----------+--------------------------------------------------+--------------------------------------------------+---------------------------------------------+
|                id                |  region  |                    publicurl                     |                   internalurl                    |                   adminurl                  |
+----------------------------------+----------+--------------------------------------------------+--------------------------------------------------+---------------------------------------------+
| 0dd9427a0a624b38abf1876651ac3b69 | myregion | http://172.20.254.190:8776/v1/$(tenant_id)s      | http://172.20.253.190:8776/v1/$(tenant_id)s      | http://172.20.253.190:8776/v1/$(tenant_id)s |
| 2d471f9b23c84bac91c17615e157bb9f | myregion | http://172.20.254.190:5000/v2.0                  | http://172.20.253.190:5000/v2.0                  | http://172.20.253.190:35357/v2.0            |
| c9f799e2c6444bbda40dbc059207de09 | myregion | http://172.20.254.190:8773/services/Cloud        | http://172.20.253.190:8773/services/Cloud        | http://172.20.253.190:8773/services/Admin   |
| e72307edc9b84c2d9403b019c3a2e5a7 | myregion | http://172.20.254.190:9292/v1                    | http://172.20.253.190:9292/v1                    | http://172.20.253.190:9292/v1               |
| ec8b9d8d41ea496492730b20e1c34380 | myregion | http://172.20.254.190:8080/v1/AUTH_$(tenant_id)s | http://172.20.253.190:8080/v1/AUTH_$(tenant_id)s | http://172.20.253.190:8080/v1               |
| edf05355fc274c02a13f900605c56769 | myregion | http://172.20.254.190:8774/v2/$(tenant_id)s      | http://172.20.253.190:8774/v2/$(tenant_id)s      | http://172.20.253.190:8774/v2/$(tenant_id)s |
+----------------------------------+----------+--------------------------------------------------+--------------------------------------------------+---------------------------------------------+
root@jupiter:~# keystone service-list
+----------------------------------+----------+--------------+----------------------------+
|                id                |   name   |     type     |        description         |
+----------------------------------+----------+--------------+----------------------------+
| 04b121f16e6045a79a4601c5171998d3 | glance   | image        | OpenStack Image Service    |
| 298de2e4a0f04b42b54818fd57d1bf2e | keystone | identity     | OpenStack Identity Service |
| 368b238c02734ee4ac1a6ac6bf440ffa | nova     | compute      | OpenStack Compute Service  |
| a075b6add5564f729253dd0339b1d5d9 | volume   | volume       | OpenStack Volume Service   |
| c23937017a7e4baa8811a24ff9c8086c | swift    | object-store | OpenStack Storage Service  |
| e998168d3d264e3aab37d4b8413fe736 | ec2      | ec2          | OpenStack EC2 Service      |
+----------------------------------+----------+--------------+----------------------------+
root@jupiter:~# 
	  </programlisting>
	</listitem>
      </orderedlist>
    </section>
  </section>

  <section xml:id="id_instUbuntu_glance">
    <title>Instalación de Glance</title>
    <para>Esta sección describe la instalación y configuración del
    módulo de OpenStack, Glance. Este servicio es el encargado de la
    gestión y registro de las imágenes que posteriormente se van a poder
    instanciar en máquinas virtuales.</para>

    <section>
      <title>¿Qué es Glance?</title>
      <para></para>
      
    </section>

    <section>
      <title>Instalación y configuración</title>
      <para>Una infraestructura OpenStack solo necesita un servidor que ejecute
      el servicio Glance, en nuestra instalación el servicio se ejecutará en el
      controlador, es decir, en la máquina <literal>jupiter</literal>.</para>

      <para>Para la instalación de Glance, como para la instalación del resto
      de servicios, partiremos de los repositorios oficiales de Ubuntu, basta
      con que sigamos los siguientes pasos:</para>

      <orderedlist>
	<listitem>
	  <para>Creamos la base de datos que el servicio necesita en MySQL. Para
	  ello iniciamos el cliente MySQL:</para>
	  <screen><prompt>root@jupiter:~# </prompt><userinput>mysql -u root -p</userinput>
	  </screen>
	  <para>Desde el prompt de MySQL ejecutamos las siguientes
	  sentencias:</para>
	  <screen>
	    <prompt>mysql> </prompt><userinput>CREATE DATABASE glance;</userinput>
	    <prompt>mysql> </prompt><userinput>GRANT ALL ON glance.* to 'glance'@'%' IDENTIFIED BY 'calex2010!!';</userinput>
	    <prompt>mysql> </prompt><userinput>FLUSH PRIVILEGES;</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Instalamos todo el software necesario:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>apt-get install glance glance-api glance-client glance-common glance-registry python-glance</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Modificamos las siguientes líneas del fichero de configuración
	  <filename>/etc/glance/glance-api-paste.ini</filename>, modificando
	  estas líneas:</para>
	  <programlisting>
admin_tenant_name = %SERVICE_TENANT_NAME%
admin_user = %SERVICE_USER%
admin_password = %SERVICE_PASSWORD%
	  </programlisting>

	  <para>Por estas otras:</para>
	  <programlisting>
admin_tenant_name = service
admin_user = glance
admin_password = calex2010!!
	  </programlisting>
	</listitem>

	<listitem>
	  <para>El mismo cambio anterior hay que hacerlo también en el fichero
	  <filename>/etc/glance/glance-registry-paste.ini</filename>, dejando
	  las últimas líneas así:</para>
	  <programlisting>
admin_tenant_name = service
admin_user = glance
admin_password = calex2010!!
	  </programlisting>
	</listitem>

	<listitem>
	  <para>Editamos el fichero
	  <filename>/etc/glance/glance-registry.conf</filename> y modificamos la
	  cadena de conexión a la base de datos MySQL recién creada:</para>
	  <programlisting>
sql_connection = mysql://glance:calex2010!!@172.20.254.190/glance
	  </programlisting>
	</listitem>

	<listitem>
	  <para>Añadimos la siguiente configuración al fichero
	  <filename>/etc/glance/glance-registry.conf</filename>:</para>
	  <programlisting>
[paste_deploy]
flavor = keystone
	  </programlisting>

	  <para>Y también al fichero
	  <filename>/etc/glance/glance-api.conf:</filename></para>
	  <programlisting>
[paste_deploy]
flavor = keystone
	  </programlisting>
	</listitem>

	<listitem>
	  <para>Creamos el esquema de la base de datos:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>glance-manage version_control 0</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>glance-manage db_sync</userinput>
	  </screen>
	  <para>Podemos ignorar los warnings de Python.</para>
	</listitem>

	<listitem>
	  <para>Reiniciamos los demonios del servicio Glance:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>service glance-api restart</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service glance-registry restart</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Añadimos las siguientes variables de entorno en el fichero
	  .bashrc del usuario que ejecute el cliente glance, en principio al
	  usuario <literal>root</literal> de la máquina <literal>jupiter</literal>:</para>
	  <programlisting language="Bash">
export SERVICE_TOKEN=CALEX2010!!
export OS_TENANT_NAME=admin
export OS_USERNAME=admin
export OS_PASSWORD=calex2010!!
export OS_AUTH_URL="http://172.20.254.190:5000/v2.0/"
export SERVICE_ENDPOINT=http://172.20.254.190:35357/v2.0
	  </programlisting>
	</listitem>

	<listitem>
	  <para>Probamos el servicio.</para>

	  <para>Nada más instalar, no hay ninguna imagen dada de alta, pero el
	  siguiente comando, que muestra la lista de imágenes disponible, nos
	  debería dar una lista vacía:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>glance index</userinput>
	  </screen>
	</listitem>

      </orderedlist>

      <para>Podemos probar el servicio Glance a través de una imagen de prueba
      siguiendo estos pasos:</para>

      <orderedlist>
	<listitem>
	  <para>Descargamos la imagen Cirros desde la siguiente URL:</para>
	  <itemizedlist>
	    <listitem>
	      <para><link xlink:href="https://launchpad.net/cirros/+download">https://launchpad.net/cirros/+download</link></para>
	    </listitem>
	  </itemizedlist>
	</listitem>

	<listitem>
	  <para>Damos la imagen de alta en Glance a través del siguiente comando:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>glance add name="Cirros (test) 0.3.0 64 bits" is_public=true container_format=bare disk_format=qcow2 &lt; cirros-0.3.0-x86_64-disk.img</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Si todo es correcto, podremos ver ahora la imagen recién subida
	  en la lista de imágenes que proporciona el servicios Glance:</para>
	  <programlisting>
root@jupiter:~# glance index
ID                                   Name                           Disk Format          Container Format     Size          
------------------------------------ ------------------------------ -------------------- -------------------- --------------
fe22ea9c-ebb2-4bb8-be34-ea7732a8a3d2 Cirros (test) 0.3.0 64 bits    qcow2                bare                        9761280
root@jupiter:~# 
	  </programlisting>
	</listitem>	
      </orderedlist>
    </section>    
  </section>

  <section xml:id="id_instUbuntu_nova">
    <title>Instalación de Nova</title>
    <para>Este capítulo describe la instalación y configuración del
    módulo de OpenStack, Nova. Este servicio es el encargado de gestionar las
    instancias (máquinas virtuales) del Cloud, por lo que lo convierte en el
    servicio central de toda la infraestructura de Openstack.</para>

    <section>
      <title>¿Qué es Nova?</title>
      <para>COMPLETAR POSTERIORMENTE</para>
      
    </section>

    <section>
      <title>Instalación y configuración</title>
      <para>Una infraestructura OpenStack puede ejecutar tantos nodos
      <literal>nova</literal> como desee. Como mínimo uno, esto hace que sea posible tener una
      infraestructura de OpenStack en un solo nodo, pero podemos desplegar
      tantos nodos de computación como servidores tengamos. El máximo depende de
      diversos factores como la carga a soportar, la disponibilidad de
      servidores, factores económicos, ancho de banda de nuestra red, uso del
      Cloud, y un largo etcétera. En nuestra instalación el servicio
      <literal>nova-compute</literal> se ejecutará en cuatro nodos,
      concretamente en las máquinas <literal>io</literal>,
      <literal>europa</literal>, <literal>ganimedes</literal> y
      <literal>calisto</literal>, pero excluyendo al controlador, a la máquina
      <literal>jupiter</literal>.</para>

      <para>Para la instalación de Nova, como para la instalación del resto
      de servicios, partiremos de los repositorios oficiales de Ubuntu, basta
      con que sigamos los siguientes pasos en la máquina
      <literal>jupiter</literal>:</para>

      <orderedlist>
	<listitem>
	  <para>Creamos la base de datos que el servicio necesita en MySQL. Para
	  ello iniciamos el cliente MySQL:</para>
	  <screen><prompt>root@jupiter:~# </prompt><userinput>mysql -u root -p</userinput>
	  </screen>
	  <para>Desde el prompt de MySQL ejecutamos las siguientes
	  sentencias:</para>
	  <screen>
	    <prompt>mysql> </prompt><userinput>CREATE DATABASE nova;</userinput>
	    <prompt>mysql> </prompt><userinput>GRANT ALL ON nova.* to 'nova'@'%' IDENTIFIED BY 'calex2010!!';</userinput>
	    <prompt>mysql> </prompt><userinput>FLUSH PRIVILEGES;</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Instalamos todo el software necesario:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>apt-get install nova-api nova-cert nova-compute nova-compute-kvm nova-doc nova-network nova-objectstore nova-scheduler nova-volume rabbitmq-server novnc nova-consoleauth</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Editamos el fichero <filename>/etc/nova/nova.conf</filename> con
	  el siguiente contenido: </para>
	  <programlisting>
<xi:include parse="text" href="samples/nova.conf.jupiter"/>
	  </programlisting>
	</listitem>

	<listitem>
	  <para>El mismo cambio anterior hay que hacerlo también en el fichero
	  <filename>/etc/glance/glance-registry-paste.ini</filename>, dejando
	  las últimas líneas así:</para>
	  <programlisting>
admin_tenant_name = service
admin_user = glance
admin_password = calex2010!!
	  </programlisting>
	</listitem>

	<listitem>
	  <para>Creamos el esquema de la base de datos que Nova necesita:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>nova-manage db sync</userinput>
	  </screen>
	  <para>Podemos ignorar los warnings de Python.</para>
	</listitem>

	<listitem>
	  <para>Reiniciamos todos los demonios del servicio Nova:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>service novnc stop</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-consoleauth stop</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-volume stop</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-schedule  stop</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-objectstore stop</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-api stop</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-compute stop</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-network stop</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-cert stop</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service libvirt-bin stop</userinput>

	    <prompt>root@jupiter:~# </prompt><userinput>service libvirt-bin start</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-cert start</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-network start</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-compute start</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-api start</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-objectstore start</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-schedule  start</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-volume start</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-consoleauth start</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service novnc start</userinput>
	  </screen>

	  <para>Mientras que trabajemos con el servicio Nova (instalación,
	  configuración, pruebas...), va a ser muy frecuente tener que reiniciar
	  todos sus servicios asociados, podemos facilitar esta tarea a través
	  del siguiente script, que podremos guardar en <filename>/root/bin/nova.sh</filename>:</para>

	  <programlisting language="Bash">
<xi:include parse="text" href="src/nova.sh"/>
	  </programlisting>
	</listitem>

	<listitem>
	  <para>Proporcionamos una red (rango de IPs) que asociaremos a las
	  instancias, lo podemos hacer a través del siguiente comando:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>nova-manage network create private --fixed_range_v4=10.0.0.0/24 --num_networks=1 --bridge=br100 --bridge_interface=bond0.62 --network_size=256 --dns1=172.20.254.235</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Nos aseguramos de tener definidas las siguientes variables de
	  entorno en la máquina que haga de cliente Nova:</para>
	  <programlisting language="Bash">
export SERVICE_TOKEN=CALEX2010!!
export OS_TENANT_NAME=admin
export OS_USERNAME=admin
export OS_PASSWORD=calex2010!!
export OS_AUTH_URL="http://172.20.254.190:5000/v2.0/"
export SERVICE_ENDPOINT=http://172.20.254.190:35357/v2.0
	  </programlisting>
	</listitem>

	<listitem>
	  <para>Volvemos a reiniciar todos los servicios:</para>
	  <screen>
	    <prompt>$ </prompt><userinput>/root/bin/nova.sh restart</userinput>
	  </screen>

	</listitem>

	<listitem>
	  <para>Comprobamos que todos los servicios estén iniciados, lo podemos
	  hacer a través del comando:</para>
	  <screen>
	    <prompt>$ </prompt><userinput>nova-manage service list</userinput>
	  </screen>
	  <para>...que nos debería proporcionar una salida similar a la
	  siguiente:</para>
	  <programlisting>
root@jupiter:~# nova-manage service list
Binary           Host                                 Zone             Status     State Updated_At
nova-consoleauth jupiter                              nova             enabled    :-)   2012-07-28 10:02:04
nova-cert        jupiter                              nova             enabled    :-)   2012-07-28 10:02:05
nova-scheduler   jupiter                              nova             enabled    :-)   2012-07-28 10:02:03
nova-compute     jupiter                              nova             enabled    :-)   2012-07-28 10:01:57
nova-volume      jupiter                              nova             enabled    :-)   2012-07-28 10:02:04
nova-network     jupiter                              nova             enabled    :-)   2012-07-28 10:02:03
nova-compute     io                                   nova             enabled    :-)   2012-07-28 10:02:03
	  </programlisting>
	</listitem>

	<listitem>
	  <para>Arrancamos una imagen de prueba, a través de cualquiera de los
	  siguientes comandos podemos comprobar las imágenes que podemos iniciar
	  proporcionadas por el servicio Glance:</para>
	  <screen>
	    <prompt>$ </prompt><userinput>glance index</userinput>
	  </screen>

	  <screen>
	    <prompt>$ </prompt><userinput>nova image-list</userinput>
	  </screen>

	  <para>Generamos las claves SSH necesarias para conectarnos a través
	  del protocolo SSH a los servidores:</para>
	  <screen>
	    <prompt>$ </prompt><userinput>nova keypair-add test > /root/test.pem</userinput>
	  </screen>

	  <para>Para iniciar una instancia necesitamos el nombre de ésta,
	  ejecutamos el siguiente comando para iniciar una instancia:</para>
	  <screen>
	    <prompt>$ </prompt><userinput>nova boot --image "Cirros (test) 0.3.0 64 bits" --flavor m1.small --key_name test my-first-server</userinput>
	  </screen>

	  <para>Podemos comprobar el estado de la instancia a través del
	  comando:</para>
	  <screen>
	    <prompt>$ </prompt><userinput>nova list</userinput>
	  </screen>

	  <para>Y obtener más información a través del UID de la instancia con
	  el comando:</para>
	  <screen>
	    <prompt>$ </prompt><userinput>nova show &lt;id&gt;</userinput>
	  </screen>

	  <para>Nos podemos conectar a través de SSH a la instancia con el comando:</para>
	  <screen>
	    <prompt>$ </prompt><userinput>ssh -i test.pem cirros@10.0.0.2</userinput>
	  </screen>
	</listitem>

      </orderedlist>

    </section>
    
  </section>


<!-- Falta completar algunos apuntes más que tengo de Nova
y el servicio Horizon -->

</chapter>
