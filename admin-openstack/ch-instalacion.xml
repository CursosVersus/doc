<?xml version="1.0" encoding="utf-8"?>
<chapter xmlns="http://docbook.org/ns/docbook"
         xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
         xml:id="ch-instalacion">
  <title>Instalación de OpenStack en GNU/Linux Ubuntu 12.04</title>

  <section xml:id="id_instUbuntu_intro">
    <title>Introducción</title>
    <para>Esta sección muestra el proceso detallado de instalación y
    configuración de OpenStack basado en la plataforma Ubuntu 12.04 (Precise
    Pangolin) usando 5 servidores (nodos). Se usará uno de los servidores como
    nodo controlador, ejecutando los componentes Nova, Glance, Swift, Keystone y
    Horizon y  el resto de nodos como nodos de computación, ejecutando
    únicamente Nova Compute.</para>

    <para>Podemos resumir el proceso global de instalación en los siguientes
    pasos:</para>

    <orderedlist>
      <listitem>
	<para>Configuración de la red del Cloud.</para>
      </listitem>

      <listitem>
	<para>Instalación de servicios básicos (NTP, MySQL, ...).</para>
      </listitem>

      <listitem>
	<para>Instalación y configuración de Keystone (servicio de autenticación).</para>
      </listitem>

      <listitem>
	<para>Instalación y configuración de Glance (servicio de gestión de imágenes).(</para>
      </listitem>

      <listitem>
	<para>Instalación y configuración de Nova (servicios de
	computación).</para>
	<para>Configuración del tipo de red: FlatDHCPNetwork.</para>
      </listitem>

      <listitem>
	<para>Añadir imágenes para la creación de máquinas virtuales.</para>
      </listitem>

      <listitem>
	<para>Iniciar una máquina virtual de prueba.</para>
      </listitem>

      <listitem>
	<para>Instalación y configuración del Dashboard (Horizon).</para>
      </listitem>

    </orderedlist>

    <para>El proceso de instalación y configuración de OpenStack es un proceso
    complejo y muy propenso a errores, ya que se trata de un proyecto muy joven
    y bajo un desarrollo muy activo. Para obtener más información más allá de
    este documento se ruega visitar las páginas oficiales:</para>

    <itemizedlist>
      <listitem>
	<para><link xlink:href="http://www.openstack.org">OpenStack Open Source
	Cloud Computing Software.</link></para>
      </listitem>
      
      <listitem>
	<para><link xlink:href="http://docs.openstack.org">OpenStack Docs: Essex.</link></para>
      </listitem>

      <listitem>
	<para><link xlink:href="https://launchpad.net/openstack/">OpenStack in Launchpad.</link></para>
      </listitem>
      
    </itemizedlist>

  </section>

  <section xml:id="id_instUbuntu_prerrequisitos">
    <title>Prerrequisitos</title>
    <para>Para el seguimiento del proceso de instalación y configuración de
    OpenStack que se detalla posteriormente, es necesario partir de algunas suposiciones y de cumplir ciertos
    requisitos. Como los siguientes:</para>

    <itemizedlist>
      <listitem>
	<para>Se necesitan, al menos tres nodos con Ubuntu 12.04 LTS instalado.</para>
      </listitem>

      <listitem>
	<para>Uno de los nodos será el nodo controlador, que ejecutará todos los
	servicios excepto <literal>nova-compute</literal>.</para>
      </listitem>

      <listitem>
	<para>Se recomienda el uso de LVM (el gestor de volúmenes lógicos de
	Linux), la configuración de LVM y el listado del esquema de
	particionamiento a seguir, se detalla en las siguientes
	secciones.</para>
      </listitem>

      <listitem>
	<para>La resolución de nombres DNS para todas las máquinas debe ser
	perfecta.</para>
	<para>Nuestra red cuenta con un nombre de dominio, concretamente
	<literal>iescierva.net</literal>, este será el dominio utilizado durante
	todo el documento.</para>
	<para>Si no se cuenta con un servidor DNS en la red, todas las máquinas
	deberán contar con el fichero <filename>/etc/hosts</filename> con todas
	las máquinas correctamente registradas.</para>
      </listitem>

      <listitem>
	<para>Todos los nodos que participen en el Cloud deben tener la fecha y
	hora sincronizada a través del servicio NTP (Network Time Protocol). Se
	recomienda que la red cuente con uno varios servidores NTP.</para>
      </listitem>

      <listitem>
	<para>Entre todos los hipervisores disponibles en la comunidad FLOSS,
	hemos optado por KVM.</para>
      </listitem>

      <listitem>
	<para>La contraseña para todos los usuarios y para todos los servicios
	será la misma: <literal>calex2010!!</literal></para>
      </listitem>

      <listitem>
	<para>El sistema deberá estar actualizado antes de empezar con el
	proceso de instalación/configuración:</para>
	<para><command>apt-get update</command></para>
	<para><command>apt-get upgrade</command></para>
      </listitem>
      
    </itemizedlist>
    
  </section>

  <section xml:id="id_instUbuntu_servBasicos">
    <title>Servicios y configuración básica</title>
    <para>A continuación se detallan los aspectos clave en la instalación de
    Ubuntu y lo servicios básicos a instalar.</para>

    <section>
      <title>Nombres de los equipos y configuración de la red</title>
      <para>Se han elegido los siguientes nombres para los equipos en los que se
      va a realizar la instalación de la infraestructura de OpenStack:</para>
      <itemizedlist>
	<listitem>
	  <para><emphasis role="bold">jupiter</emphasis>: para el nodo
	  controlador, que será el encargado de gestionar todos los recursos del
	  cloud, interaccionar con los clientes y ordenar a los nodos de
	  virtualización que ejecuten las instancias, pero en el que no se
	  ejecutarán máquinas virtuales. La mayor parte de componentes del Cloud
	  y configuración se realizará en este equipo, pero comparado con los
	  <emphasis>nodos de computación</emphasis> la carga de trabajo será
	  pequeña, por lo que no es necesario un equipo con mucha memoria RAM o
	  gran capacidad de procesamiento.</para>
	</listitem>
	<listitem>
	  <para><emphasis role="bold">io, europa, ganimedes y calisto</emphasis> (las 4 lunas
	  principales de júpiter): para los 4 nodos de virtualización o
	  nodos de computación, como se les denomina habitualmente en la
	  jerga propia de OpenStack. En estos equipos se instalarán
	  sólo los componentes necesarios para que se ejecuten las
	  instancias (máquinas virtuales) en ellos y estarán esperando las órdenes de
	  <literal>jupiter</literal>.</para>
	</listitem>
	<listitem>
	  <para><emphasis role="bold">saturno</emphasis>: para el nodo de
	  almacenamiento, ya que es tan importante como júpiter y a la vez
	  independiente. En este equipo todavía no está definido el software que
	  se instalará, pero lo más  probable es que sea una distribución
	  especializada en infraestructuras SAN/NAS como
	  <literal>OpenFiler</literal> ó <literal>FreeNAS</literal>.</para>
	</listitem>
	<listitem>
	  <para><emphasis role="bold">venus</emphasis>: un equipo convencional
	  en el que se instalarán los paquetes necesarios para usar el cliente
	  <literal>nova</literal> con el que podemos gestionar el cloud desde
	  línea de comandos sin la necesidad de realizar las operaciones desde
	  <literal>jupiter</literal>. La administración del Cloud a través de la
	  interfaz web del dashboard (Horizon) se podrá realizar desde
	  cualquier máquina que tenga acceso a la red pública del Cloud
	  (incluyendo máquinas Windows).</para> 
	</listitem>
      </itemizedlist>

      <para>Para la configuración de la red se ha optado por la siguiente
      configuración, tal como se muestra en la figura:</para>

      <figure xml:id="infra_red">
	<title>Infraestructura de nuestro Cloud Computing (IES Cierva/IES Los Albares)</title>
	<mediaobject>
	  <imageobject role="fo">
	    <imagedata fileref="figures/infra_red_iescierva.svg" format="SVG" scale="60"/>
	  </imageobject>
	  <imageobject role="html">
	    <imagedata fileref="figures/infra_red_iescierva.png" format="PNG" scale="50"/>
	  </imageobject>
	</mediaobject>
      </figure>

      <para>En nuestra infraestructura el controlador del Cloud
      (<literal>jupiter</literal>) cuenta con dos tarjetas de red Ethernet,
      mientras que los nodos de computación cuentan con seis, en lugar de
      asignar de forma estática las tarjetas a las redes se ha optado por una
      configuración en bonding y con VLANs.</para>

      <para>De esta forma, todos los nodos tienen una sola interfaz de red,
      <literal>bond0</literal>, unión de todas las interfaces
      <literal>ethX</literal> del sistema, y en la que se han configurado las
      siguientes redes locales virtuales (VLAN):</para>

      <table rules="all">
	<caption>LANs Virtuales del Cloud Computing</caption>
	<col width="30%"/>
	<col width="30%"/>
	<col width="20%"/>
	<col width="20%"/>
	<thead>
	  <tr>
	    <td>Red Virtual</td>
	    <td>Rango de direcciones IP</td>
	    <td>Interfaz virtual</td>
	    <td>Interfaz física (id. VLAN)</td>
	  </tr>
	</thead>
	<tbody>
	  <tr>
	    <td>Red Pública</td>
	    <td>172.20.254.0/24</td>
	    <td>----</td>
	    <td>bond0 (untagged)</td>
	  </tr>
	  <tr>
	    <td>
	      <para>Red Privada</para>
	      <para>Red Máquinas Virtuales</para>
	    </td>
	    <td>
	      <para>172.20.253.0/24</para>
	      <para>10.0.0.0/24</para>
	    </td>
	    <td>bond0.60</td>
	    <td>bond0 (60)</td>
	  </tr>
	  <tr>
	    <td>Red Almacenamiento SAN/NAS</td>
	    <td>172.20.252.0/24</td>
	    <td>bond0.62</td>
	    <td>bond0 (62)</td>
	  </tr>
	  <tr>
	    <td>Direcciones IP flotantes</td>
	    <td>172.18.0.0/16</td>
	    <td>bond0.61</td>
	    <td>bond0 (61)</td>
	  </tr>

	</tbody>
      </table>

      <para>De esta forma cada nodo tiene:</para>
      <itemizedlist>
	<listitem>
	  <para>Todas las interfaces desde <literal>eth0</literal> hasta
	  <literal>eth6</literal> unidas por bonding a través de la interfaz
	  bond0.</para>
	</listitem>

	<listitem>
	  <para>La interfaz bond0, untagged (sin VLAN), asociada a la red
	  pública.</para>
	  <para>En el caso del controlador la 172.20.254.190, en el caso del
	  primer nodo, la 172.20.254.191.</para>
	</listitem>

	<listitem>
	  <para>La interfaz bond0.60, unida a la red privada y utilizada para
	  los servicios del Cloud y para la comunicación de máquinas
	  virtuales.</para>
	  <para>En el caso del controlador la 172.20.253.190, en el caso del
	  primer nodo, la 172.20.253.191.</para>
	  <para>Esta interfaz también se usa para la comunicación entre las
	  máquinas virtuales a través de la red 10.0.0.0/24.</para>
	</listitem>

	<listitem>
	  <para>La interfaz bond0.62, asociada a la red de almacenamiento,
	  utilizada para el acceso a los servidores que ofrecen SAN y
	  NAS.</para>
	  <para>En el caso del controlador la 172.20.252.190, en el caso del
	  primer nodo, la 172.20.252.191.</para>
	</listitem>

	<listitem>
	  <para>La interfaz bond0.61, utilizada para asignar direcciones IP
	  flotantes. Una dirección IP flotante es la que asocia a una máquina
	  virtual de forma temporal para que sea accesible desde fuera del
	  Cloud. Esta dirección es la que se proporcionará a los clientes del
	  Cloud.</para>
	  <para>La red utilizada será la 172.18.0.0/16.</para>
	</listitem>
	
      </itemizedlist>

      <para>A continuación se muestra la figura anterior con los nombres de las
      interfaces y las direcciones IP:</para>

      <figure xml:id="infra_red2">
	<title>Infraestructura de nuestro Cloud Computing (IES Cierva/IES Los Albares)</title>
	<mediaobject>
	  <imageobject role="fo">
	    <imagedata fileref="figures/infra_red_iescierva2.svg" format="SVG" scale="60"/>
	  </imageobject>
	  <imageobject role="html">
	    <imagedata fileref="figures/infra_red_iescierva2.png" format="PNG" scale="50"/>
	  </imageobject>
	</mediaobject>
      </figure>
    </section>
    <section xml:id="id_bonding">
      <title>Configuración del bonding y de las VLAN</title>

      <section>
	<title>¿Qué es el bonding?</title>
	<para><emphasis role="bond">Bonding</emphasis>,
	es un concepto utilizado en redes de ordenadores, que describe varias
	formas de combinar (agregar) en paralelo varias interfaces de red
	creando una sola interfaz lógica. Esta nueva interfaz proporciona
	redundancia en el enlace y un mayor rendimiento que el proporcionado
	por las interfaces físicas trabajando de forma individual.</para>

	<para>También se conoce como port trunking, link bundling,
	Ethernet/network/NIC bonding, o NIC teaming. Los estándares que hay
	bajo esta tecnología son el IEEE 802.1ax (LACP: Link Aggregation Control
	Protocol para redes Ethernet) o el protocolo previo IEEE 802.3ad,
	además de varias soluciones propietarias.</para>
	
      </section>

      <section>
	<title>Configuración de bonding y VLAN</title>
	<para>Para rendimientos alto, es casi obligatorio el uso de bonding, y
	altamente recomendable el uso de LACP (802.3ad), por lo que se recomienda un
	switch con soporte LACP. Si no se cuenta con LACP, se puede optar por
	otros modos de bonding tal como se detalla en <link
	xlink:href="http://www.kernel.org/doc/Documentation/networking/bonding.txt">http://www.kernel.org/doc/Documentation/networking/bonding.txt</link>.</para>

	<para>Además de bonding, vamos a configurar varias VLAN, para tener
	soporte en Ubuntu tenemos que hacer dos cosas:</para>

	<orderedlist>
	  <listitem>
	    <para>Instalar el paquete vlan:</para>
	    <screen><prompt>$ </prompt><userinput>apt-get install vlan</userinput></screen>

	  </listitem>

	  <listitem>
	    <para>Asegurarnos de tener el módulo del kernel
	    <literal>8021q</literal> cargado, para ello modificamos el fichero
	    <filename>/etc/modules</filename> para que quede así:</para>
	    <programlisting>
# /etc/modules: kernel modules to load at boot time.
#
# This file contains the names of kernel modules that should be loaded
# at boot time, one per line. Lines beginning with "#" are ignored.

loop
lp
rtc
8021q
	    </programlisting>
	  </listitem>
	  
	</orderedlist>

	<para>Para la configuración de bonding en Ubuntu seguimos los
	siguientes pasos:</para>

	<orderedlist>
	  <listitem>
	    <para>Instalamos los siguientes paquetes:</para>
	    <screen>
	      <prompt>$ </prompt><userinput>apt-get install ifenslave ethtool</userinput>
	    </screen>
	  </listitem>

	  <listitem>
	    <para>Bajamos las interfaces de red:</para>
	    <screen>
	      <prompt>$ </prompt><userinput>ifdown eth0</userinput>
	      <prompt>$ </prompt><userinput>ifdown eth1</userinput>
	    </screen>
	  </listitem>

	  <listitem>
	    <para>Creamos el fichero
	    <filename>/etc/modprobe.d/bonding.conf</filename> con el siguiente
	    contenido:</para>
	    <programlisting>
	      alias bond0 bonding
	      # options bonding mode=0 miimon=100
	    </programlisting>
	  </listitem>

	  <listitem>
	    <para>Configuramos las nuevas interfaces a través del fichero
	    <filename>/etc/network/interfaces</filename>:</para>
	    <programlisting language="Python">
	      # This file describes the network interfaces available on your system 
	      # and how to activate them. For more information, see interfaces(5). 

	      # The loopback network interface 
	      auto lo 
	      iface lo inet loopback 

	      auto eth0 
	      iface eth0 inet manual 
	      bond-master bond0 
	      pre-up ethtool -s eth0 wol g 
	      post-down ethtool -s eth0 wol g 

	      auto eth1 
	      iface eth1 inet manual 
	      bond-master bond0 
	      pre-up ethtool -s eth1 wol g 
	      post-down ethtool -s eth1 wol g

	      # The primary network interface 
	      auto bond0 
	      iface bond0 inet static 
	      address 172.20.254.192 
	      netmask 255.255.255.0 
	      broadcast 172.20.254.255 
	      network 172.20.254.0 
	      gateway 172.20.254.254 
	      bond-mode 802.3ad 
	      bond-miimon 100 
	      bond-lacp-rate 1 
	      bond-slaves none 
	      dns-nameservers 172.20.254.104 172.20.254.235 
	      dns-search iescierva.net
	    </programlisting>
	  </listitem>

	  <listitem>
	    <para>Reconfiguramos las interfaces de red, en principio no hace
	    falta reiniciar, pero podemos hacerlo para comprobar que todo
	    funciona correctamente tras iniciarse el sistema.</para>
	  </listitem>

	  <listitem>
	    <para>Probamos que tenemos conectividad y que el bonding está
	    configurado:</para>
	    <screen>
	      <prompt>$ </prompt><userinput>cat /proc/net/bonding/bond0</userinput>
	      Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011)

	      Bonding Mode: IEEE 802.3ad Dynamic link aggregation
	      Transmit Hash Policy: layer2 (0)
	      MII Status: up
	      MII Polling Interval (ms): 100
	      Up Delay (ms): 0
	      Down Delay (ms): 0

	      802.3ad info
	      LACP rate: fast
	      Min links: 0
	      Aggregator selection policy (ad_select): stable
	      Active Aggregator Info:
	      Aggregator ID: 1
	      Number of ports: 2
	      Actor Key: 17
	      Partner Key: 58
	      Partner Mac Address: 00:9c:02:b7:f9:40

	      Slave Interface: eth1
	      MII Status: up
	      Speed: 1000 Mbps
	      Duplex: full
	      Link Failure Count: 0
	      Permanent HW addr: 00:25:90:72:2c:47
	      Aggregator ID: 1
	      Slave queue ID: 0

	      Slave Interface: eth0
	      MII Status: up
	      Speed: 1000 Mbps
	      Duplex: full
	      Link Failure Count: 0
	      Permanent HW addr: 00:25:90:72:2c:46
	      Aggregator ID: 1
	      Slave queue ID: 0
	    </screen>
	  </listitem>

	  <listitem>
	  <para>Podemos comprobar las VLAN activas a través del siguiente
	  comando:</para>
	  <screen>
            <prompt>$ </prompt><userinput>cat /proc/net/vlan/config</userinput>
VLAN Dev name	 | VLAN ID
Name-Type: VLAN_NAME_TYPE_RAW_PLUS_VID_NO_PAD
bond0.60       | 60  | bond0
bond0.61       | 61  | bond0
bond0.62       | 62  | bond0
bond0.63       | 63  | bond0
	  </screen>

	</listitem>

	</orderedlist>
	
      </section>

      <section>
	<title>Configuración para OpenStack</title>
	<para>En nuestra configuración, hemos optado por el uso de bonding más
	el uso de VLAN, tras seguir los pasos anteriores habría que configurar
	el fichero <filename>/etc/network/interfaces</filename> tal como se
	muestra a continuación.</para>

	<note>
	  <para>En los nodos de computación solo se han configurado cuatro
	  interfaces de las seis disponibles. Básicamente por si hiciesen
	  falta en un futuro para otro tipo de conectividad.</para>
	</note>

	<para>En <literal>jupiter</literal>, el nodo controlador:</para>
	<programlisting>
<xi:include parse="text" href="samples/interfaces.jupiter"/>
	</programlisting>

	<para>En <literal>io</literal>, uno de los nodos de computación (en el
	resto de nodos solo habría que cambiar las direcciones IP):</para>
	<programlisting>
<xi:include parse="text" href="samples/interfaces.io"/>
	</programlisting>
	
      </section>

    </section>

    <section>
      <title>Instalación de Ubuntu 12.04.</title>
      <para>La máquina <literal>jupiter</literal> incluye dos discos duros
      SATAII de 500 GiB y una controladora de disco <literal>3ware 9650SE</literal>. Se ha optado
      por configurar esta controladora de disco en modo RAID 1, para incluir un
      nivel elemental de seguridad y consistencia de datos, aunque obviamente
      esto no descarta la utilización adicional de otros mecanismos de copias de
      seguridad que se configurarán posteriormente. Al tratarse de una
      controladora RAID hardware y estar configurada previamente, el sistema
      operativo que arranque en el equipo sólo verá un disco duro en
      <literal>/dev/sda</literal> de aproximadamente 500 GiB.</para>

      <para>Para prevenir corrupción de datos es muy importante que la
      controladora RAID se configure con una política de escritura
      <emphasis>Write Through</emphasis>. En esta configuración, el rendimiento
      del RAID se resiente un poco, pero lo hace más inmune a una posible
      corrupción de datos en caso de cortes en el suministro eléctrico.</para>

      <para>El sistema operativo elegido para los equipos del cloud
      es la distribución de GNU/Linux Ubuntu 12.04 LTS 64 bits, que actualmente se
      conoce con el nombre en código <emphasis>Precise
      Pangolin</emphasis>. Pensamos que es la mejor opción entre todas las
      distribuciones de GNU/Linux ya que es la utilizada en toda la documentación
      de OpenStack y la mejor soportada.</para>

      <para>Durante la instalación realizamos el siguiente esquema de
      particionamiento:</para>
      <programlisting>
	Disposit.   Tamaño    Id    Sistema     SF
	/dev/sda1    32GiB    83    Linux       ext4
	/dev/sda2     4GiB    82    Linux swap  intercambio
	/dev/sda3  -resto-    8e    Linux LVM   volúmenes físicos
      </programlisting>

      <para>Crearemos tres particiones, una partición de unos 32 GiB para el
      sistema (directorio <literal>/</literal>), una partición de 4 GiB como
      área de intercambio, y el resto configurado para su uso a través de
      volúmenes lógicos con LVM.</para>

      <para>Tras la instalación del sistema, configuraremos el subsistema LVM
      pero sin llegar a crear ningún volumen:</para>
      <programlisting>
	COMPLETAR
      </programlisting>

      <para>Aunque la controladora RAID es hardware y el sistema
      operativo no la gestiona, es importante que se pueda controlar
      su estado a través de algún módulo del kernel. En este caso el
      módulo <literal>3w-9xxx</literal> que se carga automáticamente y nos
      envía estos mensajes al log del sistema:</para>
      <programlisting>
	[    2.799824] 3ware 9000 Storage Controller device driver for Linux v2.26.02.014.
	[    2.799867] 3w-9xxx 0000:01:00.0: PCI INT A -> GSI 19 (level, low) -> IRQ 19
	[    2.799880] 3w-9xxx 0000:01:00.0: setting latency timer to 64
	[    3.080257] 3w-9xxx: scsi6: Found a 3ware 9000 Storage Controller at 0xfe8df000, IRQ: 19.
	[    3.416117] 3w-9xxx: scsi6: Firmware FE9X 4.08.00.006, BIOS BE9X 4.08.00.001, Ports: 2.
	[   12.307881] 3w-9xxx: scsi6: ERROR: (0x03:0x0101): Invalid command opcode:opcode=0x85.
	[   12.308300] 3w-9xxx: scsi6: ERROR: (0x03:0x0101): Invalid command opcode:opcode=0x85.
	[   12.308695] 3w-9xxx: scsi6: ERROR: (0x03:0x0101): Invalid command opcode:opcode=0x85.
	[127301.671991] 3w-9xxx: scsi6: AEN: INFO (0x04:0x0029): Verify started:unit=0.
	[131817.864145] 3w-9xxx: scsi6: AEN: INFO (0x04:0x002B): Verify completed:unit=0.
      </programlisting>           
      <para>Pendiente de hacer:</para>
      <para>http;//jonas.genannt.name/  Hay repositorio, con
      aplicaciones para manejar el RAID 3ware 9650SE</para>

      <para>Como software a instalar solo instalaremos el servidor SSH, a través
      del comando:</para>

      <screen><prompt>$ </prompt><userinput>apt-get install
      openssh-server</userinput></screen>

      <para>Tras finalizar la instalación actualizaremos la máquina a través de
      los comandos:</para>
      <screen>
	<prompt>$ </prompt><userinput>apt-get update</userinput>
	<prompt>$ </prompt><userinput>apt-get upgrade</userinput>
	<prompt>$ </prompt><userinput>apt-get dist-upgrade</userinput>
      </screen>

      <para>Tras la instalación, lo más probable es que tengamos que reiniciar.</para>

    </section>

    <section>
      <title>NTP</title>
      <para>Para mantener todos los servicios sincronizados (a nivel de fecha y
      hora) es necesario instalar un cliente NTP (Network Time Protocol). En el
      caso de instalaciones multinodo hay que configurar uno de los nodos como
      servidor NTP, o confiar en otro servidor de nuestra red o de
      Internet.</para>

      <para>La red <literal>iescierva.net</literal> ya cuenta con un servidor
      NTP, por lo que únicamente hay que configurar correctamente el cliente,
      para ello basta con que sigamos los siguientes pasos:</para>

      <orderedlist>
	<listitem>
	  <para>Nos aseguramos que el paquete <literal>ntpdate</literal> esté
	  instalado.</para>
	  <screen><prompt>root@jupiter:~# </prompt><userinput>dpkg --list | grep ntpdate</userinput></screen>
	  <para>Si no lo estuviera lo instalamos:</para>
	  <screen><prompt>root@jupiter:~# </prompt><userinput>apt-get install ntpdate</userinput></screen>

	</listitem>
	
	<listitem>
	  <para>Configuramos crontab para que se ejecute el comando
	  <command>ntpdate</command> de forma periódica. Para ello ejecutamos
	  (como root) el comando <command>crontab -e</command> y editamos el
	  fichero que nos sugiere con el siguiente contenido:</para>
	  <programlisting>
	    0 4 * * * ntpdate ntp.iescierva.net ; hwclock --systohc
	  </programlisting>
	  <para>Podemos sustituir el servidor <literal>ntp.iescierva.net</literal> por uno en
	  Internet como <literal>ntp.ubuntu.com</literal> o como
	  <literal>hora.rediris.es</literal>.</para>

	  <para>Podemos comprobar que la configuración es correcta a través del
	  comando <command>crontab -l</command></para>
	  <programlisting>
	    root@jupiter:~# crontab -l
	    # m h  dom mon dow   command
	    0 4 * * * ntpdate ntp.iescierva.net ; hwclock -w
	    root@jupiter:~#
	  </programlisting>
	</listitem>

      </orderedlist>
      
    </section>

    <section>
      <title>MySQL</title>
      <para>Vamos a configurar todos los servicios del Cloud para que utilicen
      como base de datos <literal>MySQL</literal>, en vez de la base de datos
      <literal>SQLite</literal> que usan en la configuración por defecto. Para
      ello será necesario instalar MySQL y fijar una contraseña para el usuario
      root. Para ello seguimos los siguientes pasos:</para>

      <orderedlist>
	<listitem>
	  <para>Instalamos los paquetes necesarios, básicamente el servidor
	  MySQL, y la interfaz (DB-API) de Python para MySQL:</para>
	  <screen><prompt>root@jupiter:~# </prompt><userinput>apt-get install mysql-server python-mysqldb</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Durante la instalación del servidor se nos pedirá que
	  introduzcamos la contraseña para el usuario root de
	  MySQL, en nuestro caso <literal>calex2010!!</literal></para>
	</listitem>

	<listitem>
	  <para>Modificamos la interfaz de escucha de MySQL, aunque inicialmente
	  la fijamos a 0.0.0.0 para que el servidor escuche en todas las
	  interfaces, posteriormente fijaremos la interfaz con la IP de la red
	  privada del Cloud.</para>

	  <para>Configuramos el fichero <filename>/etc/mysql/my.cnf</filename>
	  modificando la siguiente línea:</para>
	  <programlisting>
bind-address        = 127.0.0.1
	  </programlisting>

	  <para>Por esta otra:</para>
	  <programlisting>
bind-address        = 0.0.0.0
	  </programlisting>
	</listitem>

	<listitem>
	  <para>Reiniciamos el demonio de MySQL:</para>
	  <screen><prompt>root@jupiter:~# </prompt><userinput>service mysql restart</userinput></screen>
	</listitem>
	
	<listitem>
	  <para>Probamos el cliente MySQL con la contraseña fijada:</para>
	  <programlisting>
root@jupiter:~# mysql -u root -p
Enter password: ***********
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 414
Server version: 5.5.24-0ubuntu0.12.04.1 (Ubuntu)

Copyright (c) 2000, 2011, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql>	    
	  </programlisting>
	</listitem>

      </orderedlist>
      
    </section>
  </section>

  <section xml:id="id_instUbuntu_keystone">
    <title>Instalación de KeyStone</title>
    <para>En este apartado describiremos la instalación y configuración de unos
    de los servicios básicos de OpenStack, el servicio de autenticación y
    gestión de la identidad (Identity Service): Keystone</para>

    <section>
      <title>¿Qué es Keystone?</title>
      <para>Keystone, es el componente de OpenStack encargado de la
      autentificación y la autorización de los distintos componentes que
      conforman OpenStack. Se incluyó desde la versión Essex y es el encargado
      de dos tareas principales:</para>

      <para>REVISARRRRRRRRRRRRRRRRRRRRRRRRRRRRR</para>
      <itemizedlist>
	<listitem>
	  <para>Gestión de usuarios: Keystone es el encargado de
	  mantener un registro de usuarios y los permisos que tiene cada 
	  uno de ellos.</para>
	</listitem>
	<listitem>
	  <para>Registro los servicios ofrecidos: Keystone ofrece un catálogo de
	  los servicios ofrecidos, así como información sobre cómo acceder a sus
	  APIs.</para>
	</listitem>
      </itemizedlist>

      <para>Los componentes básicos de los servicios de identidad son:</para>
      <itemizedlist>
	<listitem>
	  <para>Usuario: Podemos guardar su nombre, 
	  correo electrónico y contraseña.</para>
	</listitem>
	<listitem>
	  <para>Proyecto (<emphasis>tenant</emphasis> en la jerga de
	  OpenStack): En un proyecto podemos ejecutar un conjunto de
	  instancias con características en común, por ejemplo pueden estar
	  todas las instancias en el misma red, pueden utilizar una serie de
	  imágenes de sistemas o tener limitado el uso de recursos del
	  cloud.</para>
	</listitem>
	<listitem>
	  <para>Rol: Nos indica qué operaciones puede realizar
	  cada usuario. A un usuario se le pueden asignar diferentes roles
	  en cada proyecto.</para>
	</listitem>
      </itemizedlist>

      <para>Los conceptos fundamentales del <emphasis>registro de
      servicio</emphasis> son:</para>
      <itemizedlist>
	<listitem><para>Servicio: Corresponde a un componente de
	OpenStack que puede utilizar el módulo de
	autentificación.</para></listitem>
	<listitem><para>Endpoints: Representa las URL que nos permiten
	acceder a las API de cada uno de los servicios o componentes de
	OpenStack</para></listitem>
      </itemizedlist>
      
    </section>

    <section>
      <title>Instalación y configuración</title>
      <para>Una infraestructura OpenStack solo necesita un servidor que ejecute
      el servicio KeyStone, en nuestra instalación el servicio se ejecutará en
      el controlador, es decir, en la máquina <literal>jupiter</literal>.</para>

      <para>Tanto para la instalación de Keystone, como para la instalación del resto
      de servicios, partiremos de los repositorios oficiales de Ubuntu, basta
      con que sigamos los siguientes pasos:</para>

      <orderedlist>
	<listitem>
	  <para>Creamos la base de datos que el servicio necesita en
	  MySQL. Para ello iniciamos el cliente MySQL:</para>
	  <screen><prompt>root@jupiter:~# </prompt><userinput>mysql -u root -p</userinput>
	  </screen>
	  <para>Desde el prompt de MySQL ejecutamos las siguientes
	  sentencias:</para>
	  <screen>
	    <prompt>mysql> </prompt><userinput>CREATE DATABASE keystone;</userinput>
	    <prompt>mysql> </prompt><userinput>GRANT ALL ON keystone.* to 'keystone'@'%' IDENTIFIED BY 'calex2010!!';</userinput>
	    <prompt>mysql> </prompt><userinput>FLUSH PRIVILEGES;</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Instalamos todo el software necesario:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>apt-get install keystone python-keystone python-keystoneclient</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Keystone utiliza por defecto una base de datos <literal>SQLite</literal>,
	  por lo que tendremos que borrar la base de datos que se configura en
	  la instalación por defecto:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>rm /var/lib/keystone/keystone.db</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Editamos el fichero de configuración principal de Keystone para
	  realizar los siguientes cambios:</para>
	  <itemizedlist>
	    <listitem>
	      <para>La cadena de conexión a la base de datos
	      <literal>keystone</literal> de MySQL.</para>
	    </listitem>

	    <listitem>
	      <para>El token administrativo (admin_token).</para>
	    </listitem>   
	  </itemizedlist>

	  <para>Para ello editamos el fichero
	  <filename>/etc/keystone/keystone.conf</filename> haciendo las
	  siguientes modificaciones:</para>
	  <programlisting>
admin_token = CALEX2010!!
connection = mysql://keystone:calex2010!!@172.20.254.190/keystone
	  </programlisting>
	  <para>Ver nota posterior sobre <literal>admin_token</literal>.</para>
	</listitem>

	<listitem>
	  <para>Reiniciamos el servicio:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>service keystone restart</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Creamos la base de datos inicial:</para>
	  <screen><prompt>root@jupiter:~# </prompt><userinput>keystone-manage db_sync</userinput></screen>
	</listitem>

	<listitem>
	  <para>Exportamos las siguientes variables de entorno en el fichero
	  <filename>.bashrc</filename> del usuario que ejecute el cliente, en
	  principio el usuario <literal>root</literal> de la máquina
	  <literal>jupiter</literal>:</para>
	  <programlisting language="Bash">
export SERVICE_ENDPOINT="http://172.20.254.190:35357/v2.0"
export SERVICE_TOKEN=CALEX2010!!
	  </programlisting>
	</listitem>

	<listitem>
	  <para>Creamos los usuarios, proyectos (tenants) y roles.</para>

	  <para>Crearemos ciertos usuarios a los que daremos ciertos roles en
	  ciertos proyectos, tal como se muestra en la siguiente tabla:</para>

	  <table rules="all">
	    <caption>Usuarios, proyectos y roles</caption>
	    <col width="34%"/>
	    <col width="33%"/>
	    <col width="33%"/>
	    <thead>
	      <tr>
		<td>Usuarios</td>
		<td>Proyectos (tenants)</td>
		<td>Roles</td>
	      </tr>
	    </thead>
	    <tbody>
	      <tr>
		<td>admin</td>
		<td>admin</td>
		<td>admin</td>
	      </tr>
	      <tr>
		<td>
		  <para>nova</para>
		  <para>glance</para>
		  <para>swift</para>
		</td>
		<td>service</td>
		<td>admin</td>
	      </tr>
	      <tr>
		<td>admin</td>
		<td>admin</td>
		<td>Member</td>
	      </tr>
	    </tbody>
	  </table>

	  <para>En la tabla podemos leer que al usuario <literal>admin</literal>
	  le daremos el rol <literal>admin</literal> en el proyecto
	  <literal>admin</literal>, o que a los usuarios <literal>nova</literal>,
	  <literal>glance</literal> y <literal>swift</literal> les daremos el
	  rol <literal>admin</literal> en el proyecto <literal>service</literal>.</para>

	  <para>La creación de usuarios, proyectos y roles es un proceso
	  tedioso, muy repetitivo y propenso a errores, por lo que ejecutaremos
	  el siguiente script que nos simplificará mucho este proceso:</para>

	  <programlisting language="Bash">
<xi:include parse="text" href="src/roles.sh"/>
	  </programlisting>

	  <para>Para la correcta ejecución de script hay que configurar las
	  siguientes variables de entorno en el inicio del script:</para>

	  <itemizedlist>
	    <listitem>
	      <para><emphasis role="bold">PASSWORD</emphasis>: contraseña de
	      acceso a Keystone, como no hay ningún usuario creado hasta hora,
	      la única forma de autenticarse es a través del token
	      administrativo definido anteriomente.</para>
	    </listitem>
	    
	    <listitem>
	      <para><emphasis role="bold">email</emphasis>: al crear los
	      usuarios hay que asignar una dirección de correo, al tratarse de
	      usuarios administrativos, podemos utilizar la misma cuenta para
	      todos.</para>
	    </listitem>

	    <listitem>
	      <para><emphasis role="bold">PUBLIC_IP</emphasis>: dirección
	      pública de acceso a los servicios del cloud.</para>
	    </listitem>

	    <listitem>
	      <para><emphasis role="bold">PRIVATE_IP</emphasis>: dirección
	      privada de acceso a los servicios del cloud.</para>
	    </listitem>

	    <listitem>
	      <para><emphasis role="bold">ADMIN_IP</emphasis>: utilizaremos la
	      misma dirección que la proporcionada a la red privada.</para>
	    </listitem>

	  </itemizedlist>
	  


	  <para>El script se limita a:</para>

	  <itemizedlist>
	    <listitem>
	      <para>Crear los proyectos necesarios: <literal>admin</literal> y
	      <literal>service</literal>.</para>
	    </listitem>

	    <listitem>
	      <para>Crear todos los usuarios y asignarles contraseña y
	      dirección de correo electrónico. Los usuarios creados son:
	      <literal>admin</literal>, <literal>nova</literal>,
	      <literal>glance</literal> y <literal>swift</literal>.</para>
	    </listitem>

	    <listitem>
	      <para>Crear los roles necesarios: <literal>admin</literal>y <literal>Member</literal></para>
	    </listitem>

	    <listitem>
	      <para>Asignar roles a los usuarios en los proyectos tal como se
	      mostró en la tabla anterior.</para>
	    </listitem>

	    <listitem>
	      <para>Crear los servicios (<literal>nova</literal>,
	      <literal>volume</literal>, <literal>glance</literal>, <literal>swift</literal>,
	      <literal>keystone</literal> y <literal>ec2</literal>. Define los
	      endpoints y los asigna a los servicios.</para>
	    </listitem>
	    
	  </itemizedlist>

	</listitem>

	<listitem>
	  <para>Verificamos que todo se ha creado correctamente a través de los
	  siguientes comandos (los identificadores no serán los mismos):</para>
	  <programlisting>
root@jupiter:~# keystone tenant-list
+----------------------------------+---------+---------+
|                id                |   name  | enabled |
+----------------------------------+---------+---------+
| 634675c752634b53879656c81da70a83 | service | True    |
| e7b1868b24a742318f1d73f612ecfe1d | admin   | True    |
+----------------------------------+---------+---------+
root@jupiter:~# keystone user-list
+----------------------------------+---------+--------------------+--------+
|                id                | enabled |       email        |  name  |
+----------------------------------+---------+--------------------+--------+
| 05743001bbf14700bcdf2ecc43edbf9b | True    | alex@iescierva.net | admin  |
| 246ba4e3d81c4ae8bdde8ec5784e74d3 | True    | alex@iescierva.net | swift  |
| 291c58f7258747758d109ecee2eb4a69 | True    | alex@iescierva.net | glance |
| 404dafc53b364a7e9f1476aa98082966 | True    | alex@iescierva.net | nova   |
+----------------------------------+---------+--------------------+--------+
root@jupiter:~# keystone role-list
+----------------------------------+--------+
|                id                |  name  |
+----------------------------------+--------+
| 2a716d669ce349eb88e44049723e0cb7 | admin  |
| 622c78f77bbe4a298452783f25cb4635 | Member |
+----------------------------------+--------+
root@jupiter:~# keystone endpoint-list
+----------------------------------+----------+--------------------------------------------------+--------------------------------------------------+---------------------------------------------+
|                id                |  region  |                    publicurl                     |                   internalurl                    |                   adminurl                  |
+----------------------------------+----------+--------------------------------------------------+--------------------------------------------------+---------------------------------------------+
| 0dd9427a0a624b38abf1876651ac3b69 | myregion | http://172.20.254.190:8776/v1/$(tenant_id)s      | http://172.20.253.190:8776/v1/$(tenant_id)s      | http://172.20.253.190:8776/v1/$(tenant_id)s |
| 2d471f9b23c84bac91c17615e157bb9f | myregion | http://172.20.254.190:5000/v2.0                  | http://172.20.253.190:5000/v2.0                  | http://172.20.253.190:35357/v2.0            |
| c9f799e2c6444bbda40dbc059207de09 | myregion | http://172.20.254.190:8773/services/Cloud        | http://172.20.253.190:8773/services/Cloud        | http://172.20.253.190:8773/services/Admin   |
| e72307edc9b84c2d9403b019c3a2e5a7 | myregion | http://172.20.254.190:9292/v1                    | http://172.20.253.190:9292/v1                    | http://172.20.253.190:9292/v1               |
| ec8b9d8d41ea496492730b20e1c34380 | myregion | http://172.20.254.190:8080/v1/AUTH_$(tenant_id)s | http://172.20.253.190:8080/v1/AUTH_$(tenant_id)s | http://172.20.253.190:8080/v1               |
| edf05355fc274c02a13f900605c56769 | myregion | http://172.20.254.190:8774/v2/$(tenant_id)s      | http://172.20.253.190:8774/v2/$(tenant_id)s      | http://172.20.253.190:8774/v2/$(tenant_id)s |
+----------------------------------+----------+--------------------------------------------------+--------------------------------------------------+---------------------------------------------+
root@jupiter:~# keystone service-list
+----------------------------------+----------+--------------+----------------------------+
|                id                |   name   |     type     |        description         |
+----------------------------------+----------+--------------+----------------------------+
| 04b121f16e6045a79a4601c5171998d3 | glance   | image        | OpenStack Image Service    |
| 298de2e4a0f04b42b54818fd57d1bf2e | keystone | identity     | OpenStack Identity Service |
| 368b238c02734ee4ac1a6ac6bf440ffa | nova     | compute      | OpenStack Compute Service  |
| a075b6add5564f729253dd0339b1d5d9 | volume   | volume       | OpenStack Volume Service   |
| c23937017a7e4baa8811a24ff9c8086c | swift    | object-store | OpenStack Storage Service  |
| e998168d3d264e3aab37d4b8413fe736 | ec2      | ec2          | OpenStack EC2 Service      |
+----------------------------------+----------+--------------+----------------------------+
root@jupiter:~# 
	  </programlisting>
	</listitem>
      </orderedlist>
    </section>
  </section>

  <section xml:id="id_instUbuntu_glance">
    <title>Instalación de Glance</title>
    <para>Esta sección describe la instalación y configuración del
    módulo de OpenStack, Glance. Este servicio es el encargado de la
    gestión y registro de las imágenes que posteriormente se van a poder
    instanciar en máquinas virtuales.</para>

    <section>
      <title>¿Qué es Glance?</title>
      <para></para>
      
    </section>

    <section>
      <title>Instalación y configuración</title>
      <para>Una infraestructura OpenStack solo necesita un servidor que ejecute
      el servicio Glance, en nuestra instalación el servicio se ejecutará en el
      controlador, es decir, en la máquina <literal>jupiter</literal>.</para>

      <para>Tanto para la instalación de Glance, como para la instalación del resto
      de servicios, partiremos de los repositorios oficiales de Ubuntu, basta
      con que sigamos los siguientes pasos:</para>

      <orderedlist>
	<listitem>
	  <para>Creamos la base de datos que el servicio necesita en MySQL. Para
	  ello iniciamos el cliente MySQL:</para>
	  <screen><prompt>root@jupiter:~# </prompt><userinput>mysql -u root -p</userinput>
	  </screen>
	  <para>Desde el prompt de MySQL ejecutamos las siguientes
	  sentencias:</para>
	  <screen>
	    <prompt>mysql> </prompt><userinput>CREATE DATABASE glance;</userinput>
	    <prompt>mysql> </prompt><userinput>GRANT ALL ON glance.* to 'glance'@'%' IDENTIFIED BY 'calex2010!!';</userinput>
	    <prompt>mysql> </prompt><userinput>FLUSH PRIVILEGES;</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Instalamos todo el software necesario:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>apt-get install glance glance-api glance-client glance-common glance-registry python-glance</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Modificamos las siguientes líneas del fichero de configuración
	  <filename>/etc/glance/glance-api-paste.ini</filename>, modificando
	  estas líneas:</para>
	  <programlisting>
admin_tenant_name = %SERVICE_TENANT_NAME%
admin_user = %SERVICE_USER%
admin_password = %SERVICE_PASSWORD%
	  </programlisting>

	  <para>Por estas otras:</para>
	  <programlisting>
admin_tenant_name = service
admin_user = glance
admin_password = calex2010!!
	  </programlisting>
	</listitem>

	<listitem>
	  <para>El mismo cambio anterior hay que hacerlo también en el fichero
	  <filename>/etc/glance/glance-registry-paste.ini</filename>, dejando
	  las últimas líneas así:</para>
	  <programlisting>
admin_tenant_name = service
admin_user = glance
admin_password = calex2010!!
	  </programlisting>
	</listitem>

	<listitem>
	  <para>Editamos el fichero
	  <filename>/etc/glance/glance-registry.conf</filename> y modificamos la
	  cadena de conexión a la base de datos MySQL recién creada:</para>
	  <programlisting>
sql_connection = mysql://glance:calex2010!!@172.20.254.190/glance
	  </programlisting>

	  <para>También añadimos la siguiente configuración para que
	  <literal>Glance></literal> utilice <literal>Keystone</literal> como
	  servicio de autenticación.</para>
	  <programlisting>
[paste_deploy]
flavor = keystone
	  </programlisting>
	</listitem>

	<listitem>
	  <para>Realizamos también este último cambio al fichero
	  <filename>/etc/glance/glance-api.conf:</filename></para>
	  <programlisting>
[paste_deploy]
flavor = keystone
	  </programlisting>
	</listitem>

	<listitem>
	  <para>Creamos el esquema de la base de datos:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>glance-manage version_control 0</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>glance-manage db_sync</userinput>
	  </screen>
	  <para>Podemos ignorar los warnings de Python.</para>
	</listitem>

	<listitem>
	  <para>Reiniciamos los demonios del servicio Glance:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>service glance-api restart</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service glance-registry restart</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Añadimos las siguientes variables de entorno en el fichero
	  .bashrc del usuario que ejecute el cliente glance, en principio al
	  usuario <literal>root</literal> de la máquina <literal>jupiter</literal>:</para>
	  <programlisting language="Bash">
export SERVICE_TOKEN=CALEX2010!!
export OS_TENANT_NAME=admin
export OS_USERNAME=admin
export OS_PASSWORD=calex2010!!
export OS_AUTH_URL="http://172.20.254.190:5000/v2.0/"
export SERVICE_ENDPOINT=http://172.20.254.190:35357/v2.0
	  </programlisting>
	</listitem>

	<listitem>
	  <para>Probamos el servicio.</para>

	  <para>Nada más instalar, no hay ninguna imagen dada de alta, pero el
	  siguiente comando, que muestra la lista de imágenes disponible, nos
	  debería dar una lista vacía:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>glance index</userinput>
	  </screen>
	  <para>Podemos asegurarnos que todo ha ido bien (incluyendo la
	  autenticación con Keystone) si tras ejecutar el comando anterior, la
	  salida del comando <command>echo $?</command> nos devuelve como código
	  de salida el cero.</para>
	</listitem>

      </orderedlist>

      <para>Solo nos queda probar el servicio Glance subiendo alguna imagen de
      prueba. Para subir una imagen, podemos seguir estos pasos:</para>

      <orderedlist>
	<listitem>
	  <para>Descargamos la imagen Cirros desde la siguiente URL:</para>
	  <itemizedlist>
	    <listitem>
	      <para><link xlink:href="https://launchpad.net/cirros/+download">https://launchpad.net/cirros/+download</link></para>
	    </listitem>
	  </itemizedlist>
	  <note>
	    <para>CirrOS es una proyecto FLOSS cuyo objetivo principal es la
	    construcción de una pequeña distribución GNU/Linux especializada en
	    su ejecución en infraestructuras de Cloud Computing. Viene
	    acompañada de herramientas para la depuración, desarrollo y
	    despliegue en este tipo de infraestructuras.</para>
	  </note>

	  <para>Tenemos más imágenes prefabricadas en sitios web como: <link
	  xlink:href="http://uec-images.ubuntu.com/">Ubuntu Cloud
	  Images.</link></para>

	  <para>Más información sobre la creación de imágenes en la sección:
	  "Gestión de Imágenes".</para>
	</listitem>

	<listitem>
	  <para>Damos la imagen de alta en Glance a través del siguiente comando:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>glance add name="Cirros (test) 0.3.0 64 bits" is_public=true container_format=bare disk_format=qcow2 &lt; cirros-0.3.0-x86_64-disk.img</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Si todo es correcto, podremos ver ahora la imagen recién subida
	  en la lista de imágenes que proporciona el servicio Glance:</para>
	  <programlisting>
root@jupiter:~# glance index
ID                                   Name                           Disk Format          Container Format     Size          
------------------------------------ ------------------------------ -------------------- -------------------- --------------
fe22ea9c-ebb2-4bb8-be34-ea7732a8a3d2 Cirros (test) 0.3.0 64 bits    qcow2                bare                        9761280
root@jupiter:~# 
	  </programlisting>
	</listitem>	
      </orderedlist>
    </section>    
  </section>

  <section xml:id="id_instUbuntu_nova">
    <title>Instalación de Nova</title>
    <para>Este capítulo describe la instalación y configuración del
    módulo de OpenStack, Nova. Este servicio es el encargado de gestionar las
    instancias (máquinas virtuales) del Cloud, por lo que lo convierte en el
    servicio central de toda la infraestructura de Openstack.</para>

    <section>
      <title>¿Qué es Nova?</title>
      <para>COMPLETAR POSTERIORMENTE</para>
      
    </section>

    <section>
      <title>Instalación y configuración</title>
      <para>Una infraestructura OpenStack puede ejecutar tantos nodos
      <literal>nova</literal> como desee. Como mínimo uno, esto hace que sea posible tener una
      infraestructura de OpenStack en un solo nodo, pero podemos desplegar
      tantos nodos de computación como servidores tengamos. El máximo depende de
      diversos factores como la carga a soportar, la disponibilidad de
      servidores, factores económicos, ancho de banda de nuestra red, uso del
      Cloud, y un largo etcétera. En nuestra instalación el servicio
      <literal>nova-compute</literal> se ejecutará en cuatro nodos,
      concretamente en las máquinas <literal>io</literal>,
      <literal>europa</literal>, <literal>ganimedes</literal> y
      <literal>calisto</literal>. Inicialmente configuraremos al controlador
      (<literal>jupiter</literal>) como nodo de computación, pero lo
      eliminaremos una vez que vayamos añadiendo a los otros nodos..</para>

      <para>Tanto para la instalación de Nova, como para la instalación del resto
      de servicios, partiremos de los repositorios oficiales de Ubuntu, basta
      con que sigamos los siguientes pasos en la máquina
      <literal>jupiter</literal>:</para>

      <orderedlist>
	<listitem>
	  <para>Instalamos todo el software necesario:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>apt-get install nova-api nova-cert nova-compute nova-compute-kvm nova-doc nova-network nova-objectstore nova-scheduler nova-volume rabbitmq-server novnc nova-consoleauth</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Creamos la base de datos que el servicio necesita en MySQL. Para
	  ello iniciamos el cliente MySQL:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>mysql -u root -p</userinput>
	  </screen>
	  <para>Desde el prompt de MySQL ejecutamos las siguientes
	  sentencias:</para>
	  <screen>
	    <prompt>mysql> </prompt><userinput>CREATE DATABASE nova;</userinput>
	    <prompt>mysql> </prompt><userinput>GRANT ALL ON nova.* to 'nova'@'%' IDENTIFIED BY 'calex2010!!';</userinput>
	    <prompt>mysql> </prompt><userinput>FLUSH PRIVILEGES;</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Editamos el fichero <filename>/etc/nova/nova.conf</filename> con
	  el siguiente contenido: </para>
	  <programlisting>
<xi:include parse="text" href="samples/nova.conf.jupiter"/>
	  </programlisting>
	</listitem>

	<listitem>
	  <para>Para el trabajo con volúmenes, debemos tener configurado un
	  grupo de volúmenes LVM denominado <literal>nova-volumes</literal>, tal
	  como hemos definido a través del parámetro
	  <literal>volume-group</literal> en el fichero
	  <filename>nova.conf</filename>. Para crear el volumen ejecutamos los
	  siguientes comandos, suponiendo una partición <literal>sda4</literal>
	  libre:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput> pvcreate /dev/sda4</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput> vgcreate nova-volumes /dev/sda4</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Cambiamos los permisos del directorio /etc/nova y del fichero
	  /etc/nova/nova.conf:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>chown -R nova.nova /etc/nova</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>chmod 644 /etc/nova/nova.conf</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Editamos el fichero /etc/nova/</para>
	</listitem>

	<listitem>
	  <para>Modificamos las siguientes líneas del fichero de configuración
	  <filename>/etc/nova/api-paste.ini</filename>, modificando
	  estas líneas:</para>
	  <programlisting>
admin_tenant_name = %SERVICE_TENANT_NAME%
admin_user = %SERVICE_USER%
admin_password = %SERVICE_PASSWORD%
	  </programlisting>

	  <para>Por estas otras:</para>
	  <programlisting>
admin_tenant_name = service
admin_user = nova
admin_password = calex2010!!
	  </programlisting>
	</listitem>

	<listitem>
	  <para>Creamos el esquema de la base de datos que Nova necesita:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>nova-manage db sync</userinput>
	  </screen>
	  <para>Podemos ignorar los warnings de Python.</para>
	</listitem>

	<listitem>
	  <para>Reiniciamos todos los demonios del servicio Nova:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>service novnc stop</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-consoleauth stop</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-volume stop</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-schedule  stop</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-objectstore stop</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-api stop</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-compute stop</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-network stop</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-cert stop</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service libvirt-bin stop</userinput>

	    <prompt>root@jupiter:~# </prompt><userinput>service libvirt-bin start</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-cert start</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-network start</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-compute start</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-api start</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-objectstore start</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-schedule  start</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-volume start</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service nova-consoleauth start</userinput>
	    <prompt>root@jupiter:~# </prompt><userinput>service novnc start</userinput>
	  </screen>

	  <para>Mientras que trabajemos con el servicio Nova (instalación,
	  configuración, pruebas...), va a ser muy frecuente tener que reiniciar
	  todos sus servicios asociados, podemos facilitar esta tarea a través
	  del siguiente script, que podremos guardar en <filename>/root/bin/nova.sh</filename>:</para>

	  <programlisting language="Bash">
<xi:include parse="text" href="src/nova.sh"/>
	  </programlisting>
	</listitem>

	<listitem>
	  <para>Proporcionamos una red (rango de IPs) que asociaremos a las
	  instancias, lo podemos hacer a través del siguiente comando:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>nova-manage network create private --fixed_range_v4=10.0.0.0/24 --num_networks=1 --bridge=br100 --bridge_interface=bond0.60 --network_size=256 --dns1=172.20.254.235</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Nos aseguramos de tener definidas las siguientes variables de
	  entorno en la máquina que haga de cliente Nova:</para>
	  <programlisting language="Bash">
export SERVICE_TOKEN=CALEX2010!!
export OS_TENANT_NAME=admin
export OS_USERNAME=admin
export OS_PASSWORD=calex2010!!
export OS_AUTH_URL="http://172.20.254.190:5000/v2.0/"
export SERVICE_ENDPOINT=http://172.20.254.190:35357/v2.0
	  </programlisting>
	</listitem>

	<listitem>
	  <para>Volvemos a reiniciar todos los servicios:</para>
	  <screen>
	    <prompt>$ </prompt><userinput>/root/bin/nova.sh restart</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Comprobamos que todos los servicios estén iniciados, lo podemos
	  hacer a través del comando:</para>
	  <screen>
	    <prompt>$ </prompt><userinput>nova-manage service list</userinput>
	  </screen>
	  <para>...que nos debería proporcionar una salida similar a la
	  siguiente:</para>
	  <programlisting>
root@jupiter:~# nova-manage service list
Binary           Host                                 Zone             Status     State Updated_At
nova-consoleauth jupiter                              nova             enabled    :-)   2012-07-28 10:02:04
nova-cert        jupiter                              nova             enabled    :-)   2012-07-28 10:02:05
nova-scheduler   jupiter                              nova             enabled    :-)   2012-07-28 10:02:03
nova-compute     jupiter                              nova             enabled    :-)   2012-07-28 10:01:57
nova-volume      jupiter                              nova             enabled    :-)   2012-07-28 10:02:04
nova-network     jupiter                              nova             enabled    :-)   2012-07-28 10:02:03
	  </programlisting>
	  <para>Es posible que algún servicio no arranque de inmediato, y
	  muestre en el anterior listado la cadena "XXX" en el campo
	  <literal>State</literal>, en ese caso esperamos pero comprobamos
	  mientras que el servicio está realmente en ejecución con el comando
	  (en el caso de <literal>nova-compute</literal>):</para>
	  <screen>
	    <prompt>$ </prompt><userinput>ps ax | grep nova-compute</userinput>
	  </screen>
	  <para>Si el servicio no está en ejecución, algo ha ido mal (o bastante
	  mal), por lo que tendremos que revisar los logs:</para>
	  <screen>
	    <prompt>$ </prompt><userinput>less /var/log/nova/nova-compute.log</userinput>
	  </screen>

	  <note>
	    <para>El servicio <literal>nova-compute</literal> suele mostrar las
	    XXX durante bastante tiempo, no mostrará una cara feliz
	    mientras que haya máquinas virtuales sin iniciar.</para>
	  </note>

	</listitem>

	<listitem>
	  <para>Arrancamos una imagen de prueba.</para>
	  <para>Podemos comprobar las imágenes que podemos iniciar a través del
	  servicio Glance con cualquiera de los siguientes comandos:</para>
	  <screen>
	    <prompt>$ </prompt><userinput>glance index</userinput>
	  </screen>

	  <screen>
	    <prompt>$ </prompt><userinput>nova image-list</userinput>
	  </screen>

	  <para>Generamos las claves SSH necesarias para conectarnos a través
	  del protocolo SSH a las máquinas virtuales una vez iniciadas:</para>
	  <screen>
	    <prompt>$ </prompt><userinput>nova keypair-add test > /root/test.pem</userinput>
	  </screen>

	  <para>Para iniciar una instancia necesitamos el nombre de la imagen,
	  el flavor (podemos crear más) y las claves SSH anteriormente
	  generadas, ejecutamos el siguiente comando:</para>
	  <screen>
	    <prompt>$ </prompt><userinput>nova boot --image "Cirros (test) 0.3.0 64 bits" --flavor m1.small --key_name test my-first-server</userinput>
	  </screen>

	  <para>Podemos comprobar el estado de la instancia a través del
	  comando:</para>
	  <screen>
	    <prompt>$ </prompt><userinput>nova list</userinput>
	  </screen>

	  <para>... que debería mostrar una salida como ésta:</para>
	  <programlisting>
root@jupiter:~# nova list
+--------------------------------------+-----------------+--------+------------------+
|                  ID                  |       Name      | Status |     Networks     |
+--------------------------------------+-----------------+--------+------------------+
| 95db6cb7-6d29-4728-8a57-00f0a16fdf0f | my-first-server | ACTIVE | private=10.0.0.2 |
+--------------------------------------+-----------------+--------+------------------+
root@jupiter:~#
	  </programlisting>

	  <para>Y obtener más información a través del UID de la instancia con
	  el comando:</para>
	  <screen>
	    <prompt>$ </prompt><userinput>nova show &lt;id&gt;</userinput>
	  </screen>

	  <para>... de esta forma:</para>
	  <programlisting>
root@jupiter:~# nova show 95db6cb7-6d29-4728-8a57-00f0a16fdf0f
+-------------------------------------+----------------------------------------------------------+
|               Property              |                          Value                           |
+-------------------------------------+----------------------------------------------------------+
| OS-DCF:diskConfig                   | MANUAL                                                   |
| OS-EXT-SRV-ATTR:host                | jupiter                                                  |
| OS-EXT-SRV-ATTR:hypervisor_hostname | None                                                     |
| OS-EXT-SRV-ATTR:instance_name       | instance-00000001                                        |
| OS-EXT-STS:power_state              | 1                                                        |
| OS-EXT-STS:task_state               | None                                                     |
| OS-EXT-STS:vm_state                 | active                                                   |
| accessIPv4                          |                                                          |
| accessIPv6                          |                                                          |
| config_drive                        |                                                          |
| created                             | 2012-07-25T09:02:58Z                                     |
| flavor                              | m1.small                                                 |
| hostId                              | 3321967f22b94709966c1525bfb8fe73e88019b1a03228af56e8b847 |
| id                                  | 95db6cb7-6d29-4728-8a57-00f0a16fdf0f                     |
| image                               | Cirros (test) 0.3.0 64 bits                              |
| key_name                            | test                                                     |
| metadata                            | {}                                                       |
| name                                | my-first-server                                          |
| private network                     | 10.0.0.2                                                 |
| progress                            | 0                                                        |
| status                              | ACTIVE                                                   |
| tenant_id                           | e7b1868b24a742318f1d73f612ecfe1d                         |
| updated                             | 2012-07-26T06:49:51Z                                     |
| user_id                             | 05743001bbf14700bcdf2ecc43edbf9b                         |
+-------------------------------------+----------------------------------------------------------+
root@jupiter:~#	    
	  </programlisting>

	  <para>Nos podemos conectar a través de SSH a la instancia con el comando:</para>
	  <screen>
	    <prompt>$ </prompt><userinput>ssh -i test.pem cirros@10.0.0.2</userinput>
	  </screen>
	</listitem>

      </orderedlist>

      <para>Al iniciar la primera máquina virtual, en el nodo controlador se
      creará el bridge <literal>br100</literal> con una IP en el rango dado, en
      nuestro caso la dirección 10.0.0.1. Es importante destacar el hecho de que
      la dirección IP de la interfaz asociada al bridge "desaparece" pero se
      sigue utilizando, tal como muestra la ejecución de los siguientes
      comandos:</para>
      <programlisting>
root@jupiter:~# ifconfig bond0.60
bond0.60  Link encap:Ethernet  direcciónHW 00:25:90:72:2c:47  
          Dirección inet6: fe80::225:90ff:fe72:2c47/64 Alcance:Enlace
          ACTIVO DIFUSIÓN FUNCIONANDO MULTICAST  MTU:1500  Métrica:1
          Paquetes RX:30800 errores:0 perdidos:0 overruns:0 frame:0
          Paquetes TX:45655 errores:0 perdidos:0 overruns:0 carrier:0
          colisiones:0 long.colaTX:0 
          Bytes RX:4491363 (4.4 MB)  TX bytes:9278130 (9.2 MB)

root@jupiter:~# ifconfig br100
br100     Link encap:Ethernet  direcciónHW 00:25:90:72:2c:47  
          Direc. inet:10.0.0.1  Difus.:10.0.0.255  Másc:255.255.255.0
          Dirección inet6: fe80::c035:49ff:fe37:96e5/64 Alcance:Enlace
          ACTIVO DIFUSIÓN FUNCIONANDO MULTICAST  MTU:1500  Métrica:1
          Paquetes RX:77878 errores:0 perdidos:0 overruns:0 frame:0
          Paquetes TX:53900 errores:0 perdidos:0 overruns:0 carrier:0
          colisiones:0 long.colaTX:0 
          Bytes RX:15617344 (15.6 MB)  TX bytes:9926875 (9.9 MB)

root@jupiter:~# ping -c 5 172.20.253.190
PING 172.20.253.190 (172.20.253.190) 56(84) bytes of data.
64 bytes from 172.20.253.190: icmp_req=1 ttl=64 time=0.070 ms
64 bytes from 172.20.253.190: icmp_req=2 ttl=64 time=0.048 ms
64 bytes from 172.20.253.190: icmp_req=3 ttl=64 time=0.046 ms
64 bytes from 172.20.253.190: icmp_req=4 ttl=64 time=0.042 ms
64 bytes from 172.20.253.190: icmp_req=5 ttl=64 time=0.050 ms

--- 172.20.253.190 ping statistics ---
5 packets transmitted, 5 received, 0% packet loss, time 3996ms
rtt min/avg/max/mdev = 0.042/0.051/0.070/0.010 ms
root@jupiter:~#
      </programlisting>

      <section>
	<title>Instalación de un segundo nodo</title>
	<para>Podemos añadir a nuestra infraestructura tantos nodos de
	computación como deseemos, para ello basta seguir los siguientes
	pasos:</para>

	<orderedlist>
	  <listitem>
	    <para>No instalamos todo el software de nova, solo un
	    paquete:</para>
	  <screen>
	    <prompt>root@io:~# </prompt><userinput>apt-get install nova-compute bridge-utils</userinput>
	  </screen>
	  </listitem>

	  <listitem>
	    <para>El fichero de configuración de Nova,
	    <filename>/etc/nova/nova.conf</filename>, no es exactamente el mismo
	    que en el controlador, hay que realizar los siguientes
	    cambios:</para>
	    <programlisting>
my_ip=172.20.254.191
...
vncserver_proxyclient_address=172.20.253.191
vncserver_listen=172.20.253.191
	    </programlisting>

	    <para>... quedando de la siguiente forma. Nos aseguramos antes de los permisos
	    (<literal>600</literal>) y del propietario y grupo
	    (<literal>nova.nova</literal>):</para>
	  <programlisting>
<xi:include parse="text" href="samples/nova.conf.io"/>
	  </programlisting>

	  </listitem>

	  <listitem>
	    <para>Borramos la base de datos SQLite que acompaña a la instalación
	    de nova-compute:</para>
	    <screen><prompt>$ </prompt><userinput>rm /var/lib/nova/nova.sqlite</userinput></screen>
	  </listitem>

	  <listitem>
	    <para>Revisamos los permisos del directorio
	    <filename>/var/lib/nova</filename>:</para>
	    <screen>
	      <prompt>root@io:~# </prompt><userinput>chown -R nova.nova /var/lib/nova</userinput>
	      <prompt>root@io:~# </prompt><userinput>chmod 770 /var/lib/nova</userinput>
	    </screen>

	  </listitem>

	  <listitem>
	    <para>Si queremos que funcione el cliente nova, al menos para el
	    usuario <literal>root</literal>, añadimos las siguientes variables
	    de entorno a su fichero <filename>~/.bashrc</filename>:</para>
	    <programlisting language="Bash">
export SERVICE_TOKEN=CALEX2010!!
export OS_TENANT_NAME=admin
export OS_USERNAME=admin
export OS_PASSWORD=calex2010!!
export OS_AUTH_URL="http://172.20.254.190:5000/v2.0/"
export SERVICE_ENDPOINT=http://172.20.254.190:35357/v2.0
	    </programlisting>
	  </listitem>

	  <listitem>
	    <para>Reiniciamos el servicio
	    <literal>nova-compute</literal>:</para>
	    <screen>
	      <prompt>root@io:~# </prompt><userinput>service nova-compute restart</userinput>
	    </screen>
	  </listitem>

	  <listitem>
	    <para>Si todo ha ido bien, ahora desde el controlador se debemos
	    poder obtener la siguiente salida...</para>

	    <screen>
	      <prompt>root@jupiter:~# </prompt><userinput>nova-manage service list</userinput>
Binary           Host                                 Zone             Status     State Updated_At
nova-consoleauth jupiter                              nova             enabled    :-)   2012-07-31 15:47:23
nova-cert        jupiter                              nova             enabled    :-)   2012-07-31 15:47:25
nova-scheduler   jupiter                              nova             enabled    :-)   2012-07-31 15:47:24
nova-compute     jupiter                              nova             enabled    :-)   2012-07-31 15:47:21
nova-volume      jupiter                              nova             enabled    :-)   2012-07-31 15:47:25
nova-network     jupiter                              nova             enabled    :-)   2012-07-31 15:47:27
nova-compute     io                                   nova             enabled    :-)   2012-07-31 15:47:29
	    </screen>
	    <para>... en la que se puede ver el nuevo nodo de computación,
	    <literal>io</literal>.</para>
	  </listitem>

	</orderedlist>

      </section>
    </section>

    <section>
      <title>Instalación de Nova: problemas y puntualizaciones</title>
      <para></para>
      <orderedlist>
	<listitem>
	  <para>Nada más añadir un nodo de computación, todo funcionará
	  perfectamente, incluyendo la configuración automática del bridge
	  <literal>br100</literal>, pero todo dejará de hacerlo mostrando
	  diversos errores en los logs de <literal>nova-compute</literal> como
	  este: "Cannot get interface MTU on 'br100': No such device", tras
	  reiniciar el nuevo nodo.</para>

	  <para>Para solucionar este problema hay que:</para>
	  <orderedlist>
	    <listitem>
	      <para>Configurar el bridge manualmente y asociarle la interfaz.</para>
	    </listitem>

	    <listitem>
	      <para>Asociar manualmente la dirección IP de la interfaz al bridge.</para>
	    </listitem>

	  </orderedlist>

	  <para>Basta con modificar el fichero
	  <filename>/etc/network/interfaces</filename> realizando los
	  siguientes cambios:</para>

	  <programlisting>
auto br100
iface br100 inet static
    address 172.20.253.191
    netmask 255.255.255.0
    bridge_stp off
    bridge_fd 0
    bridge_ports bond0.60
    bridge_maxwait 0
	  </programlisting>

	  <programlisting>
auto bond0.60
iface bond0.60 inet manual
    #address 172.20.253.191
    #netmask 255.255.255.0
    #broadcast 172.20.253.255
    #network 172.20.253.0
    vlan-raw-device bond0
	  </programlisting>

	  <para>Tras estos cambios todo funcionará como debería, parece ser
	  que se trata de un bug de OpenStack documentado, pero aún no
	  solucionado en la versión Essex que acompaña a Ubuntu:</para>
	  <itemizedlist>
	    <listitem>
	      <para><link xlink:href="https://bugs.launchpad.net/nova/+bug/1010927">https://bugs.launchpad.net/nova/+bug/1010927</link></para>
	    </listitem>

	    <listitem>
	      <para><link xlink:href="https://review.openstack.org/#/c/8423/">https://review.openstack.org/#/c/8423/</link></para>
	    </listitem>
	    
	  </itemizedlist>
	  <para>El nodo controlador no está afectado por este problema, pero de vez
	  en cuando, el proceso nova-compute no se inicia tras reiniciar el
	  servidor. Basta volver a iniciar el servicio con el comando
	  <command>service nova-compute start</command>.</para>
	</listitem>

	<listitem>
	  <para>Una vez que tengamos máquinas virtuales en ejecución hay un
	  log muy recurrente en los logs del servicio
	  <literal>libvirtd</literal>: "Cannot find 'pm-is-supported' in path:
	  No such file or directory". Este log se elimina instalando el
	  siguiente paquete en todos los nodos de comutación:</para>
	  <screen>
	    <prompt>root@io:~# </prompt><userinput>apt-get install pm-utils</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Podemos borrar una instancia a través del comando de OpenStack
	  <command>nova delete</command>, pero en determinadas situaciones
	  este comando puede fallar. Si queremos eliminar de forma manual una
	  instancia debemos seguir los siguientes pasos:</para>
	  <orderedlist>
	    <listitem>
	      <para>Eliminamos la máquina virtual a través del comando
	      <command>virsh destroy &lt;domain&gt;</command>.</para>
	    </listitem>

	    <listitem>
	      <para>Nos hacemos con su identificador a través de la siguiente
	      consulta en la base de datos <literal>nova</literal>:</para>
	      <screen>
		<prompt>mysql> </prompt><userinput>select id, uuid from instances;</userinput>
	      </screen>
	      <para>Tomando el id de la instancia que queremos borrar,
	      supongamos que queremos eliminar la máquina virtual con
	      <literal>id=99</literal>.</para>
	    </listitem>

	    <listitem>
	      <para>Ejecutamos las siguientes consultas SQL en la base de
	      datos de <literal>nova</literal>:</para>
	      <screen>
		<prompt>mysql> </prompt><userinput>delete from instance_info_caches where id=99;</userinput>
		<prompt>mysql> </prompt><userinput>delete from security_group_instance_association where id=99;</userinput>
		<prompt>mysql> </prompt><userinput>delete from instances where id=99;</userinput>
	      </screen>
	    </listitem>

	    <listitem>
	      <para>Comprobamos a través del comando <command>nova
	      list</command> que la instancia ya no está en ejecución.</para>
	    </listitem>
	    
	  </orderedlist>	    

	</listitem>

	<listitem>
	  <para>Entre pruebas de configuración puede que sea interesante
	  desinstalar por completo nova y sus servicios asociados, para ello
	  seguiremos los siguientes pasos:</para>
	  <orderedlist>
	    <listitem>
	      <para>Detenemos todas las máquinas virtuales que estuvieran en
	      ejecución:</para>
	      <screen>
		<prompt>root@io:~# </prompt><userinput>for i in `virsh list | tail -n +2 | awk ' { print $2 }'`; do virsh destroy $i; done</userinput>
	      </screen>		
	    </listitem>

	    <listitem>
	      <para>Paramos todos los servicios:</para>
	      <screen>
		<prompt>root@io:~# </prompt><userinput>/root/bin/nova.sh stop</userinput>
	      </screen>		
	    </listitem>

	    <listitem>
	      <para>Matamos todos los procesos <literal>dnsmasq</literal>:</para>
	      <screen>
		<prompt>root@io:~# </prompt><userinput>for i in `pgrep dnsmasq` ; do kill $i ; done</userinput>
	      </screen>
	    </listitem>

	    <listitem>
	      <para>Eliminamos el bridge <literal>br100</literal>:</para>
	      <screen>
		<prompt>root@io:~# </prompt><userinput>ifconfig br100 down</userinput>
		<prompt>root@io:~# </prompt><userinput>brctl delbr br100</userinput>
	      </screen>
	    </listitem>

	    <listitem>
	      <para>Eliminamos los siguientes ficheros y directorios:</para>
	      <screen>
		<prompt>root@io:~# </prompt><userinput>rm /etc/libvirt/qemu/instance-*xml -f</userinput>
		<prompt>root@io:~# </prompt><userinput>rm /etc/libvirt/nwfilter/nova-instance-instance-*xml -f</userinput>
		<prompt>root@io:~# </prompt><userinput>rm /var/lib/nova/buckets/ -rf</userinput>
		<prompt>root@io:~# </prompt><userinput>rm /var/lib/nova/CA/ -rf</userinput>
		<prompt>root@io:~# </prompt><userinput>rm /var/lib/nova/instances/* -rf</userinput>
		<prompt>root@io:~# </prompt><userinput>rm /var/lib/nova/keys/ -rf</userinput>
		<prompt>root@io:~# </prompt><userinput>rm /var/lib/nova/networks/* -rf</userinput>
		<prompt>root@io:~# </prompt><userinput>rm /var/lib/rabbitmq/mnesia/* -rf</userinput>
	      </screen>
	    </listitem>

	    <listitem>
	      <para>Borramos la base de datos de
	      <literal>nova</literal>:</para>
	      <screen>
		<prompt>mysql> </prompt><userinput>drop database nova;</userinput>
	      </screen>		
	    </listitem>

	  </orderedlist>

	</listitem>

      </orderedlist>

    </section>
  </section>

  <section xml:id="id_instUbuntu_horizon">
    <title>Instalación de Horizon</title>
    <para>Este capítulo describe la instalación y configuración del
    módulo de OpenStack, Nova. Este servicio es el encargado de gestionar las
    instancias (máquinas virtuales) del Cloud, por lo que lo convierte en el
    servicio central de toda la infraestructura de Openstack.</para>

    <section>
      <title>¿Qué es Horizon?</title>
      <para>COMPLETAR POSTERIORMENTE</para>
      
    </section>

    <section>
      <title>Instalación y configuración</title>
      <para>Una infraestructura OpenStack solo necesita un servidor que ejecute
      el servicio Horizon, en nuestra instalación el servicio se ejecutará en el
      controlador, es decir, en la máquina <literal>jupiter</literal>.</para>

      <para>Tanto para la instalación de Horizon, como para la instalación del resto
      de servicios, partiremos de los repositorios oficiales de Ubuntu, basta
      con que sigamos los siguientes pasos en la máquina
      <literal>jupiter</literal>:</para>

      <orderedlist>
	<listitem>
	  <para>Instalamos todo el software necesario:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>apt-get install openstack-dashboard</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Reiniciamos el servicio:</para>
	  <screen>
	    <prompt>root@jupiter:~# </prompt><userinput>service apache2 restart</userinput>
	  </screen>
	</listitem>

	<listitem>
	  <para>Probamos el servicio visitando con un navegador web la siguiente
	  URL: <code>http://127.0.0.1</code> y utilizando como credenciales las
	  de los usuarios creados anteriormente durante la instalación de
	  Keystone. Por ejemplo, usuario <literal>admin</literal> y contraseña
	  <literal>calex2010!!</literal></para>
	</listitem>

	<listitem>
	  <para>Acceso desde otra máquina.</para>
	  <para>Podemos acceder también al dashboard desde otra máquina a través
	  de la IP asociada desde la red pública. En nuestra infraestructura la
	  siguiente URL: <code>http:://172.20.254.190</code></para>
	</listitem>

      </orderedlist>
    </section>
  </section>


<!-- Falta completar con la instalación de swift -->
<!-- Falta completar con la instalación del cliente -->

</chapter>
