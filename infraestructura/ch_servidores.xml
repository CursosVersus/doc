<?xml version="1.0" encoding="utf-8"?>
<chapter xmlns="http://docbook.org/ns/docbook"
	 xmlns:xlink="http://www.w3.org/1999/xlink"
	 version="5.0" xml:lang="es">
  <title>Servidores</title>
  <para>Los componentes principales de la infraestructura del cloud de
  infraestructura son los servidores que van a formar parte del 
  mismo. Para implantar una infraestructura de estas características
  hay que hacer una inversión importante por lo que es necesario
  conocer de forma precisa las características necesarias para estos
  equipos.</para>
  <para>La principal referencia que se utilizó fue <link
  xlink:href="http://www.referencearchitecture.org/hardware-specifications/"
  >http://www.referencearchitecture.org/hardware-specifications/</link>, que
  lamentablemente ya no está disponible. En la referencia anterior se detallaba
  de forma precisa las características mínimas y las recomendables para cada uno
  de los equipos que forman el cloud.</para>
  <para>Otras referencias que se han tenido en cuenta han sido:</para>
  <itemizedlist>
    <listitem>
      <para><link
		xlink:href="http://docs.stackops.org/display/doc03/Global+System+Requirements"
		>Global System Requirements. StackOps</link></para>
    </listitem>
    <listitem>
      <para><link
		xlink:href="http://docs.openstack.org/trunk/openstack-compute/admin/content/compute-system-requirements.html"
		>Compute and Image System Requirements. OpenStack</link></para>
    </listitem>
    <listitem>
      <para><link xlink:href="http://www.dell.com/OpenStack" >Recommended
      Systems. Dell</link></para>
    </listitem>
  </itemizedlist>
  <para>Además, se solicitó ayuda a la lista de usuarios de OpenStack
  directamente, que puede verse en el hilo "<link
  xlink:href="https://lists.launchpad.net/openstack/msg09098.html" >hardware
  specifications for a little private cloud</link>".</para>
  <para>Con toda esta información, nos decidimos por un determinado esquema en
  el que contaríamos con un nodo controlador, cuatro nodos de computación y un
  nodo de almacenamiento, equipos que se describen a continuación.</para>
  <section xml:id="tipo-servidores">
    <title>Tipos de servidores</title>
    <para>Dependiendo del tamaño de la organización y por tanto del número de
    equipos que se va a utilizar, se podrá asignar una función a cada servidor,
    tal como aparece en <link linkend="esquema-grande">ejemplo de
    arquitectura</link> de la documentación del proyecto OpenStack.</para>
    <mediaobject>
      <imageobject>
	<imagedata fileref="figures/NOVA_compute_nodes.png" scalefit="1" format="PNG" 
		   width="100%" contentdepth="100%" align="center"/>
      </imageobject>
      <caption>
	<para>Ejemplo de esquema de nodos de una infraestructura con varios
	nodos controladores y diferentes redes de computación, más información
	en la <link
	xlink:href="http://docs.openstack.org/trunk/openstack-compute/admin/content/example-installation-architecture.html">documentación
	oficial de OpenStack</link>.</para>
      </caption>
    </mediaobject>
    <para>Para una organización pequeña, como es nuestro caso, el esquema es
    mucho más sencillo y distinguiremos sólo entre tres tipos de nodos,
    dependiendo de la función que tendrán que realizar dentro del cloud:</para>
    <itemizedlist>
      <listitem>
	<para>Nodo controlador: Equipo en el que se instalarán la mayor
	parte de los servicios de gestión del cloud (autenticación,
	planificador, gestión del cloud, APIs, etc.)</para>
	<para>El nodo controlador no precisa ninguna característica
	especial y puede utilizarse cualquier equipo de
	características básicas. Sí sería importante, quizás en una
	segunda fase o como primera ampliación, tener dos nodos
	controladores funcionando en alta disponibilidad porque
	cualquier problema de funcionamiento de este equipo deja el
	cloud completamente inoperativo.</para>
      </listitem>
      <listitem>
	<para>Nodo de computación: Equipos en los que se ejecutarán
	las instancias y que por tanto necesitan procesadores potentes
	y mucha memoria RAM.</para>
	<para>Cualquier cloud de infraestructura contará con varios
	nodos de computación, tantos más cuantas más instancias sea
	necesario ejecutar simultáneamente. La cantidad de memoria RAM
	consumida por cada instancia depende mucho del sistema
	operativo que estén ejecutando, pero en el caso de un uso
	educativo y no de servidores en producción, sería suficiente
	con contar entre 0,5 y 2 GiB de RAM por instancia.</para>	
      </listitem>
      <listitem>
	<para>Nodo de almacenamiento: Equipo encargado de guardar las imágenes
	de los sistemas, las instantáneas (<emphasis>snapshots</emphasis>) y los
	volúmenes persisitentes de las instancias que se ejecuten. Obviamente en
	un equipo que necesitará tener varios discos duros configurados
	apropiadamente.</para>
      </listitem>
    </itemizedlist>
  </section>
  <section>
    <title>Configuraciones mínima y recomendada</title>
    <para>El factor fundamental que condiciona el número de nodos necesarios
    para implementar un cloud con OpenStack es el número de instancias que
    simultáneamente se van a ejecutar. En este sentido se podría plantear una
    configuración mínima como la del siguiente esquema:</para>
    <mediaobject>
      <imageobject role="fo">
	<imagedata fileref="figures/esquema-minimo.png" scalefit="1" format="PNG" 
		   width="70%" contentdepth="100%" align="center"/>
      </imageobject>
      <caption>
	<para>Esquema de cloud formado por un nodo controlador y dos nodos de
	computación.</para> 
      </caption>
    </mediaobject>
    <para>En este esquema el nodo controlador tendrá que asumir también las
    funcionalidades del nodo de almacenamiento y los dos nodos de computación
    serán los equipos que ejecuten las instancias. Dependiendo de las
    características de los nodos de computación se podrán ejecutar más o menos
    instancias simultáneamente.</para>
    <para>Para mejorar la configuración anterior, habría que aumentar el número
    de nodos de computación y separar el almacenamiento en un equipo aparte del
    nodo controlador, que permita por ejemplo instalar un sistema operativo
    específicamente orientado al almacenamiento por ejemplo. Es por esto por lo
    que planteamos como configuración recomendada para nuestro caso la del
    siguiente esquema:</para>
    <mediaobject>
      <imageobject role="fo">
	<imagedata fileref="figures/esquema_red_iesgn.png" scalefit="1" format="PNG" 
		   width="70%" contentdepth="100%" align="center"/>
      </imageobject>
      <caption>
	<para>Esquema de cloud formado por un nodo controlador y dos nodos de
	computación.</para> 
      </caption>
    </mediaobject>
    <para>Además de lo anterior, los nodos de computación se comunican con el
    nodo de almacenamiento a mediante 3 interfaces de red Gigabit Ethernet que
    habrá que configurar en modo <emphasis>bonding</emphasis> ya que es posible
    que haya mucho tráfico de red entre los nodos de computación y el de
    almacenamiento si se utiliza mucho almacenamiento permanente. Además los
    nodos de computación están directamente conectados a lo que se denomina red
    pública, que se explicará posteriormente, lo que permite que se acceda a las
    instancias directamente sin pasar por el nodo controlador.</para>
  </section>
  <section xml:id="nodo-controlador">
    <title>Nodo controlador</title>
    <para>Este es el equipo más importante del cloud, ya que de su buen
    funcionamiento depende que el cloud esté operativo o no, es por eso por lo
    que es aconsejable disponer de dos nodos similares con todos los servicios
    del cloud configurados en alta disponibilidad (<emphasis>high availability
    (HA)</emphasis>), sin embargo es un equipo que no tiene importantes demandas
    en cuanto a las características de hardware, por lo que podemos utilizar un
    equipo con unas características elementales:</para>
    <itemizedlist>
      <listitem>
	<para>Procesador x86_64 (no son necesarias extensiones de
	virtualización por hardware).</para>
      </listitem>
      <listitem>
	<para>4 GiB de RAM ECC registrada</para>
      </listitem>
      <listitem>
	<para>1 interfaz de red 1 Gbps</para>
      </listitem>
      <listitem>
	<para>30 GiB de disco duro</para>
      </listitem>
    </itemizedlist>
    <para>De acuerdo a estas características, en este proyecto se eligió el
    servidor <link linkend="supero-1012c">SuperMicro 1012-MRF</link>.</para>
    <section xml:id="supero-1012c">
      <title>Supermicro-1012C-MRF</title> 
      <para>Este equipo es el encargado de gestionar y organizar todo el
      cloud, es el que precisa una mayor configuración del software pero
      a su vez no requiere unas prestaciones elevadas. Se ha optado por
      un servidor de una unidad de armario bastante convencional, en el
      que se configurarán los dos discos duros en modo RAID 1, para
      guardar los datos replicados en previsión de futuros fallos de
      disco.</para> 
      <para>En una mejora posterior del cloud, se podría plantear poner
      este equipo en alta disponibilidad con otro equipo de
      características similares.</para>
      <para>Especificaciones técnicas del equipo:</para>
      <itemizedlist>
	<listitem><para>Chasis 
	<link xlink:href="http://www.supermicro.com/products/chassis/1U/512/SC512F-350.cfm">SC512F-350</link>. 
	Altura: 1U, anchura: 19", profundidad: 369 mm.</para></listitem> 
	<listitem><para>Placa base <link xlink:href="http://www.supermicro.com/Aplus/motherboard/Opteron4000/SR56x0/H8SCM-F.cfm">
	Supermicro H8SCM-F</link>.
	</para></listitem>
	<listitem><para>Fuente de alimentación de 350W (80+ Gold).</para></listitem>
	<listitem><para>1 procesador <link xlink:href="http://products.amd.com/en-us/OpteronCPUDetail.aspx?id=779&amp;f1=AMD+Opteron%E2%84%A2+4200+Series+Processor&amp;f2=&amp;f3=Yes&amp;f4=&amp;f5=1000&amp;f6=C32&amp;f7=B2&amp;f8=32nm&amp;f9=&amp;f10=6400&amp;f11=&amp;">AMD
	4226</link> de 6 cores a 2700 MHz.</para></listitem>
	<listitem><para>4 GB de RAM DDR3/1333 ECC registrada</para></listitem>
	<listitem><para>Dos interfaces de red Gigabit integradas Intel 82574L</para></listitem>
	<listitem><para>Controladora Raid <link 
	xlink:href="http://www.lsi.com/products/storagecomponents/Pages/3ware9650SE-2LP.aspx">LSI
	3 Ware 9650 SE – 2LP de 128Mb PCI-E Raid 0,1 hardware</link>.</para></listitem>
	<listitem><para>2 Discos duros internos de 500 GB <link xlink:href="http://edgekey.seagate.com/files/staticfiles/docs/pdf/es-ES/datasheet/disc/Constellation-2-fips-data-sheet-ds1719-3-1111-es.pdf">SATA Seagate Constellation a 7200 rpm, 64MB de caché e 
	interfaz de 6G/s</link>.</para></listitem>
	<listitem><para>Interfaz de gestión IPMI 2.0</para></listitem>
      </itemizedlist>
            <mediaobject>
	<imageobject>
	  <imagedata fileref="figures/supermicro-as-1012c-mrf.jpg" scalefit="1"
		     format="PNG" width="70%" contentdepth="100%" align="center"/>
	</imageobject>
	<caption>
	  <para>Imagen del equipo Supermicro 1012C-MRF.</para>
	</caption>
      </mediaobject>
    </section>
  </section>
  <section xml:id="nodo-computacion">
    <title>Nodo de computación</title>
    <para>Los nodos de computación (ya que lo lógico es que sean varios) sí son
    muy exigentes en cuanto a las características del hardware, porque tienen
    sólo una función que es la de ejecutar las instancias del cloud, pero como
    cada uno de ellos podrá ejecutar simultáneamente varias de ellas, cuantas
    más altas sean sus prestaciones, tantas más instancias podrán ejecutarse en
    él.</para>
    <section xml:id="procesadores">
      <title>Procesadores</title>
      <para>Por relación calidad/precio se ha considerado que los procesadores
      que se ajustaban mejor a las necesidades del cloud son los AMD Interlagos
      (AMD 62XX) con socket G34, procesadores que disponen de hasta 16
      cores.</para>
    </section>
    <section xml:id="ram">
      <title>RAM</title>
      <para>En este tipo de equipos, es recomendable utilizar memoria ECC
      (<emphasis>Error-correcting code</emphasis>) y registrada
      (<emphasis>registered</emphasis> o <emphasis>buffered</emphasis>), que
      ofrece funcionalidades adicionales a la de la RAM convencional.</para>
      <para>Puesto que la ejecución de instancias consume mucha RAM, estos
      equipos requerirán gran cantidad de ella (en nuestro caso hemos utilizado
      hasta 64 GiB por nodo).</para>
    </section>
    <section xml:id="hd">
      <title>Discos duros</title>
      <para>Hay tres opciones para los discos duros actualmente:</para>
      <itemizedlist>
	<listitem>
	  <para>SATA (7200 ó 10000 rpm): Hasta 3 TiB de almacenamiento, bus de 3
	  Gbps y hasta 100 IOPS</para>
	</listitem>
	<listitem>
	  <para>SAS 10000 ó 15000 rpm): Hasta 900 GiB de almacenamiento, bus de
	  6 Gbps y hasta 150 IOPS</para>
	</listitem>
	<listitem>
	  <para>SSD: Hasta 1 TiB de almacenamiento, bus de 3 ó 6 Gbps y hasta
	  100.000 IOPS (!).</para>
	</listitem>
      </itemizedlist>
      <para>El único factor que falta en la relación anterior es el precio, que
      es directamente proporcional al orden anterior y en nuestro caso ha hecho
      que tengamos que descartar los discos SSD por estar fuera de nuestras
      posibilidades.</para>
      <para>A la hora de elegir entre discos SAS o SATA, es necesario determinar
      la cantidad total de almacenamiento necesaria y en este caso no es
      demasiada, los nodos de computación almacenarán provisionalmente los
      discos principales de las instancias que se estén ejecutando (un máximo de
      50 instancias por nodo y hasta 10 GiB de disco por instancia (en muchos
      casos es menos)), resulta que no es necesario en cada nodo más de 500 GiB
      de almacenamiento, por lo que hemos considerado como la mejor opción 2
      discos SAS 15000 rpm de 300 GiB en modo RAID0 por nodo.</para>
    </section>
    <section>
      <title>Interfaces de red</title>
      <para>Las instancias que se ejecutan en los nodos de computación pueden
      tener volúmenes para almacenamiento permanente disponibles en el nodo de
      almacenamiento, por lo que el tráfico entre los nodos de computación y el
      nodo de almacenamiento puede ser alto. Teniendo en cuenta esta
      circunstancia es conveniente disponer de varias interfaces de red Gigabit
      Ethernet en cada nodo para configurarlas como una sola interfaz virtual
      mediante el mecanismo conocido como <emphasis>bonding</emphasis> que
      permite hacer balanceo de carga y a la vez tener redundancia en la
      conexión.</para>
      <para>De acuerdo a estas características, en este proyecto se eligió el
      servidor <link linkend="serv-doble-twin">SuperMicro
      2022TG-H6RF</link>.</para>
    </section>
    <section xml:id="serv-doble-twin">
      <title>Supermicro-2022TG-H6RF</title>
      <para>Se tomó inicialmente como referencia el equipo <link
      xlink:href="http://www.dell.com/us/enterprise/p/poweredge-c6105/pd?refid=poweredge-c6105"
      >Dell PowerEdge C6105</link> recomendado por OpenStack, pero se encontró
      un equipo de prestaciones muy superiores y mejor de precio.</para>
      <para>Este equipo realmente son 4 equipos en 1, que comparten un mismo
      chasis y 2 fuentes de alimentación. Los 4 nodos de este equipo son
      totalmente independientes y cualquier fallo en uno de ellos no afecta al
      funcionamiento de los demás.</para>
      <para>Estos 4 nodos son los equipos que realmente van a soportar la mayor
      parte del trabajo del cloud, ya que en ellos se ejecutan todas las
      instancias del cloud (máquinas virtuales), por lo que es necesario que
      sean equipos de altas prestaciones, con procesadores muy potentes y gran
      cantidad de memoria RAM, además se ha optado por un modelo con bus SAS y
      discos SAS de 15.000 rpm para conseguir altas velocidades de E/S a disco
      local. Para la comunicación con la red de almacenamiento y el <link
      linkend="nodo-controlador">servidor de gestión</link> se han incluido
      varias interfaces de red Gigabit configuradas en modo
      <emphasis>bonding</emphasis> IEEE 802.3ad (<emphasis>link
      aggregation</emphasis>) para aumentar el ancho de banda total y
      redundancia del canal.</para>
      <para>Especificaciones técnicas del equipo:</para>
      <itemizedlist>
	<listitem><para>Chasis 
	<link xlink:href="http://www.supermicro.com/products/chassis/2u/827/sc827hq-r1620.cfm">
	827HQ-R1620B</link>. Altura: 2U, anchura: 19", profundidad: 724 mm.</para></listitem>
	<listitem><para>12 bahías de 3.5" para discos duros extraíbles en caliente.</para></listitem>
	<listitem><para>4 placas base <link xlink:href="http://www.supermicro.com/Aplus/motherboard/Opteron6000/SR56x0/H8DGT-HLF.cfm">
	Supermicro H8DGT-HLF</link> de doble socket AMD G34 y chipset AMD SR5690 + SP5100.</para></listitem>
	<listitem><para>2 fuentes de alimentación redundantes de 1620W (80+ Platinum).</para></listitem>
	<listitem><para>2 procesadores <link
	xlink:href="http://products.amd.com/en-us/OpteronCPUDetail.aspx?id=762&amp;f1=AMD+Opteron%E2%84%A2+6200+Series+Processor&amp;f2=&amp;f3=Yes&amp;f4=&amp;f5=&amp;f6=G34&amp;f7=B2&amp;f8=32nm&amp;f9=&amp;f10=6400&amp;f11=&amp;">AMD
	6238</link> de 12 cores a 2600 MHz o <link
	xlink:href="http://products.amd.com/en-us/OpteronCPUDetail.aspx?id=764&amp;f1=AMD+Opteron%E2%84%A2+6200+Series+Processor&amp;f2=&amp;f3=Yes&amp;f4=&amp;f5=&amp;f6=G34&amp;f7=B2&amp;f8=32nm&amp;f9=&amp;f10=6400&amp;f11=&amp;">AMD
	6220</link> de 8 cores a 3000 MHz por nodo.</para></listitem>
	<listitem><para>32 ó 64 GiB de RAM 1333 ECC registrada por nodo.
	</para></listitem>
	<listitem><para>Dos interfaces de red Gigabit integradas Intel 82576 en cada nodo.</para></listitem>
	<listitem><para>1 tarjetas de red PCI-E de doble puerto Gigabit por nodo.</para></listitem>
	<listitem><para>2 Discos duros 350 GB <link
	xlink:href="http://www.seagate.com/files/docs/pdf/datasheet/disc/cheetah-15k.7-ds1677.3-1007us.pdf">
	SAS Seagate Cheetah a 15000 rpm, 16MB de caché e interfaz de 6G/s por
	nodo</link>.</para></listitem>
	<listitem><para>1 interfaz de gestión IPMI 2.0 por nodo.</para></listitem>
      </itemizedlist>
      <mediaobject>
	<imageobject>
	  <imagedata fileref="figures/AS-2022TG-HLIBQRF.jpg"  scalefit="1" format="JPG" 
		   width="100%" contentdepth="100%" align="center"/>
	</imageobject>
	<caption>
	  <para>Imagen posterior del equipo Supermicro 2022TG-H6RF en la que se observa los 4 nodos que lo componen 
	  y las fuentes de alimentación redundantes y extraíbles.</para>
	</caption>
      </mediaobject>
    </section>
  </section>
  <section xml:id="nodo-almacenamiento">
    <title>Nodo de almacenamiento</title>
    <para>Obviamente debe ser un equipo que pueda alojar varios discos duros de
    alta capacidad de almacenamiento (SATA) y con un procesador y RAM
    adecuadamente dimensionados. En este proyecto nos decantamos por el <link
    linkend="serv-alm">Intel SR2612UR</link>.</para>
    <section xml:id="serv-alm">
      <title>Intel-SR2612UR</title>
      <para>Este equipo es el que se va a utilizar para almacenamiento secundario y permanente de las 
      instancias del cloud, por lo que su principal característica es tener varios discos duros de
      gran capacidad.</para>
      <para>Especificaciones técnicas del equipo:</para>
      <itemizedlist>
	<listitem><para>Chasis SR2612. Altura 2U, anchura: 19" y profundidad: 781 mm.</para></listitem>
	<listitem><para>2 fuentes de alimentación redundantes de 760W.</para></listitem>
	<listitem><para>12 bahías de 3.5" para discos duros extraíbles en caliente.</para></listitem>
	<listitem><para>Placa base <link xlink:href="http://www.intel.com/content/www/us/en/motherboards/server-motherboards/server-board-s5520ur.html?wapkw=intel+server+board+s5520ur">Intel S5520UR</link> de doble socket LGA 1366 (socket B) y chipset Intel 5520/ICH10R.</para></listitem>
	<listitem><para>1 procesador <link xlink:href="http://ark.intel.com/products/47925?wapkw=intel+xeon+e5620">
	Intel Xeon E5620</link>de 4 cores a 2.4 GHz</para></listitem>
	<listitem><para>24 GB de RAM DDR3 a 1333 MHz ECC registrada</para></listitem>
	<listitem><para>Dos interfaces de red integradas Intel 82575EB.</para></listitem>
	<listitem><para>1 tarjeta de red adicional PCI-E de doble puerto Gigabit.</para></listitem>
	<listitem><para>6 discos duros de 2 TB <link xlink:href="http://edgekey.seagate.com/files/staticfiles/docs/pdf/es-ES/datasheet/disc/Constellation-2-fips-data-sheet-ds1719-3-1111-es.pdf">SATA Seagate Constellation a 7200 rpm, 64MB de caché e interfaz de 6G/s</link> 
	extraíbles en caliente.</para></listitem>
	<listitem><para>1 disco duro interno SSD de 120 GB <link xlink:href="http://www.kingston.com/datasheets/SH100S3_us.pdf">
	Kingston HyperX</link></para></listitem>
	<listitem><para>1 controladora de disco <link xlink:href="http://www.lsi.com/channel/products/storagecomponents/Pages/LSISAS9211-8i.aspx">
	LSI SAS 9211-8i</link>.</para></listitem>
      </itemizedlist>
      <mediaobject>
	<imageobject>
	  <imagedata fileref="figures/intel-sr2612.png"  scalefit="1" format="JPG" 
		   width="100%" contentdepth="100%" align="center"/>
	</imageobject>
	<caption>
	  <para>Imagen equipo Intel SR2612UR.</para>
	</caption>
      </mediaobject>
    <para><link xlink:href="http://docs.openstack.org/trunk/openstack-compute/admin/content/compute-system-requirements.html"/></para>
    </section>    
  </section>
</chapter>